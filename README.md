# 🌟 广东省数据中心爬虫项目

## 📋 项目概述
本项目是一个专门用于爬取广东省数据中心分布信息的Python工具。项目已从原来的三省（四川、云南、贵州）数据中心爬虫重构为专门针对广东省的数据中心信息采集和分析工具。

## 🎯 主要功能
- 🔍 **智能多源爬取**：使用多种URL变体提高数据覆盖率
- 🗺️ **地理验证**：仅保留广东省地理范围内的有效坐标
- 🔄 **自动去重**：基于坐标精度智能去除重复数据
- 📊 **详细报告**：生成CSV、JSON格式数据和详细分析报告
- 🛠️ **容错处理**：完善的异常处理和错误恢复机制

## 📁 项目结构
```
Pc_ZGLZ/
├── 📂 src/                          # 源代码目录
│   ├── guangdong_datacenter_crawler.py  # 广东省数据中心爬虫
│   └── [其他历史爬虫脚本...]            # 其他省份爬虫（已归档）
├── 📊 data/                         # 数据文件目录
│   ├── guangdong/                   # 广东省数据
│   │   ├── 广东省数据中心坐标_[时间戳].csv
│   │   └── 广东省数据中心坐标_[时间戳].json
│   └── archive/                     # 旧数据归档
├── 🌐 html_sources/                 # HTML源码目录
│   ├── guangdong/                   # 广东省HTML源码（调试用）
│   └── archive/                     # 旧HTML归档
├── 📋 reports/                      # 报告文件目录
│   ├── guangdong/                   # 广东省分析报告
│   └── archive/                     # 旧报告归档
├── 🔧 scripts/                      # 脚本工具目录
│   ├── organize_project.py          # 项目整理工具
│   ├── run_guangdong_crawler.bat    # 批处理运行脚本
│   └── requirements.txt             # 依赖包配置
├── 📚 docs/                         # 项目文档
└── 💾 backup/                       # 备份文件
```

## 🚀 快速开始

### 1. 环境准备
```bash
# 克隆或下载项目
# 安装Python依赖
pip install -r scripts/requirements.txt
```

### 2. 运行爬虫
#### 方式一：直接运行Python脚本
```bash
python src/guangdong_datacenter_crawler.py
```

#### 方式二：使用批处理脚本（Windows）
```bash
# 双击运行或在命令行执行
scripts/run_guangdong_crawler.bat
```

### 3. 查看结果
- **数据文件**：`data/guangdong/` 目录下的CSV和JSON文件
- **分析报告**：`reports/guangdong/` 目录下的详细报告
- **调试信息**：`html_sources/guangdong/` 目录下的HTML源码

## 📊 输出数据说明

### 数据文件格式
**CSV/JSON文件包含以下字段：**
- `province`: 省份（广东省）
- `city`: 城市/地区
- `latitude`: 纬度坐标
- `longitude`: 经度坐标
- `name`: 数据中心名称
- `source`: 数据来源
- `coordinates`: 坐标字符串（纬度,经度）
- `index`: 数据索引
- `crawl_time`: 爬取时间

### 分析报告内容
- 爬取统计信息
- 城市分布统计
- 详细数据中心列表
- 地理分布分析
- 技术说明

## 🎯 爬取目标

### 覆盖区域
- **省级**：广东省整体
- **主要城市**：深圳、广州、东莞、佛山、珠海、中山、惠州等

### URL策略
项目使用多种URL变体确保数据完整性：
```
- guangdong-sheng (主要数据源)
- guangdong
- guang-dong-sheng
- 各主要城市的独立URL
```

## ⚙️ 技术特点

### 1. 智能数据提取
- 使用正则表达式提取坐标和名称
- 多种匹配模式确保数据完整性
- 智能坐标配对算法

### 2. 数据验证
- 地理范围验证（广东省：纬度20-25.5°，经度109-117.5°）
- 坐标有效性检查
- 重复数据自动去除

### 3. 容错机制
- 网络超时重试
- 异常处理和日志记录
- 优雅降级策略

## 📦 依赖包
```
requests>=2.28.0      # HTTP请求
pandas>=1.5.0         # 数据处理
numpy>=1.21.0         # 数值计算
beautifulsoup4>=4.11.0 # HTML解析（可选）
```

## 🛠️ 项目工具

### 项目整理工具
```bash
python scripts/organize_project.py
```
功能：
- 自动整理文件到对应目录
- 备份原始文件
- 生成项目结构报告

### 批处理脚本
- `run_guangdong_crawler.bat`：交互式运行脚本
- 提供菜单选择功能

## 📈 最新成果

### 2025-06-20 运行结果
- ✅ **成功获取**：39个广东省数据中心
- 🎯 **覆盖范围**：包含深圳、广州、佛山、东莞、珠海、中山、惠州等主要城市
- 📍 **地理分布**：纬度21.24°-24.80°，经度110.41°-116.12°
- 🏢 **主要数据中心**：
  - Shenzhen Data Center
  - Guangzhou Cloud-computing Data Center
  - Foshan Shunde IDC Data Center
  - GDS SZ4 Data Center
  - 等39个专业数据中心

## 🔄 项目历史

### 版本演进
- **v1.0**：三省数据中心爬虫（四川、云南、贵州）
- **v2.0**：重构为广东省专用爬虫（当前版本）

### 改进内容
- 专注广东省数据，提高准确性
- 优化目录结构，便于维护
- 增强数据验证和去重功能
- 完善文档和使用说明

## ⚠️ 注意事项

1. **网络要求**：需要稳定的网络连接访问目标网站
2. **使用频率**：建议适度使用，避免对目标网站造成过大压力
3. **数据时效性**：爬取的数据可能存在时效性，建议定期更新
4. **合规使用**：请遵守robots.txt和相关法律法规

## 📞 技术支持

如遇到问题，请：
1. 查看 `reports/` 目录下的详细日志
2. 检查网络连接和目标网站可访问性
3. 查看 `html_sources/` 目录下的源码进行调试

## 📄 许可证
本项目仅供学习和研究使用。使用时请遵守相关法律法规和网站使用条款。

---
**更新时间**：2025-06-20  
**项目状态**：✅ 活跃开发  
**当前版本**：v2.0 - 广东省专版
