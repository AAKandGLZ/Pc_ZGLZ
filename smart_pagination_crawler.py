#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JavaScriptç¿»é¡µæ£€æµ‹å’Œçˆ¬å–å·¥å…·
ä¸ä¾èµ–WebDriverï¼Œä½¿ç”¨requests-htmlæˆ–å…¶ä»–æ–¹æ³•å¤„ç†JavaScriptç¿»é¡µ
"""

import requests
import json
import time
import os
import re
from datetime import datetime
from bs4 import BeautifulSoup
import threading

class SmartPaginationCrawler:
    def __init__(self):
        self.base_url = "https://www.datacenters.com/locations/china/shanghai"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.datacenters.com/',
            'Cache-Control': 'no-cache',
            'DNT': '1',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.all_datacenters = []
        self.page_data = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("data/shanghai", exist_ok=True)
        os.makedirs("html_sources/shanghai", exist_ok=True)
    
    def analyze_page_structure(self):
        """åˆ†æé¡µé¢ç»“æ„ï¼Œæ‰¾å‡ºJavaScriptç¿»é¡µæœºåˆ¶"""
        print("ğŸ” åˆ†æé¡µé¢çš„JavaScriptç¿»é¡µæœºåˆ¶...")
        
        try:
            response = self.session.get(self.base_url, timeout=30)
            if response.status_code != 200:
                print(f"âŒ é¡µé¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
            
            # ä¿å­˜åŸå§‹é¡µé¢
            with open("html_sources/shanghai/original_page.html", 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            analysis = {
                'pagination_info': self.extract_pagination_info(response.text, soup),
                'ajax_endpoints': self.find_ajax_endpoints(response.text),
                'javascript_functions': self.find_javascript_pagination(response.text),
                'data_structure': self.analyze_data_structure(response.text)
            }
            
            return analysis
            
        except Exception as e:
            print(f"âŒ é¡µé¢åˆ†æå¤±è´¥: {e}")
            return None
    
    def extract_pagination_info(self, content, soup):
        """æå–åˆ†é¡µä¿¡æ¯"""
        pagination_info = {
            'total_pages': 0,
            'current_page': 1,
            'items_per_page': 0,
            'total_items': 0,
            'pagination_method': 'unknown'
        }
        
        # ä»JavaScriptå˜é‡ä¸­æå–åˆ†é¡µä¿¡æ¯
        js_patterns = [
            r'"totalPages":\s*(\d+)',
            r'"pageCount":\s*(\d+)', 
            r'"currentPage":\s*(\d+)',
            r'"page":\s*(\d+)',
            r'totalPages\s*[:=]\s*(\d+)',
            r'pageCount\s*[:=]\s*(\d+)',
            r'var\s+totalPages\s*=\s*(\d+)',
            r'let\s+totalPages\s*=\s*(\d+)',
            r'const\s+totalPages\s*=\s*(\d+)',
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    value = int(matches[0])
                    if 'total' in pattern.lower() or 'count' in pattern.lower():
                        pagination_info['total_pages'] = value
                        print(f"  æ£€æµ‹åˆ°æ€»é¡µæ•°: {value}")
                    elif 'current' in pattern.lower():
                        pagination_info['current_page'] = value
                        print(f"  æ£€æµ‹åˆ°å½“å‰é¡µ: {value}")
                except:
                    continue
        
        # æŸ¥æ‰¾åˆ†é¡µæŒ‰é’®
        pagination_elements = soup.find_all(['a', 'button'], string=re.compile(r'\d+'))
        page_numbers = []
        for elem in pagination_elements:
            text = elem.get_text(strip=True)
            if text.isdigit():
                page_numbers.append(int(text))
        
        if page_numbers:
            pagination_info['total_pages'] = max(pagination_info['total_pages'], max(page_numbers))
            print(f"  ä»æŒ‰é’®æ£€æµ‹åˆ°é¡µæ•°: {max(page_numbers)}")
        
        # æŸ¥æ‰¾æ•°æ®é¡¹æ€»æ•°
        total_patterns = [
            r'(\d+)\s*(?:results?|æ•°æ®ä¸­å¿ƒ|data\s*centers?)',
            r'(?:Total|å…±)\s*[:ï¼š]?\s*(\d+)',
            r'(?:Showing|æ˜¾ç¤º)\s+\d+\s*-\s*\d+\s+(?:of|å…±)\s+(\d+)',
        ]
        
        for pattern in total_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    total = int(matches[0])
                    pagination_info['total_items'] = total
                    print(f"  æ£€æµ‹åˆ°æ•°æ®æ€»æ•°: {total}")
                    break
                except:
                    continue
        
        return pagination_info
    
    def find_ajax_endpoints(self, content):
        """æŸ¥æ‰¾AJAXç«¯ç‚¹"""
        endpoints = []
        
        ajax_patterns = [
            r'url\s*:\s*["\']([^"\']*(?:page|data|search|load)[^"\']*)["\']',
            r'fetch\s*\(\s*["\']([^"\']+)["\']',
            r'\.get\s*\(\s*["\']([^"\']+)["\']',
            r'\.post\s*\(\s*["\']([^"\']+)["\']',
            r'ajax\s*\(\s*["\']([^"\']+)["\']',
            r'loadUrl\s*:\s*["\']([^"\']+)["\']',
        ]
        
        for pattern in ajax_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if any(keyword in match.lower() for keyword in ['datacenter', 'location', 'search', 'api']):
                    endpoints.append(match)
        
        unique_endpoints = list(set(endpoints))
        if unique_endpoints:
            print(f"  æ‰¾åˆ°AJAXç«¯ç‚¹: {unique_endpoints}")
        
        return unique_endpoints
    
    def find_javascript_pagination(self, content):
        """æŸ¥æ‰¾JavaScriptç¿»é¡µå‡½æ•°"""
        functions = []
        
        function_patterns = [
            r'function\s+(\w*(?:page|load|show|navigate)\w*)\s*\(',
            r'(\w*(?:page|load|show|navigate)\w*)\s*:\s*function',
            r'(\w*(?:page|load|show|navigate)\w*)\s*=\s*function',
            r'(\w*(?:page|load|show|navigate)\w*)\s*=>\s*{',
        ]
        
        for pattern in function_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            functions.extend(matches)
        
        unique_functions = list(set(functions))
        if unique_functions:
            print(f"  æ‰¾åˆ°ç¿»é¡µå‡½æ•°: {unique_functions}")
        
        return unique_functions
    
    def analyze_data_structure(self, content):
        """åˆ†ææ•°æ®ç»“æ„"""
        data_info = {
            'data_variables': [],
            'data_arrays': [],
            'coordinate_patterns': []
        }
        
        # æŸ¥æ‰¾æ•°æ®å˜é‡
        var_patterns = [
            r'(?:var|let|const)\s+(\w*(?:data|center|location|marker)\w*)\s*=',
            r'window\.(\w*(?:data|center|location|marker)\w*)\s*=',
        ]
        
        for pattern in var_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            data_info['data_variables'].extend(matches)
        
        # æŸ¥æ‰¾æ•°ç»„æ•°æ®
        array_patterns = [
            r'(\w*(?:data|center|location|marker)\w*)\s*=\s*\[',
            r'(\w*(?:data|center|location|marker)\w*)\s*:\s*\[',
        ]
        
        for pattern in array_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            data_info['data_arrays'].extend(matches)
        
        if data_info['data_variables']:
            print(f"  æ‰¾åˆ°æ•°æ®å˜é‡: {list(set(data_info['data_variables']))}")
        if data_info['data_arrays']:
            print(f"  æ‰¾åˆ°æ•°æ®æ•°ç»„: {list(set(data_info['data_arrays']))}")
        
        return data_info
    
    def try_different_pagination_approaches(self):
        """å°è¯•ä¸åŒçš„ç¿»é¡µæ–¹æ³•"""
        print("ğŸ”„ å°è¯•ä¸åŒçš„ç¿»é¡µæ–¹æ³•è·å–å®Œæ•´æ•°æ®...")
        
        all_results = []
        successful_methods = []
        
        # æ–¹æ³•1: AJAXè¯·æ±‚ç¿»é¡µ
        print("  æ–¹æ³•1: AJAXè¯·æ±‚ç¿»é¡µ")
        ajax_results = self.try_ajax_pagination()
        if ajax_results:
            all_results.extend(ajax_results)
            successful_methods.append("AJAXç¿»é¡µ")
        
        # æ–¹æ³•2: è¡¨å•POSTç¿»é¡µ
        print("  æ–¹æ³•2: è¡¨å•POSTç¿»é¡µ")
        post_results = self.try_post_pagination()
        if post_results:
            all_results.extend(post_results)
            successful_methods.append("POSTç¿»é¡µ")
        
        # æ–¹æ³•3: æ„é€ ç‰¹æ®Šå‚æ•°
        print("  æ–¹æ³•3: æ„é€ ç‰¹æ®Šå‚æ•°")
        param_results = self.try_parameter_pagination()
        if param_results:
            all_results.extend(param_results)
            successful_methods.append("å‚æ•°ç¿»é¡µ")
        
        # æ–¹æ³•4: æ¨¡æ‹ŸJavaScriptè¯·æ±‚
        print("  æ–¹æ³•4: æ¨¡æ‹ŸJavaScriptè¯·æ±‚")
        js_results = self.try_javascript_simulation()
        if js_results:
            all_results.extend(js_results)
            successful_methods.append("JSæ¨¡æ‹Ÿ")
        
        return all_results, successful_methods
    
    def try_ajax_pagination(self):
        """å°è¯•AJAXç¿»é¡µ"""
        results = []
        
        # å¸¸è§çš„AJAXç«¯ç‚¹è·¯å¾„
        ajax_paths = [
            "/api/locations",
            "/api/datacenters", 
            "/search/locations",
            "/locations/data",
            "/ajax/search",
            "/data/locations.json",
        ]
        
        base_domain = "https://www.datacenters.com"
        
        for path in ajax_paths:
            url = base_domain + path
            
            # å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
            param_combinations = [
                {'location': 'shanghai', 'page': 1},
                {'location': 'shanghai', 'page': 2},
                {'city': 'shanghai', 'page': 1},
                {'city': 'shanghai', 'page': 2},
                {'q': 'shanghai', 'offset': 0, 'limit': 50},
                {'search': 'shanghai', 'start': 0, 'count': 50},
                {'query': 'shanghai', 'pageNumber': 1},
                {'query': 'shanghai', 'pageNumber': 2},
            ]
            
            for params in param_combinations:
                try:
                    response = self.session.get(url, params=params, timeout=20)
                    if response.status_code == 200:
                        # å°è¯•è§£æä¸ºJSON
                        try:
                            data = response.json()
                            if isinstance(data, list) and data:
                                page_results = self.process_json_response(data, f"ajax_{path.split('/')[-1]}")
                                if page_results:
                                    results.extend(page_results)
                                    print(f"    AJAXæˆåŠŸ: {url} -> {len(page_results)} ä¸ª")
                        except:
                            # å¯èƒ½æ˜¯HTMLå“åº”
                            html_results = self.extract_from_html(response.text, f"ajax_{path.split('/')[-1]}")
                            if html_results:
                                results.extend(html_results)
                                print(f"    AJAX HTMLæˆåŠŸ: {url} -> {len(html_results)} ä¸ª")
                    
                    time.sleep(1)
                    
                except Exception as e:
                    continue
        
        return results
    
    def try_post_pagination(self):
        """å°è¯•POSTç¿»é¡µ"""
        results = []
        
        try:
            # é¦–å…ˆè·å–é¡µé¢ä¸­çš„è¡¨å•ä¿¡æ¯
            response = self.session.get(self.base_url, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾è¡¨å•
                forms = soup.find_all('form')
                for form in forms:
                    action = form.get('action', '')
                    method = form.get('method', 'GET').upper()
                    
                    if method == 'POST' or 'search' in action.lower():
                        # æ„é€ è¡¨å•æ•°æ®
                        form_data = {}
                        
                        # æå–ç°æœ‰çš„inputå€¼
                        for input_elem in form.find_all('input'):
                            name = input_elem.get('name')
                            value = input_elem.get('value', '')
                            if name:
                                form_data[name] = value
                        
                        # æ·»åŠ åˆ†é¡µå‚æ•°
                        page_params = [
                            {'page': 1}, {'page': 2},
                            {'pageNumber': 1}, {'pageNumber': 2},
                            {'offset': 0}, {'offset': 40},
                            {'start': 0}, {'start': 40},
                        ]
                        
                        for page_param in page_params:
                            post_data = {**form_data, **page_param, 'location': 'shanghai', 'city': 'shanghai'}
                            
                            try:
                                if action.startswith('http'):
                                    post_url = action
                                else:
                                    post_url = f"https://www.datacenters.com{action}"
                                
                                post_response = self.session.post(post_url, data=post_data, timeout=20)
                                
                                if post_response.status_code == 200:
                                    page_results = self.extract_from_html(post_response.text, f"post_page_{page_param}")
                                    if page_results:
                                        results.extend(page_results)
                                        print(f"    POSTæˆåŠŸ: {post_url} -> {len(page_results)} ä¸ª")
                                
                                time.sleep(1)
                                
                            except:
                                continue
        
        except Exception as e:
            print(f"    POSTæ–¹æ³•å‡ºé”™: {e}")
        
        return results
    
    def try_parameter_pagination(self):
        """å°è¯•ç‰¹æ®Šå‚æ•°ç¿»é¡µ"""
        results = []
        
        # å°è¯•å„ç§å‚æ•°ç»„åˆ
        param_sets = [
            # åŸºç¡€åˆ†é¡µå‚æ•°
            {'page': 1}, {'page': 2}, {'page': 3},
            {'p': 1}, {'p': 2}, {'p': 3},
            
            # åç§»é‡å‚æ•°
            {'offset': 0}, {'offset': 40}, {'offset': 80},
            {'start': 0}, {'start': 40}, {'start': 80},
            {'from': 0}, {'from': 40}, {'from': 80},
            
            # é™åˆ¶å‚æ•°
            {'limit': 50}, {'limit': 100},
            {'count': 50}, {'count': 100},
            {'size': 50}, {'size': 100},
            
            # ç»„åˆå‚æ•°
            {'page': 1, 'size': 50}, {'page': 2, 'size': 50},
            {'offset': 0, 'limit': 50}, {'offset': 40, 'limit': 50},
            
            # ç‰¹æ®Šå‚æ•°
            {'view': 'all'}, {'view': 'list'}, {'view': 'grid'},
            {'format': 'json'}, {'format': 'xml'},
            {'ajax': '1'}, {'xhr': '1'},
            
            # æœç´¢ç›¸å…³
            {'search': 'shanghai'}, {'q': 'shanghai'}, {'query': 'shanghai'},
            {'location': 'shanghai'}, {'city': 'shanghai'}, {'country': 'china'},
        ]
        
        for params in param_sets:
            try:
                response = self.session.get(self.base_url, params=params, timeout=20)
                
                if response.status_code == 200:                    # æ£€æŸ¥æ˜¯å¦è·å¾—äº†ä¸åŒçš„æ•°æ®
                    param_str = str(params).replace(' ', '').replace(':', '').replace(',', '_').replace('{', '').replace('}', '').replace("'", '')
                    page_results = self.extract_from_html(response.text, f"param_{param_str}")
                    
                    if page_results:
                        results.extend(page_results)
                        print(f"    å‚æ•°æˆåŠŸ: {params} -> {len(page_results)} ä¸ª")
                
                time.sleep(0.5)
                
            except:
                continue
        
        return results
    
    def try_javascript_simulation(self):
        """å°è¯•æ¨¡æ‹ŸJavaScriptè¯·æ±‚"""
        results = []
        
        try:
            # è·å–åŸå§‹é¡µé¢
            response = self.session.get(self.base_url, timeout=30)
            if response.status_code != 200:
                return results
            
            # åˆ†æé¡µé¢ä¸­çš„JavaScriptè¯·æ±‚
            js_urls = re.findall(r'(?:fetch|\.get|\.post)\s*\(\s*["\']([^"\']+)["\']', response.text, re.IGNORECASE)
            
            for url in js_urls:
                try:
                    # è¡¥å…¨ç›¸å¯¹URL
                    if url.startswith('/'):
                        full_url = f"https://www.datacenters.com{url}"
                    elif url.startswith('http'):
                        full_url = url
                    else:
                        continue
                    
                    # æ·»åŠ å¿…è¦çš„å¤´éƒ¨æ¨¡æ‹ŸAJAXè¯·æ±‚
                    ajax_headers = {
                        **self.headers,
                        'X-Requested-With': 'XMLHttpRequest',
                        'Accept': 'application/json, text/javascript, */*; q=0.01',
                    }
                    
                    js_response = self.session.get(full_url, headers=ajax_headers, timeout=20)
                    
                    if js_response.status_code == 200:
                        try:
                            # å°è¯•è§£æä¸ºJSON
                            data = js_response.json()
                            page_results = self.process_json_response(data, f"js_sim_{url.split('/')[-1]}")
                            if page_results:
                                results.extend(page_results)
                                print(f"    JSæ¨¡æ‹ŸæˆåŠŸ: {url} -> {len(page_results)} ä¸ª")
                        except:
                            # HTMLå“åº”
                            page_results = self.extract_from_html(js_response.text, f"js_sim_{url.split('/')[-1]}")
                            if page_results:
                                results.extend(page_results)
                                print(f"    JSæ¨¡æ‹ŸHTMLæˆåŠŸ: {url} -> {len(page_results)} ä¸ª")
                    
                    time.sleep(1)
                    
                except:
                    continue
        
        except Exception as e:
            print(f"    JavaScriptæ¨¡æ‹Ÿå‡ºé”™: {e}")
        
        return results
    
    def extract_from_html(self, content, source):
        """ä»HTMLå†…å®¹ä¸­æå–æ•°æ®ä¸­å¿ƒä¿¡æ¯"""
        results = []
        
        # JavaScriptæ•°æ®æå–
        js_patterns = [
            r'"latitude":\s*([\d\.-]+).*?"longitude":\s*([\d\.-]+).*?"name":\s*"([^"]*)"',
            r'"lat":\s*([\d\.-]+).*?"lng":\s*([\d\.-]+).*?"title":\s*"([^"]*)"',
            r'position:\s*\[([\d\.-]+),\s*([\d\.-]+)\].*?name:\s*"([^"]*)"',
            r'latitude:\s*([\d\.-]+).*?longitude:\s*([\d\.-]+).*?name:\s*["\']([^"\']*)["\']',
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    lat, lng, name = float(match[0]), float(match[1]), match[2].strip()
                    
                    # éªŒè¯åæ ‡æ˜¯å¦åœ¨ä¸Šæµ·èŒƒå›´å†…
                    if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                        results.append({
                            'name': name or f"æ•°æ®ä¸­å¿ƒ_{len(results)+1}",
                            'latitude': lat,
                            'longitude': lng,
                            'location': 'ä¸Šæµ·å¸‚',
                            'source_page': self.extract_page_number(source),
                            'extraction_method': 'HTML_JS',
                            'source': source,
                            'raw_data': {
                                'name': name,
                                'latitude': lat,
                                'longitude': lng
                            }
                        })
                except ValueError:
                    continue
        
        # HTMLç»“æ„åŒ–æ•°æ®æå–
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # æŸ¥æ‰¾å¸¦åæ ‡å±æ€§çš„å…ƒç´ 
            coord_elements = (soup.find_all(attrs={'data-lat': True}) + 
                            soup.find_all(attrs={'data-latitude': True}) +
                            soup.find_all(attrs={'lat': True}))
            
            for elem in coord_elements:
                try:
                    lat = float(elem.get('data-lat') or elem.get('data-latitude') or elem.get('lat'))
                    lng = float(elem.get('data-lng') or elem.get('data-longitude') or elem.get('lng'))
                    
                    name = (elem.get('title') or 
                           elem.get('data-name') or 
                           elem.get_text(strip=True) or 
                           f"æ•°æ®ä¸­å¿ƒ_{len(results)+1}")
                    
                    if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                        results.append({
                            'name': name,
                            'latitude': lat,
                            'longitude': lng,
                            'location': 'ä¸Šæµ·å¸‚',
                            'source_page': self.extract_page_number(source),
                            'extraction_method': 'HTML_DOM',
                            'source': source,
                            'raw_data': {
                                'name': name,
                                'latitude': lat,
                                'longitude': lng
                            }
                        })
                except (ValueError, TypeError):
                    continue
        except:
            pass
        
        return results
    
    def process_json_response(self, data, source):
        """å¤„ç†JSONå“åº”æ•°æ®"""
        results = []
        
        try:
            if isinstance(data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼ŒæŸ¥æ‰¾æ•°æ®æ•°ç»„
                for key, value in data.items():
                    if isinstance(value, list) and value:
                        results.extend(self.process_json_array(value, source))
            elif isinstance(data, list):
                # å¦‚æœæ˜¯æ•°ç»„ï¼Œç›´æ¥å¤„ç†
                results.extend(self.process_json_array(data, source))
        except:
            pass
        
        return results
    
    def process_json_array(self, data_array, source):
        """å¤„ç†JSONæ•°ç»„æ•°æ®"""
        results = []
        
        for item in data_array:
            if not isinstance(item, dict):
                continue
            
            # å°è¯•æå–åæ ‡ä¿¡æ¯
            lat = item.get('latitude') or item.get('lat') or item.get('y')
            lng = item.get('longitude') or item.get('lng') or item.get('x')
            
            if lat is not None and lng is not None:
                try:
                    lat, lng = float(lat), float(lng)
                    
                    # éªŒè¯åæ ‡èŒƒå›´
                    if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                        name = (item.get('name') or 
                               item.get('title') or 
                               item.get('label') or 
                               f"æ•°æ®ä¸­å¿ƒ_{len(results)+1}")
                        
                        results.append({
                            'name': name,
                            'latitude': lat,
                            'longitude': lng,
                            'location': 'ä¸Šæµ·å¸‚',
                            'source_page': self.extract_page_number(source),
                            'extraction_method': 'JSON',
                            'source': source,
                            'raw_data': item
                        })
                except ValueError:
                    continue
        
        return results
    
    def extract_page_number(self, source):
        """ä»æºæ ‡è¯†ä¸­æå–é¡µç """
        page_patterns = [
            r'page_?(\d+)',
            r'p(\d+)',
            r'offset_?(\d+)',
        ]
        
        for pattern in page_patterns:
            match = re.search(pattern, str(source), re.IGNORECASE)
            if match:
                try:
                    if 'offset' in pattern:
                        # åç§»é‡è½¬é¡µç  (å‡è®¾æ¯é¡µ40ä¸ª)
                        return int(match.group(1)) // 40 + 1
                    else:
                        return int(match.group(1))
                except:
                    pass
        
        return 1  # é»˜è®¤ç¬¬1é¡µ
    
    def run_smart_crawl(self):
        """è¿è¡Œæ™ºèƒ½ç¿»é¡µçˆ¬è™«"""
        print("ğŸš€ ä¸Šæµ·æ•°æ®ä¸­å¿ƒæ™ºèƒ½ç¿»é¡µçˆ¬è™«å¯åŠ¨")
        print("ğŸ¯ ç›®æ ‡ï¼šé€šè¿‡å¤šç§æ–¹æ³•è·å–æ‰€æœ‰54ä¸ªæ•°æ®ä¸­å¿ƒ")
        print("="*70)
        
        # 1. åˆ†æé¡µé¢ç»“æ„
        analysis = self.analyze_page_structure()
        if not analysis:
            print("âŒ é¡µé¢åˆ†æå¤±è´¥")
            return []
        
        # 2. å°è¯•ä¸åŒçš„ç¿»é¡µæ–¹æ³•
        all_results, methods = self.try_different_pagination_approaches()
        
        # 3. å»é‡å¤„ç†
        unique_results = self.deduplicate_results(all_results)
        
        # 4. æŒ‰é¡µé¢åˆ†ç»„
        self.group_by_pages(unique_results)
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æ™ºèƒ½çˆ¬å–ç»“æœ:")
        print(f"  æˆåŠŸæ–¹æ³•: {methods}")
        print(f"  åŸå§‹æ•°æ®: {len(all_results)} ä¸ª")
        print(f"  å»é‡åæ•°æ®: {len(unique_results)} ä¸ª")
        print(f"  ç›®æ ‡å®Œæˆåº¦: {len(unique_results)}/54 ({len(unique_results)/54*100:.1f}%)")
        
        # æŒ‰é¡µç»Ÿè®¡
        page_stats = {}
        for result in unique_results:
            page = result['source_page']
            page_stats[page] = page_stats.get(page, 0) + 1
        
        print(f"\næŒ‰é¡µé¢ç»Ÿè®¡:")
        for page in sorted(page_stats.keys()):
            print(f"  ç¬¬ {page} é¡µ: {page_stats[page]} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        return unique_results
    
    def deduplicate_results(self, results):
        """å»é‡ç»“æœ"""
        print(f"ğŸ”„ æ•°æ®å»é‡: {len(results)} -> ", end="")
        
        unique_results = []
        seen_coords = set()
        
        for result in results:
            # åŸºäºåæ ‡å»é‡ï¼ˆç²¾ç¡®åˆ°å°æ•°ç‚¹å5ä½ï¼‰
            coord_key = (round(result['latitude'], 5), round(result['longitude'], 5))
            
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                unique_results.append(result)
        
        print(f"{len(unique_results)}")
        return unique_results
    
    def group_by_pages(self, results):
        """æŒ‰é¡µé¢åˆ†ç»„"""
        for result in results:
            page_key = f"page_{result['source_page']}"
            if page_key not in self.page_data:
                self.page_data[page_key] = []
            self.page_data[page_key].append(result)
    
    def save_results(self, results):
        """ä¿å­˜ç»“æœ"""
        if not results:
            print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ•°æ®
        json_file = f"data/shanghai/ä¸Šæµ·æ•°æ®ä¸­å¿ƒæ™ºèƒ½ç¿»é¡µ_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        
        # ä¿å­˜åˆ†é¡µæ•°æ®
        page_file = f"data/shanghai/åˆ†é¡µæ•°æ®_{timestamp}.json"
        with open(page_file, 'w', encoding='utf-8') as f:
            json.dump(self.page_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ†é¡µæ•°æ®å·²ä¿å­˜: {page_file}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_smart_report(results, timestamp)
    
    def generate_smart_report(self, results, timestamp):
        """ç”Ÿæˆæ™ºèƒ½çˆ¬å–æŠ¥å‘Š"""
        report_file = f"data/shanghai/æ™ºèƒ½ç¿»é¡µæŠ¥å‘Š_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ä¸Šæµ·æ•°æ®ä¸­å¿ƒæ™ºèƒ½ç¿»é¡µçˆ¬å–æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ•°æ®ä¸­å¿ƒ: {len(results)} ä¸ª\n")
            f.write(f"ç›®æ ‡å®Œæˆåº¦: {len(results)}/54 ({len(results)/54*100:.1f}%)\n\n")
            
            # æŒ‰æå–æ–¹æ³•ç»Ÿè®¡
            method_stats = {}
            for result in results:
                method = result.get('extraction_method', 'æœªçŸ¥')
                method_stats[method] = method_stats.get(method, 0) + 1
            
            f.write("æå–æ–¹æ³•ç»Ÿè®¡:\n")
            f.write("-"*30 + "\n")
            for method, count in method_stats.items():
                f.write(f"{method}: {count} ä¸ª\n")
            f.write("\n")
            
            # æŒ‰é¡µé¢ç»Ÿè®¡
            page_stats = {}
            for result in results:
                page = result['source_page']
                page_stats[page] = page_stats.get(page, 0) + 1
            
            f.write("é¡µé¢ç»Ÿè®¡:\n")
            f.write("-"*30 + "\n")
            for page in sorted(page_stats.keys()):
                f.write(f"ç¬¬ {page} é¡µ: {page_stats[page]} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
            f.write("\n")
            
            # è¯¦ç»†æ•°æ®åˆ—è¡¨
            f.write("æ•°æ®ä¸­å¿ƒè¯¦ç»†åˆ—è¡¨:\n")
            f.write("-"*30 + "\n")
            for i, dc in enumerate(results, 1):
                f.write(f"{i:2d}. {dc['name']}\n")
                f.write(f"    åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                f.write(f"    æ¥æºé¡µ: ç¬¬{dc['source_page']}é¡µ\n")
                f.write(f"    æå–æ–¹æ³•: {dc['extraction_method']}\n")
                f.write(f"    æ•°æ®æº: {dc['source']}\n\n")
        
        print(f"âœ… æ™ºèƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¸Šæµ·æ•°æ®ä¸­å¿ƒæ™ºèƒ½ç¿»é¡µçˆ¬è™«")
    print("ğŸ”„ ä¸ä¾èµ–WebDriverçš„JavaScriptç¿»é¡µå¤„ç†")
    print("ğŸ“„ ç›®æ ‡ï¼šç¬¬1é¡µ40ä¸ªï¼Œç¬¬2é¡µ14ä¸ªï¼Œå…±54ä¸ªæ•°æ®ä¸­å¿ƒ")
    
    crawler = SmartPaginationCrawler()
    
    try:
        # è¿è¡Œæ™ºèƒ½çˆ¬è™«
        results = crawler.run_smart_crawl()
        
        if results:
            print(f"\nğŸ‰ æ™ºèƒ½çˆ¬å–å®Œæˆï¼")
            print(f"âœ… è·å–åˆ° {len(results)} ä¸ªä¸Šæµ·æ•°æ®ä¸­å¿ƒ")
            
            # æ˜¾ç¤ºå‰10ä¸ªç»“æœé¢„è§ˆ
            print(f"\nğŸ“‹ å‰10ä¸ªæ•°æ®ä¸­å¿ƒé¢„è§ˆ:")
            for i, dc in enumerate(results[:10], 1):
                print(f"  {i:2d}. {dc['name']} (ç¬¬{dc['source_page']}é¡µ, {dc['extraction_method']})")
            
            if len(results) > 10:
                print(f"     ... è¿˜æœ‰ {len(results) - 10} ä¸ª")
            
            # ä¿å­˜ç»“æœ
            crawler.save_results(results)
            
            # æ£€æŸ¥ç›®æ ‡è¾¾æˆæƒ…å†µ
            if len(results) >= 54:
                print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆï¼è·å–åˆ° {len(results)} ä¸ªæ•°æ®ä¸­å¿ƒ (ç›®æ ‡54ä¸ª)")
            else:
                print(f"\nâš ï¸ è·ç¦»ç›®æ ‡è¿˜å·® {54 - len(results)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                print(f"å»ºè®®ï¼šå¯èƒ½éœ€è¦è¿›ä¸€æ­¥åˆ†æç½‘ç«™çš„JavaScriptç¿»é¡µæœºåˆ¶")
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
