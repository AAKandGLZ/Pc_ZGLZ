#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬è™«
æ”¯æŒåœ°å›¾ç¼©æ”¾ã€ç¿»é¡µå’ŒJavaScriptæ¸²æŸ“ï¼Œè·å–å®Œæ•´çš„æ•°æ®ä¸­å¿ƒä¿¡æ¯
"""

import requests
import re
import json
import pandas as pd
import time
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedShanghaiDataCenterCrawler:
    def __init__(self):
        # ä¸Šæµ·å¸‚çš„ä¸»è¦URL
        self.main_url = "https://www.datacenters.com/locations/china/shanghai/shanghai"
        
        # ä¸Šæµ·å¸‚è¡Œæ”¿åŒºåŸŸåæ ‡èŒƒå›´ï¼ˆæ›´ç²¾ç¡®çš„è¾¹ç•Œï¼‰
        self.shanghai_bounds = {
            'lat_min': 30.6,    # æœ€å—ç«¯ï¼ˆå¥‰è´¤åŒºå—éƒ¨ï¼‰
            'lat_max': 31.9,    # æœ€åŒ—ç«¯ï¼ˆå´‡æ˜å²›åŒ—éƒ¨ï¼‰
            'lng_min': 120.8,   # æœ€è¥¿ç«¯ï¼ˆé’æµ¦åŒºè¥¿éƒ¨ï¼‰
            'lng_max': 122.2    # æœ€ä¸œç«¯ï¼ˆå´‡æ˜å²›ä¸œéƒ¨ï¼‰
        }
        
        # ä¸Šæµ·å¸‚å„åŒºçš„ä¸­å¿ƒåæ ‡ï¼ˆç”¨äºéªŒè¯ï¼‰
        self.shanghai_districts = {
            'é»„æµ¦åŒº': (31.2304, 121.4737),
            'å¾æ±‡åŒº': (31.1880, 121.4370),
            'é•¿å®åŒº': (31.2200, 121.4252),
            'é™å®‰åŒº': (31.2290, 121.4480),
            'æ™®é™€åŒº': (31.2500, 121.3960),
            'è™¹å£åŒº': (31.2650, 121.5050),
            'æ¨æµ¦åŒº': (31.2590, 121.5220),
            'æµ¦ä¸œæ–°åŒº': (31.2450, 121.5670),
            'é—µè¡ŒåŒº': (31.1120, 121.3810),
            'å®å±±åŒº': (31.4040, 121.4890),
            'å˜‰å®šåŒº': (31.3760, 121.2650),
            'é‡‘å±±åŒº': (30.7410, 121.3420),
            'æ¾æ±ŸåŒº': (31.0300, 121.2230),
            'é’æµ¦åŒº': (31.1510, 121.1240),
            'å¥‰è´¤åŒº': (30.9180, 121.4740),
            'å´‡æ˜åŒº': (31.6230, 121.3970)
        }
        
        self.all_results = []
        self.unique_coordinates = set()
        self.driver = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_directories()
    
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        directories = [
            "data/shanghai",
            "reports/shanghai", 
            "html_sources/shanghai"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def setup_driver(self):
        """è®¾ç½®Chrome WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # ç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥æé«˜é€Ÿåº¦
            prefs = {"profile.managed_default_content_settings.images": 2}
            chrome_options.add_experimental_option("prefs", prefs)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            logger.info("âœ… Chrome WebDriver åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            # å›é€€åˆ°requestsæ–¹å¼
            return False
    
    def is_in_shanghai(self, lat, lng):
        """æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨ä¸Šæµ·å¸‚èŒƒå›´å†…"""
        return (self.shanghai_bounds['lat_min'] <= lat <= self.shanghai_bounds['lat_max'] and
                self.shanghai_bounds['lng_min'] <= lng <= self.shanghai_bounds['lng_max'])
    
    def get_district_by_coordinates(self, lat, lng):
        """æ ¹æ®åæ ‡æ¨æ–­æ‰€å±åŒºåŸŸ"""
        min_distance = float('inf')
        closest_district = "ä¸Šæµ·å¸‚"
        
        for district, (d_lat, d_lng) in self.shanghai_districts.items():
            distance = ((lat - d_lat) ** 2 + (lng - d_lng) ** 2) ** 0.5
            if distance < min_distance:
                min_distance = distance
                closest_district = district
        
        # å¦‚æœè·ç¦»å¤ªè¿œï¼Œè¯´æ˜å¯èƒ½ä¸åœ¨ä¸Šæµ·å¸‚å†…
        if min_distance > 0.5:  # çº¦55å…¬é‡Œ
            return None
        
        return closest_district
    
    def extract_data_with_selenium(self):
        """ä½¿ç”¨Seleniumæå–æ•°æ®"""
        try:
            logger.info(f"ğŸŒ æ­£åœ¨è®¿é—®: {self.main_url}")
            self.driver.get(self.main_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(5)
            
            # å°è¯•æ‰¾åˆ°åœ°å›¾å®¹å™¨
            try:
                map_container = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "map-container"))
                )
                logger.info("âœ… æ‰¾åˆ°åœ°å›¾å®¹å™¨")
            except TimeoutException:
                try:
                    map_container = WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.ID, "map"))
                    )
                    logger.info("âœ… æ‰¾åˆ°åœ°å›¾å…ƒç´ ")
                except TimeoutException:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°åœ°å›¾å®¹å™¨ï¼Œç»§ç»­ä½¿ç”¨é¡µé¢æºç ")
            
            # å°è¯•å¤šç§ç¼©æ”¾çº§åˆ«
            zoom_levels = [10, 11, 12, 13, 14, 15]
            all_data = []
            
            for zoom in zoom_levels:
                logger.info(f"ğŸ” å°è¯•ç¼©æ”¾çº§åˆ«: {zoom}")
                
                # å°è¯•è®¾ç½®åœ°å›¾ç¼©æ”¾ï¼ˆå¦‚æœæ”¯æŒï¼‰
                try:
                    self.driver.execute_script(f"""
                        if (window.map && window.map.setZoom) {{
                            window.map.setZoom({zoom});
                        }}
                    """)
                    time.sleep(3)
                except:
                    pass
                
                # æå–å½“å‰é¡µé¢æ•°æ®
                page_data = self.extract_data_from_page_source()
                if page_data:
                    all_data.extend(page_data)
                    logger.info(f"  ğŸ“Š ç¼©æ”¾çº§åˆ« {zoom}: æ‰¾åˆ° {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                
                # å°è¯•ç‚¹å‡»æ›´å¤šæŒ‰é’®æˆ–åŠ è½½æ›´å¤š
                try:
                    load_more_buttons = self.driver.find_elements(By.XPATH, "//button[contains(text(), 'Load More') or contains(text(), 'Show More') or contains(text(), 'åŠ è½½æ›´å¤š') or contains(text(), 'æ˜¾ç¤ºæ›´å¤š')]")
                    for button in load_more_buttons:
                        if button.is_displayed() and button.is_enabled():
                            self.driver.execute_script("arguments[0].click();", button)
                            logger.info("ğŸ”„ ç‚¹å‡»äº†åŠ è½½æ›´å¤šæŒ‰é’®")
                            time.sleep(3)
                            
                            # æå–æ–°åŠ è½½çš„æ•°æ®
                            new_data = self.extract_data_from_page_source()
                            if new_data:
                                all_data.extend(new_data)
                except:
                    pass
                
                # å°è¯•æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
            
            # å°è¯•ç¿»é¡µ
            page_num = 1
            while page_num <= 10:  # æœ€å¤šå°è¯•10é¡µ
                try:
                    # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                    next_buttons = self.driver.find_elements(By.XPATH, "//a[contains(text(), 'Next') or contains(text(), 'ä¸‹ä¸€é¡µ') or contains(@class, 'next')] | //button[contains(text(), 'Next') or contains(text(), 'ä¸‹ä¸€é¡µ')]")
                    
                    clicked = False
                    for button in next_buttons:
                        if button.is_displayed() and button.is_enabled():
                            self.driver.execute_script("arguments[0].click();", button)
                            logger.info(f"ğŸ“„ è·³è½¬åˆ°ç¬¬ {page_num + 1} é¡µ")
                            time.sleep(5)
                            
                            # æå–æ–°é¡µé¢æ•°æ®
                            page_data = self.extract_data_from_page_source()
                            if page_data:
                                all_data.extend(page_data)
                                logger.info(f"  ğŸ“Š ç¬¬ {page_num + 1} é¡µ: æ‰¾åˆ° {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                            clicked = True
                            break
                    
                    if not clicked:
                        break
                    
                    page_num += 1
                    
                except Exception as e:
                    logger.info(f"ğŸ“„ ç¿»é¡µç»“æŸ: {e}")
                    break
            
            # å»é‡å¹¶è¿‡æ»¤
            filtered_data = []
            seen_coords = set()
            
            for data_center in all_data:
                coord_key = (round(data_center['latitude'], 6), round(data_center['longitude'], 6))
                if coord_key not in seen_coords:
                    seen_coords.add(coord_key)
                    filtered_data.append(data_center)
            
            logger.info(f"ğŸ¯ æ€»è®¡æ‰¾åˆ° {len(all_data)} ä¸ªæ•°æ®ä¸­å¿ƒï¼Œå»é‡å {len(filtered_data)} ä¸ª")
            return filtered_data
            
        except Exception as e:
            logger.error(f"âŒ Seleniumæå–æ•°æ®å¤±è´¥: {e}")
            return []
    
    def extract_data_from_page_source(self):
        """ä»å½“å‰é¡µé¢æºç æå–æ•°æ®"""
        try:
            page_source = self.driver.page_source
            return self.extract_data_from_content(page_source)
        except:
            return []
    
    def extract_data_from_content(self, content):
        """ä»å†…å®¹ä¸­æå–æ•°æ®ä¸­å¿ƒä¿¡æ¯"""
        found_data = []
        
        try:
            # å¤šç§åæ ‡æå–æ¨¡å¼
            coordinate_patterns = [
                # JSONæ ¼å¼
                r'"lat":\s*([\d\.\-]+).*?"lng":\s*([\d\.\-]+)',
                r'"latitude":\s*([\d\.\-]+).*?"longitude":\s*([\d\.\-]+)',
                r'"y":\s*([\d\.\-]+).*?"x":\s*([\d\.\-]+)',
                
                # JavaScriptæ ¼å¼
                r'lat:\s*([\d\.\-]+).*?lng:\s*([\d\.\-]+)',
                r'latitude:\s*([\d\.\-]+).*?longitude:\s*([\d\.\-]+)',
                
                # æ•°ç»„æ ¼å¼
                r'\[([\d\.\-]+),\s*([\d\.\-]+)\]',
                
                # å…¶ä»–æ ¼å¼
                r'position:\s*\{\s*lat:\s*([\d\.\-]+).*?lng:\s*([\d\.\-]+)',
            ]
            
            # åç§°æå–æ¨¡å¼
            name_patterns = [
                r'"name":\s*"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC)[^"]*)"',
                r'"title":\s*"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC)[^"]*)"',
                r'"facility_name":\s*"([^"]*)"',
                r'<h[1-6][^>]*>([^<]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC)[^<]*)</h[1-6]>',
                r'"name":\s*"([^"]*(?:ä¸Šæµ·|Shanghai|SH)[^"]*)"',
                r'data-name="([^"]*)"',
                r'title="([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ)[^"]*)"',
            ]
            
            # æ”¶é›†æ‰€æœ‰åæ ‡
            all_coordinates = []
            for pattern in coordinate_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    try:
                        if len(match) == 2:
                            lat, lng = float(match[0]), float(match[1])
                            # éªŒè¯åæ ‡åˆç†æ€§
                            if 20 <= lat <= 50 and 100 <= lng <= 130:
                                all_coordinates.append((lat, lng))
                    except:
                        continue
            
            # æ”¶é›†æ‰€æœ‰åç§°
            all_names = []
            for pattern in name_patterns:
                names = re.findall(pattern, content, re.IGNORECASE)
                all_names.extend([name.strip() for name in names if name.strip()])
            
            # å»é‡åæ ‡
            unique_coords = list(set(all_coordinates))
            logger.info(f"  ğŸ“ æå–åˆ° {len(unique_coords)} ä¸ªå”¯ä¸€åæ ‡")
            
            # è¿‡æ»¤ä¸Šæµ·å¸‚èŒƒå›´å†…çš„åæ ‡
            shanghai_coords = []
            filtered_out = 0
            
            for lat, lng in unique_coords:
                if self.is_in_shanghai(lat, lng):
                    district = self.get_district_by_coordinates(lat, lng)
                    if district:  # ç¡®ä¿èƒ½åŒ¹é…åˆ°åŒºåŸŸ
                        shanghai_coords.append((lat, lng, district))
                    else:
                        filtered_out += 1
                else:
                    filtered_out += 1
            
            logger.info(f"  âœ… ä¸Šæµ·å¸‚å†…åæ ‡: {len(shanghai_coords)} ä¸ª")
            logger.info(f"  âŒ è¿‡æ»¤æ‰å‘¨è¾¹åœ°åŒº: {filtered_out} ä¸ª")
            
            # åˆ›å»ºæ•°æ®ä¸­å¿ƒè®°å½•
            for i, (lat, lng, district) in enumerate(shanghai_coords):
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                coord_key = (round(lat, 6), round(lng, 6))
                if coord_key in self.unique_coordinates:
                    continue
                
                self.unique_coordinates.add(coord_key)
                
                # é€‰æ‹©åç§°
                if i < len(all_names):
                    name = all_names[i]
                else:
                    name = f"{district}æ•°æ®ä¸­å¿ƒ{len(found_data)+1}"
                
                data_center = {
                    'province': 'ä¸Šæµ·å¸‚',
                    'district': district,
                    'latitude': lat,
                    'longitude': lng,
                    'name': name,
                    'source': 'enhanced_crawler',
                    'coordinates': f"{lat},{lng}",
                    'index': len(self.all_results) + len(found_data) + 1,
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                found_data.append(data_center)
            
            return found_data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æå–é”™è¯¯: {e}")
            return []
    
    def crawl_with_requests_fallback(self):
        """ä½¿ç”¨requestsä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        logger.info("ğŸ”„ ä½¿ç”¨requestså¤‡ç”¨æ–¹æ¡ˆ")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        try:
            response = requests.get(self.main_url, headers=headers, timeout=30)
            if response.status_code == 200:
                return self.extract_data_from_content(response.text)
        except Exception as e:
            logger.error(f"âŒ requestså¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
        
        return []
    
    def crawl_all_data(self):
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬è™«")
        logger.info("ğŸ¯ ç›®æ ‡ï¼šè·å–å®Œæ•´çš„ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒä¿¡æ¯ï¼ˆ80+ä¸ªï¼‰")
        logger.info("="*70)
        
        all_data = []
        
        # å°è¯•ä½¿ç”¨Selenium
        if self.setup_driver():
            try:
                selenium_data = self.extract_data_with_selenium()
                all_data.extend(selenium_data)
                logger.info(f"âœ… Seleniumæ–¹å¼è·å–: {len(selenium_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
            except Exception as e:
                logger.error(f"âŒ Seleniumæ–¹å¼å¤±è´¥: {e}")
            finally:
                if self.driver:
                    self.driver.quit()
        
        # å¦‚æœSeleniumæ•°æ®ä¸å¤Ÿï¼Œä½¿ç”¨requestså¤‡ç”¨
        if len(all_data) < 50:  # æœŸæœ›è‡³å°‘50ä¸ª
            logger.info("ğŸ“Š æ•°æ®é‡ä¸è¶³ï¼Œå¯ç”¨requestså¤‡ç”¨æ–¹æ¡ˆ")
            requests_data = self.crawl_with_requests_fallback()
            
            # åˆå¹¶æ•°æ®å¹¶å»é‡
            seen_coords = set()
            for dc in all_data:
                seen_coords.add((round(dc['latitude'], 6), round(dc['longitude'], 6)))
            
            for dc in requests_data:
                coord_key = (round(dc['latitude'], 6), round(dc['longitude'], 6))
                if coord_key not in seen_coords:
                    all_data.append(dc)
                    seen_coords.add(coord_key)
        
        self.all_results = all_data
        logger.info(f"ğŸ‰ æœ€ç»ˆè·å–: {len(self.all_results)} ä¸ªä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒ")
        return self.all_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.all_results:
            logger.error("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç»Ÿè®¡ä¿¡æ¯
        district_stats = {}
        for result in self.all_results:
            district = result['district']
            district_stats[district] = district_stats.get(district, 0) + 1
        
        logger.info(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        total = sum(district_stats.values())
        for district, count in district_stats.items():
            logger.info(f"  {district}: {count} ä¸ªæ•°æ®ä¸­å¿ƒ")
        logger.info(f"  ğŸ“ æ€»è®¡: {total} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # ä¿å­˜CSV
        csv_file = f"data/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåæ ‡_enhanced_{timestamp}.csv"
        try:
            df = pd.DataFrame(self.all_results)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        
        # ä¿å­˜JSON
        json_file = f"data/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåæ ‡_enhanced_{timestamp}.json"
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_enhanced_report(timestamp)
    
    def generate_enhanced_report(self, timestamp):
        """ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š"""
        report_file = f"reports/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåˆ†å¸ƒæŠ¥å‘Š_enhanced_{timestamp}.txt"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåˆ†å¸ƒåˆ†ææŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"çˆ¬å–æ–¹å¼: Selenium + requests åŒé‡ä¿éšœ\n")
                f.write(f"æ€»æ•°æ®ä¸­å¿ƒæ•°é‡: {len(self.all_results)}\n")
                f.write(f"é¢„æœŸç›®æ ‡: 80+ ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                f.write(f"å®Œæˆåº¦: {len(self.all_results)/80*100:.1f}%\n\n")
                
                # åŒºåŸŸåˆ†å¸ƒç»Ÿè®¡
                district_data = {}
                for result in self.all_results:
                    district = result['district']
                    if district not in district_data:
                        district_data[district] = []
                    district_data[district].append(result)
                
                f.write(f"åŒºåŸŸåˆ†å¸ƒç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")
                for district, data in district_data.items():
                    percentage = len(data) / len(self.all_results) * 100
                    f.write(f"{district}: {len(data)} ä¸ª ({percentage:.1f}%)\n")
                
                f.write(f"\nè¯¦ç»†åˆ—è¡¨:\n")
                f.write("-" * 30 + "\n")
                
                for district, data in district_data.items():
                    f.write(f"\n{district} ({len(data)} ä¸ª):\n")
                    for i, dc in enumerate(data, 1):
                        f.write(f"  {i:2d}. {dc['name']}\n")
                        f.write(f"      åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                        f.write(f"      çˆ¬å–æ—¶é—´: {dc['crawl_time']}\n\n")
                
                # æŠ€æœ¯è¯´æ˜
                f.write(f"æŠ€æœ¯æ”¹è¿›è¯´æ˜:\n")
                f.write("-" * 30 + "\n")
                f.write(f"1. ä½¿ç”¨Seleniumå¤„ç†JavaScriptæ¸²æŸ“\n")
                f.write(f"2. æ”¯æŒå¤šç§åœ°å›¾ç¼©æ”¾çº§åˆ«\n")
                f.write(f"3. è‡ªåŠ¨ç¿»é¡µè·å–æ›´å¤šæ•°æ®\n")
                f.write(f"4. æ™ºèƒ½ç‚¹å‡»'åŠ è½½æ›´å¤š'æŒ‰é’®\n")
                f.write(f"5. æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½\n")
                f.write(f"6. requestså¤‡ç”¨æ–¹æ¡ˆä¿éšœ\n")
                f.write(f"7. ç²¾ç¡®çš„ä¸Šæµ·å¸‚è¡Œæ”¿åŒºåŸŸè¿‡æ»¤\n")
                f.write(f"8. åæ ‡å»é‡å’Œæ•°æ®éªŒè¯\n")
            
            logger.info(f"âœ… å¢å¼ºç‰ˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def display_results(self):
        """æ˜¾ç¤ºç»“æœ"""
        if not self.all_results:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬å–ç»“æœï¼ˆå¢å¼ºç‰ˆï¼‰")
        print(f"{'='*80}")
        
        # æŒ‰åŒºåŸŸåˆ†ç»„æ˜¾ç¤º
        district_data = {}
        for result in self.all_results:
            district = result['district']
            if district not in district_data:
                district_data[district] = []
            district_data[district].append(result)
        
        for district, data in district_data.items():
            print(f"\nğŸ“ {district} ({len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ):")
            print("-" * 50)
            for i, dc in enumerate(data, 1):
                print(f"{i:2d}. {dc['name']}")
                print(f"    ğŸ—ºï¸  åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})")
                print(f"    â° æ—¶é—´: {dc['crawl_time']}")
                print()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¢å¼ºç‰ˆä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬è™«å¯åŠ¨")
    print("ğŸ¯ ç›®æ ‡ï¼šè·å–å®Œæ•´çš„ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒä¿¡æ¯ï¼ˆ80+ä¸ªï¼‰")
    print("ğŸ”§ æŠ€æœ¯ï¼šSelenium + åœ°å›¾ç¼©æ”¾ + ç¿»é¡µ + requestså¤‡ç”¨")
    
    crawler = EnhancedShanghaiDataCenterCrawler()
    
    try:
        # å¼€å§‹çˆ¬å–
        results = crawler.crawl_all_data()
        
        if results:
            # æ˜¾ç¤ºç»“æœ
            crawler.display_results()
            
            # ä¿å­˜æ•°æ®
            crawler.save_results()
            
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
            print(f"âœ… æˆåŠŸè·å– {len(results)} ä¸ªä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒ")
            if len(results) >= 80:
                print(f"ğŸ¯ è¾¾åˆ°é¢„æœŸç›®æ ‡ï¼ˆ80+ä¸ªï¼‰")
            else:
                print(f"âš ï¸ æœªè¾¾åˆ°é¢„æœŸç›®æ ‡ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° data/shanghai/ ç›®å½•")
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆåˆ° reports/shanghai/ ç›®å½•")
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            print("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç›®æ ‡ç½‘ç«™ç»“æ„å˜åŒ–")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä»»åŠ¡")
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
