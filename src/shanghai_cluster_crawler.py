#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒèšåˆæ•°æ®è§£æå™¨
ä¸“é—¨å¤„ç†åœ°å›¾èšåˆæ ‡è®°å’Œéšè—æ•°æ®
"""

import requests
import re
import json
import pandas as pd
import time
import os
from datetime import datetime
from urllib.parse import urlencode, parse_qs, urlparse

class ShanghaiClusterCrawler:
    def __init__(self):
        self.base_url = "https://www.datacenters.com/locations/china/shanghai"
        
        # ä»æ‚¨æä¾›çš„èšåˆæ ‡è®°ä¸­æå–çš„åæ ‡ç‚¹
        self.cluster_points = [
            {"lat": 30.765168, "lng": 120.709018, "count": "unknown"},
            {"lat": 30.8556073, "lng": 121.0881835, "count": "unknown"},
            {"lat": 29.88112907600477, "lng": 121.6189134120941, "count": 6},
            {"lat": 30.866923071457734, "lng": 120.135498046875, "count": 2},
            {"lat": 31.247448448494325, "lng": 121.5220758526824, "count": 86}, # ä¸»è¦èšåˆç‚¹ï¼
            {"lat": 31.280839522072, "lng": 120.62127828598022, "count": 5},
            {"lat": 31.348264727716113, "lng": 119.82238054275511, "count": 3},
            {"lat": 31.41220361509835, "lng": 121.04774951934814, "count": 4},
            {"lat": 31.533138994778028, "lng": 120.3129905462265, "count": 4},
            {"lat": 31.703637759244117, "lng": 120.92243671417236, "count": 3},
            {"lat": 31.748243980827056, "lng": 119.94232892990111, "count": 3},
            {"lat": 31.967395368542427, "lng": 120.74172377586363, "count": 6},
        ]
        
        # ä¸Šæµ·å¸‚ç²¾ç¡®è¾¹ç•Œ
        self.shanghai_bounds = {
            'lat_min': 30.6, 'lat_max': 31.9,
            'lng_min': 120.8, 'lng_max': 122.2
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.datacenters.com/locations/china/shanghai',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.all_results = []
        self.cluster_details = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("data/shanghai", exist_ok=True)
        os.makedirs("reports/shanghai", exist_ok=True)
        os.makedirs("html_sources/shanghai", exist_ok=True)
    
    def analyze_cluster_requests(self):
        """åˆ†æèšåˆç‚¹çš„è¯·æ±‚æ¨¡å¼"""
        print("ğŸ” åˆ†æåœ°å›¾èšåˆç‚¹è¯·æ±‚æ¨¡å¼...")
        
        # å°è¯•å¤šç§å¯èƒ½çš„APIç«¯ç‚¹
        api_patterns = [
            "/api/v1/locations/clusters",
            "/api/v2/locations/search", 
            "/api/map/clusters",
            "/api/locations/markers",
            "/locations/clusters",
            "/search/locations",
            "/api/facilities/search",
            "/v1/datacenters/search"
        ]
        
        # ä¸åŒçš„ç¼©æ”¾çº§åˆ«å’Œè¾¹ç•Œå‚æ•°
        zoom_levels = [8, 9, 10, 11, 12, 13, 14, 15, 16]
        
        cluster_data = []
        
        for zoom in zoom_levels:
            print(f"\nğŸ” æµ‹è¯•ç¼©æ”¾çº§åˆ«: {zoom}")
            
            # è®¡ç®—ä¸åŒç¼©æ”¾çº§åˆ«çš„è¾¹ç•Œ
            bounds = self.calculate_bounds_for_zoom(zoom)
            
            for api_pattern in api_patterns:
                try:
                    # æ„å»ºAPI URL
                    api_url = f"https://www.datacenters.com{api_pattern}"
                    
                    # å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
                    param_sets = [
                        {
                            'zoom': zoom,
                            'bounds': f"{bounds['sw_lat']},{bounds['sw_lng']},{bounds['ne_lat']},{bounds['ne_lng']}"
                        },
                        {
                            'z': zoom,
                            'bbox': f"{bounds['sw_lng']},{bounds['sw_lat']},{bounds['ne_lng']},{bounds['ne_lat']}"
                        },
                        {
                            'zoom': zoom,
                            'lat': 31.2304,
                            'lng': 121.4737,
                            'radius': 50
                        },
                        {
                            'location': 'shanghai',
                            'zoom': zoom,
                            'clustering': 'true'
                        }
                    ]
                    
                    for params in param_sets:
                        try:
                            response = self.session.get(api_url, params=params, timeout=10)
                            
                            if response.status_code == 200:
                                try:
                                    data = response.json()
                                    if data and (isinstance(data, list) or 
                                                (isinstance(data, dict) and len(data) > 0)):
                                        
                                        print(f"    âœ… APIæˆåŠŸ: {api_pattern} (zoom={zoom})")
                                        print(f"       æ•°æ®é•¿åº¦: {len(str(data))} å­—ç¬¦")
                                        
                                        # ä¿å­˜APIå“åº”
                                        api_file = f"html_sources/shanghai/cluster_api_{api_pattern.replace('/', '_')}_z{zoom}_{len(cluster_data)}.json"
                                        with open(api_file, 'w', encoding='utf-8') as f:
                                            json.dump(data, f, ensure_ascii=False, indent=2)
                                        
                                        cluster_data.append({
                                            'api': api_pattern,
                                            'zoom': zoom,
                                            'params': params,
                                            'data': data
                                        })
                                        
                                        # è§£ææ•°æ®
                                        parsed = self.parse_cluster_api_data(data)
                                        if parsed:
                                            print(f"       è§£æå‡º: {len(parsed)} ä¸ªæ•°æ®ç‚¹")
                                        
                                except json.JSONDecodeError:
                                    # ä¸æ˜¯JSONæ•°æ®ï¼Œå¯èƒ½æ˜¯HTMLæˆ–å…¶ä»–æ ¼å¼
                                    if len(response.text) > 100:
                                        print(f"    ğŸ“„ éJSONå“åº”: {api_pattern} (zoom={zoom}) - {len(response.text)} å­—ç¬¦")
                                        
                                        # ä¿å­˜éJSONå“åº”
                                        html_file = f"html_sources/shanghai/cluster_response_{api_pattern.replace('/', '_')}_z{zoom}.html"
                                        with open(html_file, 'w', encoding='utf-8') as f:
                                            f.write(response.text)
                            
                            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                            
                        except Exception as e:
                            pass  # é™é»˜å¤„ç†å•ä¸ªè¯·æ±‚é”™è¯¯
                            
                except Exception as e:
                    pass  # é™é»˜å¤„ç†APIé”™è¯¯
        
        return cluster_data
    
    def calculate_bounds_for_zoom(self, zoom):
        """æ ¹æ®ç¼©æ”¾çº§åˆ«è®¡ç®—è¾¹ç•Œ"""
        # ä¸Šæµ·å¸‚ä¸­å¿ƒç‚¹
        center_lat = 31.2304
        center_lng = 121.4737
        
        # æ ¹æ®ç¼©æ”¾çº§åˆ«è®¡ç®—èŒƒå›´
        zoom_factor = 1.0 / (2 ** (zoom - 10))
        
        lat_delta = 0.5 * zoom_factor
        lng_delta = 0.5 * zoom_factor
        
        return {
            'sw_lat': center_lat - lat_delta,
            'sw_lng': center_lng - lng_delta,
            'ne_lat': center_lat + lat_delta,
            'ne_lng': center_lng + lng_delta
        }
    
    def parse_cluster_api_data(self, data):
        """è§£æèšåˆAPIæ•°æ®"""
        results = []
        
        try:
            if isinstance(data, dict):
                # æ£€æŸ¥å¸¸è§çš„æ•°æ®é”®
                for key in ['clusters', 'markers', 'facilities', 'locations', 'data', 'results']:
                    if key in data:
                        results.extend(self.parse_cluster_api_data(data[key]))
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å•ä¸ªæ•°æ®ç‚¹
                if 'lat' in data or 'latitude' in data:
                    parsed = self.parse_single_location(data)
                    if parsed:
                        results.append(parsed)
            
            elif isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        parsed = self.parse_single_location(item)
                        if parsed:
                            results.append(parsed)
        
        except Exception as e:
            pass
        
        return results
    
    def parse_single_location(self, item):
        """è§£æå•ä¸ªä½ç½®æ•°æ®"""
        try:
            lat = lng = None
            name = "Unknown Data Center"
            count = 1
            
            # æå–åæ ‡
            if 'lat' in item and 'lng' in item:
                lat, lng = float(item['lat']), float(item['lng'])
            elif 'latitude' in item and 'longitude' in item:
                lat, lng = float(item['latitude']), float(item['longitude'])
            elif 'y' in item and 'x' in item:
                lat, lng = float(item['y']), float(item['x'])
            
            # æå–åç§°
            for key in ['name', 'title', 'label', 'facility_name']:
                if key in item and item[key]:
                    name = str(item[key]).strip()
                    break
            
            # æå–æ•°é‡ï¼ˆèšåˆç‚¹ï¼‰
            for key in ['count', 'cluster_count', 'facilities_count', 'size']:
                if key in item:
                    try:
                        count = int(item[key])
                        break
                    except:
                        pass
            
            if lat and lng and self.is_in_shanghai_area(lat, lng):
                return {
                    'latitude': lat,
                    'longitude': lng,
                    'name': name,
                    'count': count,
                    'source': 'cluster_api'
                }
        
        except Exception as e:
            pass
        
        return None
    
    def is_in_shanghai_area(self, lat, lng):
        """æ£€æŸ¥æ˜¯å¦åœ¨ä¸Šæµ·å¸‚åŒºåŸŸå†…ï¼ˆå®½æ¾æ£€æŸ¥ï¼‰"""
        return (30.5 <= lat <= 32.0 and 120.5 <= lng <= 122.5)
    
    def crawl_detailed_locations(self):
        """é’ˆå¯¹èšåˆç‚¹çˆ¬å–è¯¦ç»†ä½ç½®"""
        print("\nğŸ¯ é’ˆå¯¹èšåˆç‚¹çˆ¬å–è¯¦ç»†ä½ç½®...")
        
        detailed_results = []
        
        # é‡ç‚¹å…³æ³¨æœ‰86ä¸ªæ•°æ®ä¸­å¿ƒçš„èšåˆç‚¹
        main_cluster = {"lat": 31.247448448494325, "lng": 121.5220758526824, "count": 86}
        
        print(f"ğŸ” é‡ç‚¹åˆ†æä¸»èšåˆç‚¹: ({main_cluster['lat']:.6f}, {main_cluster['lng']:.6f}) - {main_cluster['count']}ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # å›´ç»•ä¸»èšåˆç‚¹è¿›è¡Œç»†åˆ†çˆ¬å–
        detailed_results.extend(self.crawl_cluster_details(main_cluster))
          # å¤„ç†å…¶ä»–èšåˆç‚¹
        for cluster in self.cluster_points:
            count = cluster.get('count', 0)
            if isinstance(count, str):
                try:
                    count = int(count)
                except:
                    count = 0
            
            if count > 2:  # åªå¤„ç†æ•°é‡è¾ƒå¤šçš„èšåˆç‚¹
                print(f"ğŸ” åˆ†æèšåˆç‚¹: ({cluster['lat']:.6f}, {cluster['lng']:.6f}) - {count}ä¸ª")
                cluster_details = self.crawl_cluster_details(cluster)
                detailed_results.extend(cluster_details)
                time.sleep(1)
        
        return detailed_results
    
    def crawl_cluster_details(self, cluster):
        """çˆ¬å–å•ä¸ªèšåˆç‚¹çš„è¯¦ç»†ä¿¡æ¯"""
        results = []
        
        try:
            # å°è¯•å¤šç§è¯¦ç»†æŸ¥è¯¢æ–¹å¼
            detail_strategies = [
                self.crawl_by_radius_zoom,
                self.crawl_by_grid_subdivision,
                self.crawl_by_facility_ids
            ]
            
            for strategy in detail_strategies:
                try:
                    strategy_results = strategy(cluster)
                    if strategy_results:
                        results.extend(strategy_results)
                        print(f"    ç­–ç•¥ {strategy.__name__}: è·å– {len(strategy_results)} ä¸ªç»“æœ")
                except Exception as e:
                    pass
        
        except Exception as e:
            pass
        
        return results
    
    def crawl_by_radius_zoom(self, cluster):
        """é€šè¿‡åŠå¾„ç¼©æ”¾çˆ¬å–"""
        results = []
        
        # å°è¯•ä¸åŒçš„åŠå¾„å’Œç¼©æ”¾çº§åˆ«
        radii = [1, 2, 5, 10, 20, 50]  # å…¬é‡Œ
        zoom_levels = [12, 13, 14, 15, 16, 17, 18]
        
        for radius in radii:
            for zoom in zoom_levels:
                try:
                    # æ„å»ºè¯·æ±‚URL
                    params = {
                        'lat': cluster['lat'],
                        'lng': cluster['lng'],
                        'radius': radius,
                        'zoom': zoom,
                        'format': 'json'
                    }
                    
                    # å°è¯•ä¸åŒçš„ç«¯ç‚¹
                    endpoints = [
                        '/api/locations/nearby',
                        '/api/facilities/radius',
                        '/locations/search',
                        '/api/search'
                    ]
                    
                    for endpoint in endpoints:
                        try:
                            url = f"https://www.datacenters.com{endpoint}"
                            response = self.session.get(url, params=params, timeout=10)
                            
                            if response.status_code == 200:
                                try:
                                    data = response.json()
                                    parsed = self.parse_cluster_api_data(data)
                                    if parsed:
                                        results.extend(parsed)
                                        
                                        # ä¿å­˜æˆåŠŸçš„å“åº”
                                        file_name = f"html_sources/shanghai/radius_{endpoint.replace('/', '_')}_r{radius}_z{zoom}.json"
                                        with open(file_name, 'w', encoding='utf-8') as f:
                                            json.dump(data, f, ensure_ascii=False, indent=2)
                                except:
                                    pass
                        except:
                            pass
                    
                    time.sleep(0.3)
                    
                except Exception as e:
                    pass
        
        return results
    
    def crawl_by_grid_subdivision(self, cluster):
        """é€šè¿‡ç½‘æ ¼ç»†åˆ†çˆ¬å–"""
        results = []
        
        # å›´ç»•èšåˆç‚¹åˆ›å»ºç½‘æ ¼
        grid_size = 0.01  # çº¦1å…¬é‡Œ
        grid_range = 0.05  # æ€»èŒƒå›´çº¦5å…¬é‡Œ
        
        lat_start = cluster['lat'] - grid_range
        lng_start = cluster['lng'] - grid_range
        
        grid_points = []
        lat = lat_start
        while lat <= cluster['lat'] + grid_range:
            lng = lng_start
            while lng <= cluster['lng'] + grid_range:
                grid_points.append({'lat': lat, 'lng': lng})
                lng += grid_size
            lat += grid_size
        
        print(f"    åˆ›å»º {len(grid_points)} ä¸ªç½‘æ ¼ç‚¹è¿›è¡Œç»†åˆ†æŸ¥è¯¢")
        
        for i, point in enumerate(grid_points):
            if i % 10 == 0:  # åªæŸ¥è¯¢éƒ¨åˆ†ç½‘æ ¼ç‚¹ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
                try:
                    # é’ˆå¯¹æ¯ä¸ªç½‘æ ¼ç‚¹è¿›è¡Œç²¾ç¡®æŸ¥è¯¢
                    params = {
                        'lat': point['lat'],
                        'lng': point['lng'],
                        'precision': 'high',
                        'limit': 50
                    }
                    
                    url = f"https://www.datacenters.com/api/locations/point"
                    response = self.session.get(url, params=params, timeout=5)
                    
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            parsed = self.parse_cluster_api_data(data)
                            if parsed:
                                results.extend(parsed)
                        except:
                            pass
                    
                    time.sleep(0.2)
                    
                except Exception as e:
                    pass
        
        return results
    
    def crawl_by_facility_ids(self, cluster):
        """é€šè¿‡è®¾æ–½IDçˆ¬å–"""
        results = []
        
        # å°è¯•çŒœæµ‹è®¾æ–½IDèŒƒå›´ï¼ˆåŸºäºå¸¸è§æ¨¡å¼ï¼‰
        try:
            # åŸºäºåæ ‡ç”Ÿæˆå¯èƒ½çš„ID
            base_id = int((cluster['lat'] * 1000 + cluster['lng'] * 1000) % 10000)
            
            id_ranges = [
                range(base_id, base_id + 100),
                range(1, 1000),  # å¸¸è§çš„IDèŒƒå›´
                range(10000, 10500),
                range(20000, 20200)
            ]
            
            for id_range in id_ranges:
                for facility_id in list(id_range)[::10]:  # æ¯10ä¸ªIDæµ‹è¯•ä¸€ä¸ª
                    try:
                        url = f"https://www.datacenters.com/api/facilities/{facility_id}"
                        response = self.session.get(url, timeout=5)
                        
                        if response.status_code == 200:
                            try:
                                data = response.json()
                                parsed = self.parse_single_location(data)
                                if parsed and self.is_in_shanghai_area(parsed['latitude'], parsed['longitude']):
                                    results.append(parsed)
                            except:
                                pass
                        
                        time.sleep(0.1)
                        
                    except Exception as e:
                        pass
        
        except Exception as e:
            pass
        
        return results
    
    def run_comprehensive_crawl(self):
        """è¿è¡Œç»¼åˆçˆ¬å–"""
        print("ğŸš€ ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒèšåˆæ•°æ®è§£æå™¨å¯åŠ¨")
        print("ğŸ¯ ä¸“é—¨å¤„ç†åœ°å›¾èšåˆæ ‡è®°å’Œéšè—æ•°æ®")
        print("ğŸ“Š ç›®æ ‡ï¼šè§£æ86+ä¸ªæ•°æ®ä¸­å¿ƒèšåˆç‚¹")
        print("="*70)
        
        all_results = []
        
        # 1. åˆ†æèšåˆè¯·æ±‚æ¨¡å¼
        print("\nğŸ” é˜¶æ®µ1: åˆ†æåœ°å›¾èšåˆAPIæ¨¡å¼")
        cluster_api_data = self.analyze_cluster_requests()
        
        if cluster_api_data:
            print(f"âœ… å‘ç° {len(cluster_api_data)} ä¸ªAPIå“åº”")
            for api_response in cluster_api_data:
                parsed = self.parse_cluster_api_data(api_response['data'])
                if parsed:
                    all_results.extend(parsed)
        
        # 2. è¯¦ç»†ä½ç½®çˆ¬å–
        print("\nğŸ” é˜¶æ®µ2: èšåˆç‚¹è¯¦ç»†ä½ç½®çˆ¬å–")
        detailed_results = self.crawl_detailed_locations()
        if detailed_results:
            all_results.extend(detailed_results)
        
        # 3. å»é‡å’ŒéªŒè¯
        unique_results = self.deduplicate_results(all_results)
        
        # 4. æœ€ç»ˆéªŒè¯å’Œåˆ†ç±»
        final_results = []
        for result in unique_results:
            if self.is_in_shanghai_area(result['latitude'], result['longitude']):
                result['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                result['index'] = len(final_results) + 1
                final_results.append(result)
                print(f"  âœ… {len(final_results)}. {result['name']} ({result['latitude']:.6f}, {result['longitude']:.6f}) [æ•°é‡:{result.get('count', 1)}]")
        
        self.all_results = final_results
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š èšåˆè§£æå®Œæˆç»Ÿè®¡:")
        print(f"  âœ… è§£æå‡ºæ•°æ®ä¸­å¿ƒ: {len(self.all_results)} ä¸ª")
        print(f"  ğŸ“ èšåˆç‚¹æ€»æ•°é‡: {sum(r.get('count', 1) for r in self.all_results)}")
        
        return self.all_results
    
    def deduplicate_results(self, results):
        """å»é‡ç»“æœ"""
        unique_results = []
        seen_coords = set()
        
        for result in results:
            coord_key = (round(result['latitude'], 6), round(result['longitude'], 6))
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                unique_results.append(result)
        
        return unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.all_results:
            print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç»Ÿè®¡èšåˆæ•°é‡
        total_count = sum(result.get('count', 1) for result in self.all_results)
        
        print(f"\nğŸ“Š èšåˆæ•°æ®ç»Ÿè®¡:")
        print(f"  è§£æçš„èšåˆç‚¹: {len(self.all_results)} ä¸ª")
        print(f"  ä¼°è®¡æ€»æ•°æ®ä¸­å¿ƒ: {total_count} ä¸ª")
        
        # ä¿å­˜CSV
        csv_file = f"data/shanghai/ä¸Šæµ·å¸‚èšåˆæ•°æ®ä¸­å¿ƒ_{timestamp}.csv"
        try:
            df = pd.DataFrame(self.all_results)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"\nâœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        
        # ä¿å­˜JSON
        json_file = f"data/shanghai/ä¸Šæµ·å¸‚èšåˆæ•°æ®ä¸­å¿ƒ_{timestamp}.json"
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
        
        # ç”Ÿæˆèšåˆåˆ†ææŠ¥å‘Š
        self.generate_cluster_report(timestamp, total_count)
    
    def generate_cluster_report(self, timestamp, total_count):
        """ç”Ÿæˆèšåˆåˆ†ææŠ¥å‘Š"""
        report_file = f"reports/shanghai/ä¸Šæµ·å¸‚èšåˆæ•°æ®åˆ†æ_{timestamp}.txt"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒèšåˆæ•°æ®åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"è§£æèšåˆç‚¹æ•°é‡: {len(self.all_results)}\n")
                f.write(f"ä¼°è®¡æ€»æ•°æ®ä¸­å¿ƒæ•°é‡: {total_count}\n\n")
                
                f.write("èšåˆè§£æç­–ç•¥:\n")
                f.write("-" * 30 + "\n")
                f.write("1. APIæ¨¡å¼åˆ†æï¼šæµ‹è¯•å¤šç§ç¼©æ”¾çº§åˆ«å’Œè¾¹ç•Œå‚æ•°\n")
                f.write("2. åŠå¾„ç¼©æ”¾ï¼šå›´ç»•èšåˆç‚¹è¿›è¡ŒåŠå¾„æŸ¥è¯¢\n")
                f.write("3. ç½‘æ ¼ç»†åˆ†ï¼šå°†èšåˆåŒºåŸŸç»†åˆ†ä¸ºç½‘æ ¼è¿›è¡ŒæŸ¥è¯¢\n")
                f.write("4. è®¾æ–½IDï¼šé€šè¿‡IDèŒƒå›´çŒœæµ‹è¿›è¡ŒæŸ¥è¯¢\n\n")
                
                f.write("å‘ç°çš„èšåˆç‚¹:\n")
                f.write("-" * 30 + "\n")
                for i, result in enumerate(self.all_results, 1):
                    f.write(f"{i:2d}. {result['name']}\n")
                    f.write(f"    åæ ‡: ({result['latitude']:.6f}, {result['longitude']:.6f})\n")
                    f.write(f"    ä¼°è®¡æ•°é‡: {result.get('count', 1)} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                    f.write(f"    æ¥æº: {result.get('source', 'æœªçŸ¥')}\n\n")
                
                f.write("æŠ€æœ¯è¯´æ˜:\n")
                f.write("-" * 30 + "\n")
                f.write("1. èšåˆæ ‡è®°æ˜¾ç¤ºè¯¥åŒºåŸŸæœ‰86ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                f.write("2. ç½‘ç«™ä½¿ç”¨èšåˆç®—æ³•éšè—äº†è¯¦ç»†ä½ç½®\n")
                f.write("3. éœ€è¦ç‰¹å®šçš„APIè°ƒç”¨æˆ–å‚æ•°æ‰èƒ½è·å–å®Œæ•´æ•°æ®\n")
                f.write("4. å»ºè®®ä½¿ç”¨æµè§ˆå™¨å¼€å‘å·¥å…·ç›‘æ§ç½‘ç»œè¯·æ±‚\n")
            
            print(f"âœ… èšåˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒèšåˆæ•°æ®è§£æå™¨")
    print("ğŸ” ä¸“é—¨è§£æåœ°å›¾èšåˆæ ‡è®°ä¸­çš„éšè—æ•°æ®")
    
    crawler = ShanghaiClusterCrawler()
    
    try:
        # è¿è¡Œç»¼åˆçˆ¬å–
        results = crawler.run_comprehensive_crawl()
        
        if results:
            print(f"\nğŸ‰ èšåˆè§£æå®Œæˆï¼")
            print(f"âœ… è§£æå‡º {len(results)} ä¸ªèšåˆç‚¹")
            
            total_estimated = sum(r.get('count', 1) for r in results)
            print(f"ğŸ“Š ä¼°è®¡æ€»æ•°æ®ä¸­å¿ƒ: {total_estimated} ä¸ª")
            
            # ä¿å­˜ç»“æœ
            crawler.save_results()
        else:
            print("âŒ æœªèƒ½è§£æå‡ºèšåˆæ•°æ®")
            print("ğŸ’¡ å»ºè®®ï¼š")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ä½¿ç”¨æµè§ˆå™¨å¼€å‘å·¥å…·ç›‘æ§å®é™…çš„APIè¯·æ±‚")
            print("   3. å¯èƒ½éœ€è¦æ¨¡æ‹ŸçœŸå®çš„ç”¨æˆ·äº¤äº’")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä»»åŠ¡")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
