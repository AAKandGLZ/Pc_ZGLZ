#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå¢å¼ºç‰ˆçˆ¬è™«
æ”¯æŒåˆ†é¡µã€åœ°å›¾æ•°æ®å’ŒAPIè°ƒç”¨è·å–å®Œæ•´æ•°æ®
"""

import requests
import re
import json
import pandas as pd
import time
import os
from datetime import datetime
import urllib.parse
from bs4 import BeautifulSoup

class ShanghaiEnhancedCrawler:
    def __init__(self):
        self.base_url = "https://www.datacenters.com/locations/china/shanghai"
        self.api_base = "https://www.datacenters.com"
        
        # ä¸Šæµ·å¸‚åœ°ç†è¾¹ç•Œï¼ˆç²¾ç¡®èŒƒå›´ï¼‰
        self.shanghai_bounds = {
            'lat_min': 30.6,   # æœ€å—ç«¯
            'lat_max': 31.9,   # æœ€åŒ—ç«¯  
            'lng_min': 120.8,  # æœ€è¥¿ç«¯
            'lng_max': 122.2   # æœ€ä¸œç«¯ï¼ˆåŒ…å«å´‡æ˜å²›ï¼‰
        }
        
        # ä¸Šæµ·å¸‚å„åŒºçš„è¯¦ç»†è¾¹ç•Œ
        self.shanghai_districts = {
            'é»„æµ¦åŒº': {'lat': (31.22, 31.24), 'lng': (121.47, 121.51)},
            'å¾æ±‡åŒº': {'lat': (31.17, 31.22), 'lng': (121.42, 121.47)},
            'é•¿å®åŒº': {'lat': (31.20, 31.24), 'lng': (121.40, 121.45)},
            'é™å®‰åŒº': {'lat': (31.22, 31.26), 'lng': (121.44, 121.47)},
            'æ™®é™€åŒº': {'lat': (31.23, 31.28), 'lng': (121.39, 121.45)},
            'è™¹å£åŒº': {'lat': (31.26, 31.29), 'lng': (121.48, 121.53)},
            'æ¨æµ¦åŒº': {'lat': (31.26, 31.32), 'lng': (121.50, 121.56)},
            'é—µè¡ŒåŒº': {'lat': (31.05, 31.20), 'lng': (121.32, 121.47)},
            'å®å±±åŒº': {'lat': (31.29, 31.51), 'lng': (121.44, 121.53)},
            'å˜‰å®šåŒº': {'lat': (31.35, 31.42), 'lng': (121.20, 121.32)},
            'æµ¦ä¸œæ–°åŒº': {'lat': (30.85, 31.35), 'lng': (121.50, 121.95)},
            'é‡‘å±±åŒº': {'lat': (30.72, 30.92), 'lng': (121.20, 121.47)},
            'æ¾æ±ŸåŒº': {'lat': (30.98, 31.15), 'lng': (121.20, 121.40)},
            'é’æµ¦åŒº': {'lat': (31.10, 31.25), 'lng': (121.05, 121.25)},
            'å¥‰è´¤åŒº': {'lat': (30.78, 30.98), 'lng': (121.35, 121.65)},
            'å´‡æ˜åŒº': {'lat': (31.40, 31.85), 'lng': (121.30, 121.95)},
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.datacenters.com/',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.all_results = []
        self.unique_coordinates = set()
        self.filtered_out = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_directories()
    
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        directories = [
            "data/shanghai",
            "reports/shanghai", 
            "html_sources/shanghai"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def is_in_shanghai(self, lat, lng):
        """ç²¾ç¡®æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨ä¸Šæµ·å¸‚èŒƒå›´å†…"""
        # é¦–å…ˆæ£€æŸ¥å¤§è‡´èŒƒå›´
        if not (self.shanghai_bounds['lat_min'] <= lat <= self.shanghai_bounds['lat_max'] and
                self.shanghai_bounds['lng_min'] <= lng <= self.shanghai_bounds['lng_max']):
            return False, "è¶…å‡ºä¸Šæµ·å¸‚å¤§è‡´èŒƒå›´"
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½•ä¸€ä¸ªåŒºå†…
        for district, bounds in self.shanghai_districts.items():
            if (bounds['lat'][0] <= lat <= bounds['lat'][1] and 
                bounds['lng'][0] <= lng <= bounds['lng'][1]):
                return True, district
        
        # å¦‚æœä¸åœ¨ä»»ä½•åŒºå†…ï¼Œä½†åœ¨å¤§è‡´èŒƒå›´å†…ï¼Œå¯èƒ½æ˜¯è¾¹ç•Œåœ°åŒº
        return True, "ä¸Šæµ·å¸‚è¾¹ç•Œåœ°åŒº"
    
    def crawl_multiple_pages(self):
        """çˆ¬å–å¤šä¸ªé¡µé¢çš„æ•°æ®"""
        print("ğŸ” å¼€å§‹çˆ¬å–ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒï¼ˆå¤šé¡µé¢æ¨¡å¼ï¼‰")
        print("="*70)
        
        all_page_data = []
        
        # å°è¯•çˆ¬å–å¤šä¸ªé¡µé¢
        for page in range(1, 6):  # å°è¯•å‰5é¡µ
            page_url = f"{self.base_url}?page={page}"
            print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            print(f"ğŸ”— URL: {page_url}")
            
            try:
                response = self.session.get(page_url, timeout=30)
                
                if response.status_code != 200:
                    print(f"  âŒ è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                    continue
                
                print(f"  âœ… è¯·æ±‚æˆåŠŸï¼Œé¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
                
                # ä¿å­˜é¡µé¢æºç 
                page_file = f"html_sources/shanghai/page_{page}_source.html"
                with open(page_file, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                # æå–é¡µé¢æ•°æ®
                page_data = self.extract_data_from_page(response.text, f"page_{page}")
                
                if page_data:
                    all_page_data.extend(page_data)
                    print(f"  ğŸ‰ ç¬¬{page}é¡µæå–: {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                else:
                    print(f"  âš ï¸ ç¬¬{page}é¡µæœªæ‰¾åˆ°æ•°æ®")
                    # å¦‚æœè¿ç»­2é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢
                    if page > 2:
                        break
                
                time.sleep(2)  # é¡µé¢é—´éš”
                
            except Exception as e:
                print(f"  âŒ ç¬¬{page}é¡µçˆ¬å–å¤±è´¥: {e}")
        
        return all_page_data
    
    def crawl_map_api_data(self):
        """å°è¯•é€šè¿‡APIè·å–åœ°å›¾æ•°æ®"""
        print("\nğŸ—ºï¸ å°è¯•è·å–åœ°å›¾APIæ•°æ®...")
        
        api_endpoints = [
            "/api/locations/search",
            "/api/map/markers", 
            "/api/datacenters/location",
            "/locations/api/search"
        ]
        
        search_params = [
            {"location": "shanghai", "country": "china"},
            {"city": "shanghai", "region": "china"},
            {"q": "shanghai data center", "type": "datacenter"},
            {"bounds": f"{self.shanghai_bounds['lat_min']},{self.shanghai_bounds['lng_min']},{self.shanghai_bounds['lat_max']},{self.shanghai_bounds['lng_max']}"}
        ]
        
        api_data = []
        
        for endpoint in api_endpoints:
            for params in search_params:
                try:
                    api_url = f"{self.api_base}{endpoint}"
                    print(f"  ğŸ”— å°è¯•API: {endpoint}")
                    
                    # GETè¯·æ±‚
                    response = self.session.get(api_url, params=params, timeout=15)
                    
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if data and isinstance(data, (list, dict)):
                                api_data.append(data)
                                print(f"    âœ… APIå“åº”æˆåŠŸ: {len(str(data))} å­—ç¬¦")
                                
                                # ä¿å­˜APIå“åº”
                                api_file = f"html_sources/shanghai/api_{endpoint.replace('/', '_')}_{len(api_data)}.json"
                                with open(api_file, 'w', encoding='utf-8') as f:
                                    json.dump(data, f, ensure_ascii=False, indent=2)
                        except json.JSONDecodeError:
                            pass
                    
                    time.sleep(1)
                    
                except Exception as e:
                    pass  # é™é»˜å¤„ç†APIé”™è¯¯
        
        # å¤„ç†APIæ•°æ®
        processed_api_data = []
        for data in api_data:
            extracted = self.extract_data_from_api(data)
            if extracted:
                processed_api_data.extend(extracted)
        
        if processed_api_data:
            print(f"  ğŸ‰ APIæ•°æ®æå–: {len(processed_api_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        return processed_api_data
    
    def extract_data_from_api(self, api_data):
        """ä»APIå“åº”ä¸­æå–æ•°æ®"""
        results = []
        
        try:
            if isinstance(api_data, dict):
                # æ£€æŸ¥å¸¸è§çš„æ•°æ®ç»“æ„
                data_keys = ['data', 'results', 'locations', 'markers', 'facilities']
                for key in data_keys:
                    if key in api_data:
                        api_data = api_data[key]
                        break
            
            if isinstance(api_data, list):
                for item in api_data:
                    if isinstance(item, dict):
                        result = self.parse_api_item(item)
                        if result:
                            results.append(result)
            elif isinstance(api_data, dict):
                result = self.parse_api_item(api_data)
                if result:
                    results.append(result)
        
        except Exception as e:
            pass
        
        return results
    
    def parse_api_item(self, item):
        """è§£æå•ä¸ªAPIæ•°æ®é¡¹"""
        try:
            # å°è¯•æå–åæ ‡
            lat = lng = None
            name = "Unknown Data Center"
            
            # åæ ‡æå–çš„å¤šç§æ–¹å¼
            coord_keys = [
                ('latitude', 'longitude'),
                ('lat', 'lng'), 
                ('lat', 'lon'),
                ('y', 'x')
            ]
            
            for lat_key, lng_key in coord_keys:
                if lat_key in item and lng_key in item:
                    try:
                        lat = float(item[lat_key])
                        lng = float(item[lng_key])
                        break
                    except:
                        pass
            
            # åç§°æå–
            name_keys = ['name', 'title', 'facility_name', 'datacenter_name', 'label']
            for key in name_keys:
                if key in item and item[key]:
                    name = str(item[key]).strip()
                    break
            
            if lat and lng:
                # éªŒè¯åæ ‡
                is_valid, location = self.is_in_shanghai(lat, lng)
                if is_valid:
                    return {
                        'latitude': lat,
                        'longitude': lng,
                        'name': name,
                        'source': 'api',
                        'district': location
                    }
        
        except Exception as e:
            pass
        
        return None
    
    def extract_data_from_page(self, content, source_key):
        """ä»é¡µé¢å†…å®¹ä¸­æå–æ•°æ®"""
        found_data = []
        
        try:
            print(f"  æ­£åœ¨è§£æé¡µé¢å†…å®¹...")
            
            # 1. æå–JSON-LDç»“æ„åŒ–æ•°æ®
            json_ld_data = self.extract_json_ld_data(content)
            if json_ld_data:
                found_data.extend(json_ld_data)
                print(f"    JSON-LDæ•°æ®: {len(json_ld_data)} ä¸ª")
            
            # 2. æå–Google Mapsæ ‡è®°æ•°æ®
            map_data = self.extract_map_markers(content)
            if map_data:
                found_data.extend(map_data)
                print(f"    åœ°å›¾æ ‡è®°: {len(map_data)} ä¸ª")
            
            # 3. æå–å¸¸è§„åæ ‡æ¨¡å¼
            coord_data = self.extract_coordinate_patterns(content, source_key)
            if coord_data:
                found_data.extend(coord_data)
                print(f"    åæ ‡æ¨¡å¼: {len(coord_data)} ä¸ª")
            
            # 4. æå–è¡¨æ ¼å’Œåˆ—è¡¨æ•°æ®
            structured_data = self.extract_structured_data(content, source_key)
            if structured_data:
                found_data.extend(structured_data)
                print(f"    ç»“æ„åŒ–æ•°æ®: {len(structured_data)} ä¸ª")
            
            # å»é‡å¤„ç†
            unique_data = self.deduplicate_data(found_data)
            print(f"    å»é‡å: {len(unique_data)} ä¸ªå”¯ä¸€æ•°æ®ä¸­å¿ƒ")
            
            return unique_data
            
        except Exception as e:
            print(f"  æ•°æ®æå–é”™è¯¯: {e}")
            return []
    
    def extract_json_ld_data(self, content):
        """æå–JSON-LDç»“æ„åŒ–æ•°æ®"""
        results = []
        
        try:
            # æŸ¥æ‰¾JSON-LDè„šæœ¬
            json_ld_pattern = r'<script[^>]*type=["\']*application/ld\+json["\']*[^>]*>(.*?)</script>'
            json_scripts = re.findall(json_ld_pattern, content, re.DOTALL | re.IGNORECASE)
            
            for script in json_scripts:
                try:
                    data = json.loads(script.strip())
                    extracted = self.parse_json_ld(data)
                    if extracted:
                        results.extend(extracted)
                except:
                    pass
        
        except Exception as e:
            pass
        
        return results
    
    def parse_json_ld(self, data):
        """è§£æJSON-LDæ•°æ®"""
        results = []
        
        try:
            if isinstance(data, list):
                for item in data:
                    results.extend(self.parse_json_ld(item))
            elif isinstance(data, dict):
                # æŸ¥æ‰¾åœ°å€å’Œåæ ‡ä¿¡æ¯
                if 'geo' in data or 'address' in data or '@type' in data:
                    result = self.parse_location_data(data)
                    if result:
                        results.append(result)
                
                # é€’å½’æŸ¥æ‰¾åµŒå¥—æ•°æ®
                for key, value in data.items():
                    if isinstance(value, (dict, list)):
                        results.extend(self.parse_json_ld(value))
        
        except Exception as e:
            pass
        
        return results
    
    def parse_location_data(self, data):
        """è§£æä½ç½®æ•°æ®"""
        try:
            lat = lng = None
            name = "Unknown Data Center"
            
            # æå–åæ ‡
            if 'geo' in data:
                geo = data['geo']
                if isinstance(geo, dict):
                    lat = geo.get('latitude') or geo.get('lat')
                    lng = geo.get('longitude') or geo.get('lng')
            
            # æå–åç§°
            name_keys = ['name', 'title', '@name']
            for key in name_keys:
                if key in data and data[key]:
                    name = str(data[key]).strip()
                    break
            
            if lat and lng:
                try:
                    lat = float(lat)
                    lng = float(lng)
                    
                    is_valid, district = self.is_in_shanghai(lat, lng)
                    if is_valid:
                        return {
                            'latitude': lat,
                            'longitude': lng,
                            'name': name,
                            'source': 'json_ld',
                            'district': district
                        }
                except:
                    pass
        
        except Exception as e:
            pass
        
        return None
    
    def extract_map_markers(self, content):
        """æå–Google Mapsæ ‡è®°æ•°æ®"""
        results = []
        
        try:
            # æŸ¥æ‰¾gmp-advanced-markeræ ‡è®°
            marker_pattern = r'<gmp-advanced-marker[^>]*position="([^"]*)"[^>]*>.*?</gmp-advanced-marker>'
            markers = re.findall(marker_pattern, content, re.DOTALL)
            
            for i, position in enumerate(markers):
                try:
                    if ',' in position:
                        parts = position.split(',')
                        if len(parts) >= 2:
                            lat = float(parts[0].strip())
                            lng = float(parts[1].strip())
                            
                            is_valid, district = self.is_in_shanghai(lat, lng)
                            if is_valid:
                                results.append({
                                    'latitude': lat,
                                    'longitude': lng,
                                    'name': f"ä¸Šæµ·æ•°æ®ä¸­å¿ƒ_{i+1}",
                                    'source': 'map_marker',
                                    'district': district
                                })
                except:
                    pass
            
            # æŸ¥æ‰¾èšåˆæ ‡è®°ä¸­çš„æ•°é‡ä¿¡æ¯
            cluster_pattern = r'aria-label="(\d+)"[^>]*title="(\d+)"[^>]*position="([^"]*)"'
            clusters = re.findall(cluster_pattern, content)
            
            print(f"    å‘ç°èšåˆæ ‡è®°: {len(clusters)} ä¸ª")
            for label, title, position in clusters:
                print(f"      èšåˆç‚¹: {label}ä¸ªæ•°æ®ä¸­å¿ƒ ä½ç½®:{position}")
        
        except Exception as e:
            pass
        
        return results
    
    def extract_coordinate_patterns(self, content, source_key):
        """æå–åæ ‡æ¨¡å¼"""
        results = []
        
        try:
            # å¤šç§åæ ‡æ¨¡å¼
            patterns = [
                r'"latitude":\s*([\d\.-]+).*?"longitude":\s*([\d\.-]+)',
                r'"lat":\s*([\d\.-]+).*?"lng":\s*([\d\.-]+)',
                r'"lat":\s*([\d\.-]+).*?"lon":\s*([\d\.-]+)',
                r'latitude["\']?\s*:\s*["\']?([\d\.-]+)["\']?.*?longitude["\']?\s*:\s*["\']?([\d\.-]+)["\']?',
                r'coords?\[.*?([\d\.]+),\s*([\d\.]+).*?\]'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    try:
                        lat = float(match[0])
                        lng = float(match[1])
                        
                        is_valid, district = self.is_in_shanghai(lat, lng)
                        if is_valid:
                            coord_key = (round(lat, 6), round(lng, 6))
                            if coord_key not in self.unique_coordinates:
                                self.unique_coordinates.add(coord_key)
                                results.append({
                                    'latitude': lat,
                                    'longitude': lng,
                                    'name': f"ä¸Šæµ·æ•°æ®ä¸­å¿ƒ_{len(results)+1}",
                                    'source': source_key,
                                    'district': district
                                })
                    except:
                        pass
        
        except Exception as e:
            pass
        
        return results
    
    def extract_structured_data(self, content, source_key):
        """æå–ç»“æ„åŒ–æ•°æ®"""
        results = []
        
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # æŸ¥æ‰¾æ•°æ®ä¸­å¿ƒåˆ—è¡¨
            datacenter_selectors = [
                '.datacenter-item', '.facility-item', '.location-item',
                '[data-testid*="datacenter"]', '[data-testid*="facility"]',
                '.result-item', '.search-result'
            ]
            
            for selector in datacenter_selectors:
                items = soup.select(selector)
                for item in items:
                    # æå–åç§°
                    name_elem = item.select_one('h1, h2, h3, h4, .title, .name, .facility-name')
                    name = name_elem.get_text().strip() if name_elem else "æœªçŸ¥æ•°æ®ä¸­å¿ƒ"
                    
                    # æŸ¥æ‰¾åµŒå…¥çš„åæ ‡ä¿¡æ¯
                    item_text = str(item)
                    coord_matches = re.findall(r'([\d\.]+),\s*([\d\.]+)', item_text)
                    
                    for lat_str, lng_str in coord_matches:
                        try:
                            lat = float(lat_str)
                            lng = float(lng_str)
                            
                            if 30 <= lat <= 32 and 120 <= lng <= 122:  # å¤§è‡´èŒƒå›´è¿‡æ»¤
                                is_valid, district = self.is_in_shanghai(lat, lng)
                                if is_valid:
                                    results.append({
                                        'latitude': lat,
                                        'longitude': lng,
                                        'name': name,
                                        'source': source_key,
                                        'district': district
                                    })
                                    break
                        except:
                            pass
        
        except Exception as e:
            pass
        
        return results
    
    def deduplicate_data(self, data):
        """å»é‡æ•°æ®"""
        unique_data = []
        seen_coords = set()
        
        for item in data:
            coord_key = (round(item['latitude'], 6), round(item['longitude'], 6))
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                unique_data.append(item)
        
        return unique_data
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰æ•°æ®æº"""
        print("ğŸŒŸ ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå¢å¼ºç‰ˆçˆ¬è™«å¯åŠ¨")
        print("ğŸ¯ ç›®æ ‡ï¼šè·å–ä¸Šæµ·å¸‚å®Œæ•´çš„æ•°æ®ä¸­å¿ƒåˆ†å¸ƒä¿¡æ¯")
        print("ğŸ“‹ ç­–ç•¥ï¼šå¤šé¡µé¢ + åœ°å›¾API + ç»“æ„åŒ–æ•°æ®")
        print("="*70)
        
        all_data = []
        
        # 1. çˆ¬å–åˆ†é¡µæ•°æ®
        page_data = self.crawl_multiple_pages()
        if page_data:
            all_data.extend(page_data)
            print(f"\nğŸ“„ åˆ†é¡µæ•°æ®æ€»è®¡: {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # 2. å°è¯•APIæ•°æ®
        api_data = self.crawl_map_api_data()
        if api_data:
            all_data.extend(api_data)
            print(f"\nğŸ”— APIæ•°æ®æ€»è®¡: {len(api_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # 3. å»é‡å’Œå¤„ç†
        final_data = self.deduplicate_data(all_data)
        
        # 4. è¯¦ç»†éªŒè¯å’Œåˆ†ç±»
        validated_data = []
        for item in final_data:
            is_valid, district = self.is_in_shanghai(item['latitude'], item['longitude'])
            if is_valid:
                item['district'] = district
                item['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                item['index'] = len(validated_data) + 1
                validated_data.append(item)
                print(f"  âœ… {len(validated_data)}. {item['name']} - {district} ({item['latitude']:.6f}, {item['longitude']:.6f})")
            else:
                self.filtered_out.append({**item, 'filter_reason': district})
        
        self.all_results = validated_data
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡:")
        print(f"  âœ… æœ‰æ•ˆæ•°æ®ä¸­å¿ƒ: {len(self.all_results)} ä¸ª")
        print(f"  âŒ è¿‡æ»¤æ‰: {len(self.filtered_out)} ä¸ª")
        print(f"  ğŸ¯ æ€»çˆ¬å–: {len(final_data)} ä¸ªåŸå§‹æ•°æ®")
        
        return self.all_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.all_results:
            print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰åŒºç»Ÿè®¡
        district_stats = {}
        for result in self.all_results:
            district = result.get('district', 'æœªçŸ¥')
            district_stats[district] = district_stats.get(district, 0) + 1
        
        print(f"\nğŸ“Š ä¸Šæµ·å¸‚å„åŒºåˆ†å¸ƒ:")
        for district, count in sorted(district_stats.items()):
            print(f"  {district}: {count} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # ä¿å­˜CSV
        csv_file = f"data/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå®Œæ•´åˆ†å¸ƒ_{timestamp}.csv"
        try:
            df = pd.DataFrame(self.all_results)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"\nâœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        
        # ä¿å­˜JSON
        json_file = f"data/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå®Œæ•´åˆ†å¸ƒ_{timestamp}.json"
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
        
        # ä¿å­˜è¿‡æ»¤æ•°æ®ï¼ˆç”¨äºåˆ†æï¼‰
        if self.filtered_out:
            filtered_file = f"data/shanghai/è¿‡æ»¤æ•°æ®_{timestamp}.json"
            try:
                with open(filtered_file, 'w', encoding='utf-8') as f:
                    json.dump(self.filtered_out, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“„ è¿‡æ»¤æ•°æ®å·²ä¿å­˜: {filtered_file}")
            except Exception as e:
                print(f"âŒ ä¿å­˜è¿‡æ»¤æ•°æ®å¤±è´¥: {e}")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_detailed_report(timestamp)
    
    def generate_detailed_report(self, timestamp):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report_file = f"reports/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå®Œæ•´åˆ†ææŠ¥å‘Š_{timestamp}.txt"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå®Œæ•´åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»æ•°æ®ä¸­å¿ƒæ•°é‡: {len(self.all_results)}\n")
                f.write(f"è¿‡æ»¤æ‰çš„æ•°æ®: {len(self.filtered_out)}\n\n")
                
                # çˆ¬å–ç­–ç•¥è¯´æ˜
                f.write("çˆ¬å–ç­–ç•¥:\n")
                f.write("-" * 30 + "\n")
                f.write("1. å¤šé¡µé¢çˆ¬å–ï¼šå°è¯•çˆ¬å–å‰5é¡µæ•°æ®\n")
                f.write("2. åœ°å›¾APIè°ƒç”¨ï¼šå°è¯•è·å–åœ°å›¾æ ‡è®°æ•°æ®\n")
                f.write("3. ç»“æ„åŒ–æ•°æ®æå–ï¼šJSON-LDã€è¡¨æ ¼ã€åˆ—è¡¨æ•°æ®\n")
                f.write("4. åœ°ç†éªŒè¯ï¼šç²¾ç¡®çš„ä¸Šæµ·å¸‚åŒºç•ŒéªŒè¯\n\n")
                
                # æ•°æ®æºç»Ÿè®¡
                f.write("æ•°æ®æºç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")
                source_stats = {}
                for result in self.all_results:
                    source = result.get('source', 'æœªçŸ¥')
                    source_stats[source] = source_stats.get(source, 0) + 1
                
                for source, count in source_stats.items():
                    f.write(f"{source}: {count} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                
                # æŒ‰åŒºåˆ†å¸ƒ
                district_data = {}
                for result in self.all_results:
                    district = result.get('district', 'æœªçŸ¥')
                    if district not in district_data:
                        district_data[district] = []
                    district_data[district].append(result)
                
                f.write(f"\nå„åŒºåˆ†å¸ƒç»Ÿè®¡:\n")
                f.write("-" * 20 + "\n")
                for district, data in sorted(district_data.items()):
                    f.write(f"{district}: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                
                f.write(f"\nè¯¦ç»†åˆ—è¡¨:\n")
                f.write("-" * 20 + "\n")
                
                for district, data in sorted(district_data.items()):
                    f.write(f"\n{district} ({len(data)} ä¸ª):\n")
                    for i, dc in enumerate(data, 1):
                        f.write(f"  {i:2d}. {dc['name']}\n")
                        f.write(f"      åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                        f.write(f"      æ¥æº: {dc.get('source', 'æœªçŸ¥')}\n")
                        f.write(f"      æ—¶é—´: {dc.get('crawl_time', 'æœªçŸ¥')}\n\n")
                
                # è¿‡æ»¤æ•°æ®åˆ†æ
                if self.filtered_out:
                    f.write(f"\nè¿‡æ»¤æ•°æ®åˆ†æ:\n")
                    f.write("-" * 20 + "\n")
                    f.write(f"æ€»è®¡è¿‡æ»¤: {len(self.filtered_out)} ä¸ª\n\n")
                    
                    filter_reasons = {}
                    for item in self.filtered_out:
                        reason = item.get('filter_reason', 'æœªçŸ¥')
                        filter_reasons[reason] = filter_reasons.get(reason, 0) + 1
                    
                    for reason, count in filter_reasons.items():
                        f.write(f"{reason}: {count} ä¸ª\n")
                
                # åœ°ç†åˆ†å¸ƒåˆ†æ
                if self.all_results:
                    lats = [r['latitude'] for r in self.all_results]
                    lngs = [r['longitude'] for r in self.all_results]
                    
                    f.write(f"\nåœ°ç†åˆ†å¸ƒåˆ†æ:\n")
                    f.write(f"-" * 20 + "\n")
                    f.write(f"çº¬åº¦èŒƒå›´: {min(lats):.6f} ~ {max(lats):.6f}\n")
                    f.write(f"ç»åº¦èŒƒå›´: {min(lngs):.6f} ~ {max(lngs):.6f}\n")
                    f.write(f"ä¸­å¿ƒç‚¹: ({sum(lats)/len(lats):.6f}, {sum(lngs)/len(lngs):.6f})\n")
                
                # æŠ€æœ¯è¯´æ˜
                f.write(f"\næŠ€æœ¯è¯´æ˜:\n")
                f.write(f"-" * 20 + "\n")
                f.write(f"1. åœ°ç†éªŒè¯ï¼šä½¿ç”¨ä¸Šæµ·å¸‚16ä¸ªåŒºçš„ç²¾ç¡®è¾¹ç•Œ\n")
                f.write(f"2. å»é‡ç­–ç•¥ï¼šåŸºäºåæ ‡ç²¾åº¦åˆ°å°æ•°ç‚¹å6ä½\n")
                f.write(f"3. å¤šæºçˆ¬å–ï¼šç»“åˆåˆ†é¡µã€APIã€ç»“æ„åŒ–æ•°æ®\n")
                f.write(f"4. æ•°æ®å®Œæ•´æ€§ï¼šç›®æ ‡è·å–80+ä¸ªæ•°æ®ä¸­å¿ƒ\n")
            
            print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒå¢å¼ºç‰ˆçˆ¬è™«")
    print("ğŸ¯ é‡‡ç”¨å¤šç§ç­–ç•¥è·å–å®Œæ•´æ•°æ®")
    print("ğŸ“ ç²¾ç¡®çš„åœ°ç†éªŒè¯å’ŒåŒºåŸŸåˆ†ç±»")
    
    crawler = ShanghaiEnhancedCrawler()
    
    try:
        # å¼€å§‹çˆ¬å–
        results = crawler.crawl_all_sources()
        
        if results:
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
            print(f"âœ… æˆåŠŸè·å– {len(results)} ä¸ªä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒ")
            print(f"ğŸ“Š æ•°æ®å·²ä¿å­˜åˆ° data/shanghai/ ç›®å½•")
            print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆåˆ° reports/shanghai/ ç›®å½•")
            
            # ä¿å­˜æ•°æ®
            crawler.save_results()
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            print("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä»»åŠ¡")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
