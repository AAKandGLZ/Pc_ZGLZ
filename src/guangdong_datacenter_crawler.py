#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒçˆ¬è™« - ä¸“é—¨çˆ¬å–å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒåˆ†å¸ƒä¿¡æ¯
"""

import requests
import re
import json
import pandas as pd
import time
import os
from datetime import datetime

class GuangdongDataCenterCrawler:
    def __init__(self):
        # å¹¿ä¸œçœçš„æ‰€æœ‰å¯èƒ½URLå˜ä½“
        self.urls = {
            # å¹¿ä¸œçœçš„å„ç§è‹±æ–‡æ‹¼å†™å˜ä½“
            "å¹¿ä¸œçœ-guangdong-sheng": "https://www.datacenters.com/locations/china/guangdong-sheng",
            "å¹¿ä¸œçœ-guangdong": "https://www.datacenters.com/locations/china/guangdong",
            "å¹¿ä¸œçœ-guang-dong-sheng": "https://www.datacenters.com/locations/china/guang-dong-sheng",
            "å¹¿ä¸œçœ-guang-dong": "https://www.datacenters.com/locations/china/guang-dong",
            "å¹¿ä¸œçœ-kwantung": "https://www.datacenters.com/locations/china/kwantung",
            "å¹¿ä¸œçœ-canton": "https://www.datacenters.com/locations/china/canton",
            
            # ä¸»è¦åŸå¸‚
            "æ·±åœ³-shenzhen": "https://www.datacenters.com/locations/china/shenzhen",
            "å¹¿å·-guangzhou": "https://www.datacenters.com/locations/china/guangzhou",
            "ä¸œè-dongguan": "https://www.datacenters.com/locations/china/dongguan",
            "ä½›å±±-foshan": "https://www.datacenters.com/locations/china/foshan",
            "ç æµ·-zhuhai": "https://www.datacenters.com/locations/china/zhuhai",
            "ä¸­å±±-zhongshan": "https://www.datacenters.com/locations/china/zhongshan",
            "æƒ å·-huizhou": "https://www.datacenters.com/locations/china/huizhou",
        }
        
        # URLä¸çœä»½/åŸå¸‚çš„æ˜ å°„
        self.location_mapping = {
            "å¹¿ä¸œçœ-guangdong-sheng": "å¹¿ä¸œçœ",
            "å¹¿ä¸œçœ-guangdong": "å¹¿ä¸œçœ",
            "å¹¿ä¸œçœ-guang-dong-sheng": "å¹¿ä¸œçœ",
            "å¹¿ä¸œçœ-guang-dong": "å¹¿ä¸œçœ",
            "å¹¿ä¸œçœ-kwantung": "å¹¿ä¸œçœ",
            "å¹¿ä¸œçœ-canton": "å¹¿ä¸œçœ",
            "æ·±åœ³-shenzhen": "æ·±åœ³å¸‚",
            "å¹¿å·-guangzhou": "å¹¿å·å¸‚",
            "ä¸œè-dongguan": "ä¸œèå¸‚",
            "ä½›å±±-foshan": "ä½›å±±å¸‚",
            "ç æµ·-zhuhai": "ç æµ·å¸‚",
            "ä¸­å±±-zhongshan": "ä¸­å±±å¸‚",
            "æƒ å·-huizhou": "æƒ å·å¸‚",
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.all_results = []
        self.unique_coordinates = set()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_directories()
    
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        directories = [
            "data/guangdong",
            "reports/guangdong", 
            "html_sources/guangdong"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def extract_data_from_page(self, content, source_key):
        """ä»é¡µé¢å†…å®¹ä¸­æå–æ•°æ®ä¸­å¿ƒä¿¡æ¯"""
        found_data = []
        
        try:
            print(f"  æ­£åœ¨è§£æé¡µé¢å†…å®¹...")
            
            # æå–åæ ‡ä¿¡æ¯
            latitude_patterns = [
                r'"latitude":\s*([\d\.\-]+)',
                r'"lat":\s*([\d\.\-]+)',
                r'latitude:\s*([\d\.\-]+)',
                r'lat:\s*([\d\.\-]+)'
            ]
            
            longitude_patterns = [
                r'"longitude":\s*([\d\.\-]+)',
                r'"lng":\s*([\d\.\-]+)',
                r'"lon":\s*([\d\.\-]+)',
                r'longitude:\s*([\d\.\-]+)',
                r'lng:\s*([\d\.\-]+)',
                r'lon:\s*([\d\.\-]+)'
            ]
            
            # æ”¶é›†æ‰€æœ‰åæ ‡
            latitudes = []
            longitudes = []
            
            for pattern in latitude_patterns:
                latitudes.extend(re.findall(pattern, content))
            
            for pattern in longitude_patterns:
                longitudes.extend(re.findall(pattern, content))
            
            # å»é‡å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            latitudes = list(set([float(lat) for lat in latitudes if self.is_valid_latitude(lat)]))
            longitudes = list(set([float(lng) for lng in longitudes if self.is_valid_longitude(lng)]))
            
            print(f"  æ‰¾åˆ°åæ ‡: {len(latitudes)} ä¸ªçº¬åº¦, {len(longitudes)} ä¸ªç»åº¦")
            
            # æå–æ•°æ®ä¸­å¿ƒåç§°
            name_patterns = [
                r'"name":\s*"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—)[^"]*)"',
                r'"title":\s*"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—)[^"]*)"',
                r'"facility_name":\s*"([^"]*)"',
                r'<h[1-6][^>]*>([^<]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—)[^<]*)</h[1-6]>',
                # å¹¿ä¸œç›¸å…³çš„ç‰¹å®šæ¨¡å¼
                r'"name":\s*"([^"]*(?:å¹¿ä¸œ|æ·±åœ³|å¹¿å·|ä¸œè|ä½›å±±|ç æµ·|ä¸­å±±|æƒ å·|Guangdong|Shenzhen|Guangzhou)[^"]*)"',
                r'"title":\s*"([^"]*(?:å¹¿ä¸œ|æ·±åœ³|å¹¿å·|ä¸œè|ä½›å±±|ç æµ·|ä¸­å±±|æƒ å·|Guangdong|Shenzhen|Guangzhou)[^"]*)"',
            ]
            
            all_names = []
            for pattern in name_patterns:
                names = re.findall(pattern, content, re.IGNORECASE)
                all_names.extend(names)
            
            # æ¸…ç†å’Œå»é‡åç§°
            unique_names = self.clean_and_dedupe_names(all_names)
            print(f"  æ‰¾åˆ°åç§°: {len(unique_names)} ä¸ªå”¯ä¸€åç§°")
            
            # ç»„åˆåæ ‡æ•°æ®
            location = self.location_mapping[source_key]
            
            # åˆ›å»ºåæ ‡å¯¹
            coordinates_pairs = []
            
            # å¦‚æœçº¬åº¦å’Œç»åº¦æ•°é‡ç›¸ç­‰ï¼Œç›´æ¥é…å¯¹
            if len(latitudes) == len(longitudes):
                coordinates_pairs = list(zip(latitudes, longitudes))
            else:
                # å¦‚æœæ•°é‡ä¸ç­‰ï¼Œå°è¯•æ™ºèƒ½åŒ¹é…
                coordinates_pairs = self.smart_coordinate_matching(latitudes, longitudes)
            
            # åˆ›å»ºæ•°æ®ä¸­å¿ƒè®°å½•
            for i, (lat, lng) in enumerate(coordinates_pairs):
                # éªŒè¯åæ ‡æ˜¯å¦åœ¨å¹¿ä¸œçœèŒƒå›´å†…
                if not self.is_in_guangdong_region(lat, lng):
                    print(f"  è·³è¿‡éå¹¿ä¸œçœåæ ‡: ({lat}, {lng})")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                coord_key = (round(lat, 6), round(lng, 6))
                if coord_key in self.unique_coordinates:
                    print(f"  è·³è¿‡é‡å¤åæ ‡: ({lat}, {lng})")
                    continue
                
                self.unique_coordinates.add(coord_key)
                
                # é€‰æ‹©åç§°
                if i < len(unique_names):
                    name = unique_names[i]
                else:
                    name = f"{location}æ•°æ®ä¸­å¿ƒ{i+1}"
                
                data_center = {
                    'province': 'å¹¿ä¸œçœ',
                    'city': location,
                    'latitude': lat,
                    'longitude': lng,
                    'name': name,
                    'source': source_key,
                    'coordinates': f"{lat},{lng}",
                    'index': len(self.all_results) + len(found_data) + 1,
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                found_data.append(data_center)
                print(f"  {len(found_data)}. {name} - ({lat:.6f}, {lng:.6f})")
        
        except Exception as e:
            print(f"  æ•°æ®æå–é”™è¯¯: {e}")
        
        return found_data
    
    def is_valid_latitude(self, lat_str):
        """éªŒè¯çº¬åº¦æ˜¯å¦æœ‰æ•ˆ"""
        try:
            lat = float(lat_str)
            return 20.0 <= lat <= 25.5  # å¹¿ä¸œçœçº¬åº¦èŒƒå›´
        except:
            return False
    
    def is_valid_longitude(self, lng_str):
        """éªŒè¯ç»åº¦æ˜¯å¦æœ‰æ•ˆ"""
        try:
            lng = float(lng_str)
            return 109.0 <= lng <= 117.5  # å¹¿ä¸œçœç»åº¦èŒƒå›´
        except:
            return False
    
    def is_in_guangdong_region(self, lat, lng):
        """æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨å¹¿ä¸œçœèŒƒå›´å†…"""
        # å¹¿ä¸œçœå¤§è‡´çš„åœ°ç†è¾¹ç•Œ
        return (20.0 <= lat <= 25.5) and (109.0 <= lng <= 117.5)
    
    def clean_and_dedupe_names(self, names):
        """æ¸…ç†å’Œå»é‡åç§°"""
        unique_names = []
        seen_names = set()
        
        for name in names:
            clean_name = name.strip()
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„åç§°
            if 3 <= len(clean_name) <= 100:
                # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒï¼Œä½†ä¿æŒåŸå§‹å¤§å°å†™
                name_lower = clean_name.lower()
                if name_lower not in seen_names:
                    unique_names.append(clean_name)
                    seen_names.add(name_lower)
        
        return unique_names
    
    def smart_coordinate_matching(self, latitudes, longitudes):
        """æ™ºèƒ½åæ ‡åŒ¹é…"""
        coordinates_pairs = []
        
        # å¦‚æœå…¶ä¸­ä¸€ä¸ªåˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not latitudes or not longitudes:
            return coordinates_pairs
        
        # ä½¿ç”¨è¾ƒçŸ­çš„åˆ—è¡¨é•¿åº¦
        min_length = min(len(latitudes), len(longitudes))
        
        for i in range(min_length):
            coordinates_pairs.append((latitudes[i], longitudes[i]))
        
        return coordinates_pairs
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰æ•°æ®æº"""
        print("ğŸŒŸ å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒçˆ¬è™«å¯åŠ¨")
        print("ğŸ¯ ç›®æ ‡ï¼šçˆ¬å–å¹¿ä¸œçœåŠä¸»è¦åŸå¸‚çš„æ•°æ®ä¸­å¿ƒåˆ†å¸ƒä¿¡æ¯")
        print("="*70)
        
        success_count = 0
        failed_count = 0
        
        for source_key, url in self.urls.items():
            print(f"\nğŸ” æ­£åœ¨çˆ¬å–: {source_key}")
            print(f"ğŸ“ URL: {url}")
            print("-" * 50)
            
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                
                if response.status_code != 200:
                    print(f"  âŒ è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                    failed_count += 1
                    continue
                
                print(f"  âœ… è¯·æ±‚æˆåŠŸï¼Œé¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
                
                # æå–æ•°æ®
                page_data = self.extract_data_from_page(response.text, source_key)
                
                if page_data:
                    self.all_results.extend(page_data)
                    print(f"  ğŸ‰ æˆåŠŸæå–: {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                    success_count += 1
                else:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°æ•°æ®")
                
                # ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•
                filename = f"html_sources/guangdong/{source_key.replace('-', '_')}_source.html"
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"  ğŸ’¾ é¡µé¢æºç å·²ä¿å­˜: {filename}")
                
                # è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«å°IP
                time.sleep(3)
                
            except requests.exceptions.Timeout:
                print(f"  â±ï¸ è¯·æ±‚è¶…æ—¶")
                failed_count += 1
            except requests.exceptions.RequestException as e:
                print(f"  âŒ ç½‘ç»œé”™è¯¯: {e}")
                failed_count += 1
            except Exception as e:
                print(f"  âŒ æœªçŸ¥é”™è¯¯: {e}")
                failed_count += 1
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  âœ… æˆåŠŸ: {success_count} ä¸ªæ•°æ®æº")
        print(f"  âŒ å¤±è´¥: {failed_count} ä¸ªæ•°æ®æº")
        print(f"  ğŸ¯ æ€»è®¡æ‰¾åˆ°: {len(self.all_results)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        return self.all_results
    
    def save_results(self):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        if not self.all_results:
            print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æŒ‰åŸå¸‚ç»Ÿè®¡
        city_stats = {}
        for result in self.all_results:
            city = result['city']
            city_stats[city] = city_stats.get(city, 0) + 1
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        total = sum(city_stats.values())
        for city, count in city_stats.items():
            print(f"  {city}: {count} ä¸ªæ•°æ®ä¸­å¿ƒ")
        print(f"  ğŸ“ æ€»è®¡: {total} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_file = f"data/guangdong/å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒåæ ‡_{timestamp}.csv"
        try:
            df = pd.DataFrame(self.all_results)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_file = f"data/guangdong/å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒåæ ‡_{timestamp}.json"
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(timestamp)
    
    def generate_report(self, timestamp):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report_file = f"reports/guangdong/å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒåˆ†å¸ƒæŠ¥å‘Š_{timestamp}.txt"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒåˆ†å¸ƒåˆ†ææŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»æ•°æ®ä¸­å¿ƒæ•°é‡: {len(self.all_results)}\n\n")
                
                # æ•°æ®æºç»Ÿè®¡
                f.write("æ•°æ®æºç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")
                source_stats = {}
                for result in self.all_results:
                    source = result['source']
                    source_stats[source] = source_stats.get(source, 0) + 1
                
                for source, count in source_stats.items():
                    f.write(f"{source}: {count} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                
                # æŒ‰åŸå¸‚åˆ†ç»„
                city_data = {}
                for result in self.all_results:
                    city = result['city']
                    if city not in city_data:
                        city_data[city] = []
                    city_data[city].append(result)
                
                f.write(f"\nåŸå¸‚åˆ†å¸ƒç»Ÿè®¡:\n")
                f.write("-" * 20 + "\n")
                for city, data in city_data.items():
                    f.write(f"{city}: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                
                f.write(f"\nè¯¦ç»†åˆ—è¡¨:\n")
                f.write("-" * 20 + "\n")
                
                for city, data in city_data.items():
                    f.write(f"\n{city} ({len(data)} ä¸ª):\n")
                    for i, dc in enumerate(data, 1):
                        f.write(f"  {i:2d}. {dc['name']}\n")
                        f.write(f"      åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                        f.write(f"      æ¥æº: {dc['source']}\n")
                        f.write(f"      çˆ¬å–æ—¶é—´: {dc['crawl_time']}\n\n")
                
                # åœ°ç†åˆ†å¸ƒåˆ†æ
                if self.all_results:
                    lats = [r['latitude'] for r in self.all_results]
                    lngs = [r['longitude'] for r in self.all_results]
                    
                    f.write(f"åœ°ç†åˆ†å¸ƒåˆ†æ:\n")
                    f.write(f"-" * 20 + "\n")
                    f.write(f"çº¬åº¦èŒƒå›´: {min(lats):.6f} ~ {max(lats):.6f}\n")
                    f.write(f"ç»åº¦èŒƒå›´: {min(lngs):.6f} ~ {max(lngs):.6f}\n")
                    f.write(f"ä¸­å¿ƒç‚¹: ({sum(lats)/len(lats):.6f}, {sum(lngs)/len(lngs):.6f})\n")
                
                # æŠ€æœ¯è¯´æ˜
                f.write(f"\næŠ€æœ¯è¯´æ˜:\n")
                f.write(f"-" * 20 + "\n")
                f.write(f"1. çˆ¬å–èŒƒå›´ï¼šå¹¿ä¸œçœåŠå…¶ä¸»è¦åŸå¸‚\n")
                f.write(f"2. åæ ‡éªŒè¯ï¼šä»…ä¿ç•™å¹¿ä¸œçœåœ°ç†èŒƒå›´å†…çš„åæ ‡\n")
                f.write(f"3. å»é‡ç­–ç•¥ï¼šåŸºäºåæ ‡ç²¾åº¦åˆ°å°æ•°ç‚¹å6ä½\n")
                f.write(f"4. æ•°æ®æ¥æºï¼šdatacenters.comç½‘ç«™\n")
                f.write(f"5. çˆ¬å–ç­–ç•¥ï¼šå¤šURLå˜ä½“ï¼ŒåŒ…å«çœçº§å’Œå¸‚çº§æŸ¥è¯¢\n")
            
            print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def display_results(self):
        """æ˜¾ç¤ºçˆ¬å–ç»“æœ"""
        if not self.all_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒçˆ¬å–ç»“æœ")
        print(f"{'='*80}")
        
        # æŒ‰åŸå¸‚åˆ†ç»„æ˜¾ç¤º
        city_data = {}
        for result in self.all_results:
            city = result['city']
            if city not in city_data:
                city_data[city] = []
            city_data[city].append(result)
        
        for city, data in city_data.items():
            print(f"\nğŸ“ {city} ({len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ):")
            print("-" * 50)
            for i, dc in enumerate(data, 1):
                print(f"{i:2d}. {dc['name']}")
                print(f"    ğŸ—ºï¸  åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})")
                print(f"    ğŸ“Š æ¥æº: {dc['source']}")
                print(f"    â° æ—¶é—´: {dc['crawl_time']}")
                print()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒçˆ¬è™«å¯åŠ¨")
    print("ğŸ¯ ä¸“ä¸šçˆ¬å–å¹¿ä¸œçœæ•°æ®ä¸­å¿ƒåˆ†å¸ƒä¿¡æ¯")
    print("ğŸ“ åŒ…å«ï¼šæ·±åœ³ã€å¹¿å·ã€ä¸œèã€ä½›å±±ã€ç æµ·ã€ä¸­å±±ã€æƒ å·ç­‰ä¸»è¦åŸå¸‚")
    
    crawler = GuangdongDataCenterCrawler()
    
    try:
        # å¼€å§‹çˆ¬å–
        results = crawler.crawl_all_sources()
        
        if results:
            # æ˜¾ç¤ºç»“æœ
            crawler.display_results()
            
            # ä¿å­˜æ•°æ®
            crawler.save_results()
            
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
            print(f"âœ… æˆåŠŸè·å– {len(results)} ä¸ªå¹¿ä¸œçœæ•°æ®ä¸­å¿ƒ")
            print(f"âœ… æ¶µç›–å¹¿ä¸œçœä¸»è¦åŸå¸‚")
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° data/guangdong/ ç›®å½•")
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆåˆ° reports/guangdong/ ç›®å½•")
            
            print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"  - CSVæ ¼å¼æ•°æ®æ–‡ä»¶")
            print(f"  - JSONæ ¼å¼æ•°æ®æ–‡ä»¶") 
            print(f"  - è¯¦ç»†åˆ†ææŠ¥å‘Š")
            print(f"  - HTMLæºç æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰")
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            print("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä»»åŠ¡")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
