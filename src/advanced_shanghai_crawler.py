#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬è™«
ä½¿ç”¨å¤šç§ç­–ç•¥è·å–å®Œæ•´çš„ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒä¿¡æ¯ï¼Œä¸ä¾èµ–Selenium
"""

import requests
import re
import json
import pandas as pd
import time
import os
from datetime import datetime
import urllib.parse
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedShanghaiDataCenterCrawler:
    def __init__(self):
        # å¤šç§URLç­–ç•¥
        self.base_urls = [
            "https://www.datacenters.com/locations/china/shanghai/shanghai",
            "https://www.datacenters.com/locations/china/shanghai",
            "https://www.datacenters.com/api/locations/china/shanghai/shanghai",
            "https://www.datacenters.com/search?location=shanghai",
            "https://www.datacenters.com/search?location=ä¸Šæµ·",
        ]
        
        # åˆ†é¡µURLæ¨¡å¼
        self.pagination_patterns = [
            "?page={page}",
            "?offset={offset}",
            "&page={page}",
            "&offset={offset}",
            "?start={start}",
            "&start={start}",
        ]
        
        # ä¸Šæµ·å¸‚å„åŒºçš„URL
        self.district_urls = [
            "https://www.datacenters.com/locations/china/shanghai/pudong",
            "https://www.datacenters.com/locations/china/shanghai/huangpu", 
            "https://www.datacenters.com/locations/china/shanghai/xuhui",
            "https://www.datacenters.com/locations/china/shanghai/changning",
            "https://www.datacenters.com/locations/china/shanghai/jingan",
            "https://www.datacenters.com/locations/china/shanghai/putuo",
            "https://www.datacenters.com/locations/china/shanghai/hongkou",
            "https://www.datacenters.com/locations/china/shanghai/yangpu",
            "https://www.datacenters.com/locations/china/shanghai/minhang",
            "https://www.datacenters.com/locations/china/shanghai/baoshan",
            "https://www.datacenters.com/locations/china/shanghai/jiading",
            "https://www.datacenters.com/locations/china/shanghai/jinshan",
            "https://www.datacenters.com/locations/china/shanghai/songjiang",
            "https://www.datacenters.com/locations/china/shanghai/qingpu",
            "https://www.datacenters.com/locations/china/shanghai/fengxian",
            "https://www.datacenters.com/locations/china/shanghai/chongming",
        ]
        
        # APIå¯èƒ½çš„ç«¯ç‚¹
        self.api_endpoints = [
            "https://www.datacenters.com/api/locations",
            "https://www.datacenters.com/api/search",
            "https://www.datacenters.com/api/facilities",
            "https://api.datacenters.com/locations",
            "https://api.datacenters.com/facilities",
        ]
        
        # ä¸Šæµ·å¸‚ç²¾ç¡®è¾¹ç•Œ
        self.shanghai_bounds = {
            'lat_min': 30.6,    
            'lat_max': 31.9,    
            'lng_min': 120.8,   
            'lng_max': 122.2    
        }
        
        # ä¸Šæµ·å¸‚å„åŒºåŸŸä¸­å¿ƒç‚¹
        self.shanghai_districts = {
            'é»„æµ¦åŒº': (31.2304, 121.4737),
            'å¾æ±‡åŒº': (31.1880, 121.4370),
            'é•¿å®åŒº': (31.2200, 121.4252),
            'é™å®‰åŒº': (31.2290, 121.4480),
            'æ™®é™€åŒº': (31.2500, 121.3960),
            'è™¹å£åŒº': (31.2650, 121.5050),
            'æ¨æµ¦åŒº': (31.2590, 121.5220),
            'æµ¦ä¸œæ–°åŒº': (31.2450, 121.5670),
            'é—µè¡ŒåŒº': (31.1120, 121.3810),
            'å®å±±åŒº': (31.4040, 121.4890),
            'å˜‰å®šåŒº': (31.3760, 121.2650),
            'é‡‘å±±åŒº': (30.7410, 121.3420),
            'æ¾æ±ŸåŒº': (31.0300, 121.2230),
            'é’æµ¦åŒº': (31.1510, 121.1240),
            'å¥‰è´¤åŒº': (30.9180, 121.4740),
            'å´‡æ˜åŒº': (31.6230, 121.3970)
        }
        
        self.all_results = []
        self.unique_coordinates = set()
        self.session = requests.Session()
        
        # è®¾ç½®é«˜çº§è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_directories()
    
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        directories = [
            "data/shanghai",
            "reports/shanghai", 
            "html_sources/shanghai"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def is_in_shanghai(self, lat, lng):
        """æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨ä¸Šæµ·å¸‚èŒƒå›´å†…"""
        return (self.shanghai_bounds['lat_min'] <= lat <= self.shanghai_bounds['lat_max'] and
                self.shanghai_bounds['lng_min'] <= lng <= self.shanghai_bounds['lng_max'])
    
    def get_district_by_coordinates(self, lat, lng):
        """æ ¹æ®åæ ‡æ¨æ–­æ‰€å±åŒºåŸŸ"""
        min_distance = float('inf')
        closest_district = "ä¸Šæµ·å¸‚"
        
        for district, (d_lat, d_lng) in self.shanghai_districts.items():
            distance = ((lat - d_lat) ** 2 + (lng - d_lng) ** 2) ** 0.5
            if distance < min_distance:
                min_distance = distance
                closest_district = district
        
        # å¦‚æœè·ç¦»å¤ªè¿œï¼Œè¯´æ˜å¯èƒ½ä¸åœ¨ä¸Šæµ·å¸‚å†…
        if min_distance > 0.5:  # çº¦55å…¬é‡Œ
            return None
        
        return closest_district
    
    def fetch_url_with_retry(self, url, max_retries=3):
        """å¸¦é‡è¯•çš„URLè·å–"""
        for attempt in range(max_retries):
            try:
                # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°IP
                time.sleep(1 + attempt * 0.5)
                
                response = self.session.get(url, timeout=30)
                if response.status_code == 200:
                    return response.text
                elif response.status_code == 404:
                    return None  # é¡µé¢ä¸å­˜åœ¨
                else:
                    logger.warning(f"âš ï¸ URL {url} è¿”å›çŠ¶æ€ç : {response.status_code}")
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return None
        
        return None
    
    def extract_coordinates_advanced(self, content):
        """é«˜çº§åæ ‡æå–"""
        coordinates = []
        
        # å¤šç§åæ ‡æ ¼å¼æ¨¡å¼
        patterns = [
            # JSONæ ¼å¼ - æœ€å¸¸è§
            r'"lat":\s*([\d\.\-]+).*?"lng":\s*([\d\.\-]+)',
            r'"latitude":\s*([\d\.\-]+).*?"longitude":\s*([\d\.\-]+)',
            r'"y":\s*([\d\.\-]+).*?"x":\s*([\d\.\-]+)',
            
            # JavaScriptå¯¹è±¡æ ¼å¼
            r'lat:\s*([\d\.\-]+).*?lng:\s*([\d\.\-]+)',
            r'latitude:\s*([\d\.\-]+).*?longitude:\s*([\d\.\-]+)',
            
            # æ•°ç»„æ ¼å¼
            r'\[([\d\.\-]+),\s*([\d\.\-]+)\]',
            r'new google\.maps\.LatLng\(([\d\.\-]+),\s*([\d\.\-]+)\)',
            
            # GeoJSONæ ¼å¼
            r'"coordinates":\s*\[([\d\.\-]+),\s*([\d\.\-]+)\]',
            
            # å…¶ä»–å¯èƒ½æ ¼å¼
            r'position:\s*\{\s*lat:\s*([\d\.\-]+).*?lng:\s*([\d\.\-]+)',
            r'center:\s*\[([\d\.\-]+),\s*([\d\.\-]+)\]',
            r'latlng:\s*\[([\d\.\-]+),\s*([\d\.\-]+)\]',
            
            # HTML dataå±æ€§
            r'data-lat="([\d\.\-]+)".*?data-lng="([\d\.\-]+)"',
            r'data-latitude="([\d\.\-]+)".*?data-longitude="([\d\.\-]+)"',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                try:
                    if len(match) == 2:
                        lat, lng = float(match[0]), float(match[1])
                        # åŸºæœ¬åˆç†æ€§æ£€æŸ¥
                        if 20 <= lat <= 50 and 100 <= lng <= 130:
                            coordinates.append((lat, lng))
                except ValueError:
                    continue
        
        return coordinates
    
    def extract_names_advanced(self, content):
        """é«˜çº§åç§°æå–"""
        names = []
        
        patterns = [
            # JSONä¸­çš„åç§°
            r'"name":\s*"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC|Center)[^"]*)"',
            r'"title":\s*"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC|Center)[^"]*)"',
            r'"facility_name":\s*"([^"]*)"',
            r'"company_name":\s*"([^"]*)"',
            
            # HTMLæ ‡ç­¾ä¸­çš„åç§°
            r'<h[1-6][^>]*>([^<]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC|Center)[^<]*)</h[1-6]>',
            r'<title>([^<]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿|äº‘è®¡ç®—|Cloud|DC|Center)[^<]*)</title>',
            
            # åŒ…å«ä¸Šæµ·å…³é”®è¯çš„
            r'"name":\s*"([^"]*(?:ä¸Šæµ·|Shanghai|SH)[^"]*)"',
            r'"title":\s*"([^"]*(?:ä¸Šæµ·|Shanghai|SH)[^"]*)"',
            
            # HTMLå±æ€§ä¸­çš„åç§°
            r'data-name="([^"]*)"',
            r'title="([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ)[^"]*)"',
            r'alt="([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ)[^"]*)"',
            
            # é“¾æ¥æ–‡æœ¬
            r'<a[^>]*>([^<]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿)[^<]*)</a>',
            
            # è¡¨æ ¼ä¸­çš„åç§°
            r'<td[^>]*>([^<]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ|æœºæˆ¿)[^<]*)</td>',
        ]
        
        for pattern in patterns:
            found_names = re.findall(pattern, content, re.IGNORECASE)
            names.extend([name.strip() for name in found_names if name.strip()])
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_names = []
        seen = set()
        for name in names:
            clean_name = name.strip()
            if (clean_name not in seen and 
                3 <= len(clean_name) <= 100 and
                not clean_name.isdigit()):
                unique_names.append(clean_name)
                seen.add(clean_name)
        
        return unique_names
    
    def extract_data_from_content(self, content, source_url):
        """ä»å†…å®¹ä¸­æå–æ•°æ®"""
        found_data = []
        
        try:
            # æå–åæ ‡å’Œåç§°
            coordinates = self.extract_coordinates_advanced(content)
            names = self.extract_names_advanced(content)
            
            logger.info(f"  ğŸ“ å‘ç° {len(coordinates)} ä¸ªåæ ‡ï¼Œ{len(names)} ä¸ªåç§°")
            
            # è¿‡æ»¤ä¸Šæµ·å¸‚èŒƒå›´å†…çš„åæ ‡
            shanghai_coords = []
            for lat, lng in coordinates:
                if self.is_in_shanghai(lat, lng):
                    district = self.get_district_by_coordinates(lat, lng)
                    if district:
                        shanghai_coords.append((lat, lng, district))
            
            logger.info(f"  âœ… ä¸Šæµ·å¸‚å†…åæ ‡: {len(shanghai_coords)} ä¸ª")
            
            # åˆ›å»ºæ•°æ®ä¸­å¿ƒè®°å½•
            for i, (lat, lng, district) in enumerate(shanghai_coords):
                # æ£€æŸ¥é‡å¤
                coord_key = (round(lat, 6), round(lng, 6))
                if coord_key in self.unique_coordinates:
                    continue
                
                self.unique_coordinates.add(coord_key)
                
                # åˆ†é…åç§°
                if i < len(names):
                    name = names[i]
                else:
                    name = f"{district}æ•°æ®ä¸­å¿ƒ{len(found_data)+1}"
                
                data_center = {
                    'province': 'ä¸Šæµ·å¸‚',
                    'district': district,
                    'latitude': lat,
                    'longitude': lng,
                    'name': name,
                    'source': source_url,
                    'coordinates': f"{lat},{lng}",
                    'index': len(self.all_results) + len(found_data) + 1,
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                found_data.append(data_center)
            
            return found_data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æå–é”™è¯¯: {e}")
            return []
    
    def crawl_url_with_pagination(self, base_url, max_pages=20):
        """çˆ¬å–URLåŠå…¶åˆ†é¡µ"""
        all_data = []
        
        # å…ˆçˆ¬å–åŸºç¡€URL
        logger.info(f"ğŸ” çˆ¬å–åŸºç¡€URL: {base_url}")
        content = self.fetch_url_with_retry(base_url)
        if content:
            data = self.extract_data_from_content(content, base_url)
            all_data.extend(data)
            logger.info(f"  ğŸ“Š åŸºç¡€é¡µé¢: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # å°è¯•åˆ†é¡µ
        for page in range(2, max_pages + 1):
            found_page = False
            
            for pattern in self.pagination_patterns:
                if '{page}' in pattern:
                    paginated_url = base_url + pattern.format(page=page)
                elif '{offset}' in pattern:
                    offset = (page - 1) * 20  # å‡è®¾æ¯é¡µ20ä¸ª
                    paginated_url = base_url + pattern.format(offset=offset)
                elif '{start}' in pattern:
                    start = (page - 1) * 20
                    paginated_url = base_url + pattern.format(start=start)
                else:
                    continue
                
                logger.info(f"ğŸ” å°è¯•åˆ†é¡µ: {paginated_url}")
                content = self.fetch_url_with_retry(paginated_url)
                
                if content:
                    data = self.extract_data_from_content(content, paginated_url)
                    if data:
                        all_data.extend(data)
                        logger.info(f"  ğŸ“Š ç¬¬{page}é¡µ: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                        found_page = True
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåˆ†é¡µï¼Œåœæ­¢å°è¯•
            if not found_page:
                logger.info(f"ğŸ“„ åˆ†é¡µç»“æŸåœ¨ç¬¬{page-1}é¡µ")
                break
        
        return all_data
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰æ•°æ®æº"""
        logger.info("ğŸš€ å¯åŠ¨é«˜çº§ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬è™«")
        logger.info("ğŸ¯ ç›®æ ‡ï¼šé€šè¿‡å¤šç§ç­–ç•¥è·å–80+ä¸ªæ•°æ®ä¸­å¿ƒ")
        logger.info("="*70)
        
        all_data = []
        
        # 1. çˆ¬å–ä¸»è¦URLåŠå…¶åˆ†é¡µ
        logger.info("\nğŸ“‹ é˜¶æ®µ1: çˆ¬å–ä¸»è¦URLåŠåˆ†é¡µ")
        logger.info("-" * 40)
        for url in self.base_urls:
            try:
                data = self.crawl_url_with_pagination(url)
                all_data.extend(data)
                logger.info(f"âœ… {url}: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
            except Exception as e:
                logger.error(f"âŒ {url}: {e}")
        
        # 2. çˆ¬å–å„åŒºURL
        logger.info(f"\nğŸ“‹ é˜¶æ®µ2: çˆ¬å–å„åŒºURL")
        logger.info("-" * 40)
        
        def crawl_district_url(url):
            try:
                data = self.crawl_url_with_pagination(url, max_pages=5)
                return url, data
            except Exception as e:
                logger.error(f"âŒ {url}: {e}")
                return url, []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘çˆ¬å–
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_url = {executor.submit(crawl_district_url, url): url for url in self.district_urls}
            
            for future in as_completed(future_to_url):
                url, data = future.result()
                if data:
                    all_data.extend(data)
                    logger.info(f"âœ… {url}: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # 3. å°è¯•APIç«¯ç‚¹
        logger.info(f"\nğŸ“‹ é˜¶æ®µ3: å°è¯•APIç«¯ç‚¹")
        logger.info("-" * 40)
        for api_url in self.api_endpoints:
            try:
                # æ„é€ APIè¯·æ±‚
                api_params = [
                    {"location": "shanghai"},
                    {"city": "shanghai"},
                    {"region": "shanghai"},
                    {"q": "shanghai data center"},
                ]
                
                for params in api_params:
                    full_url = f"{api_url}?" + urllib.parse.urlencode(params)
                    content = self.fetch_url_with_retry(full_url)
                    if content:
                        data = self.extract_data_from_content(content, full_url)
                        if data:
                            all_data.extend(data)
                            logger.info(f"âœ… API {full_url}: {len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                            
            except Exception as e:
                logger.error(f"âŒ API {api_url}: {e}")
        
        # å»é‡å¤„ç†
        unique_data = []
        seen_coords = set()
        
        for dc in all_data:
            coord_key = (round(dc['latitude'], 6), round(dc['longitude'], 6))
            if coord_key not in seen_coords:
                unique_data.append(dc)
                seen_coords.add(coord_key)
        
        self.all_results = unique_data
        logger.info(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(all_data)} ä¸ª")
        logger.info(f"âœ… å»é‡å: {len(unique_data)} ä¸ªä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒ")
        
        return unique_data
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.all_results:
            logger.error("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç»Ÿè®¡ä¿¡æ¯
        district_stats = {}
        for result in self.all_results:
            district = result['district']
            district_stats[district] = district_stats.get(district, 0) + 1
        
        logger.info(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        total = sum(district_stats.values())
        for district, count in district_stats.items():
            logger.info(f"  {district}: {count} ä¸ªæ•°æ®ä¸­å¿ƒ")
        logger.info(f"  ğŸ“ æ€»è®¡: {total} ä¸ªæ•°æ®ä¸­å¿ƒ")
        
        # ä¿å­˜CSV
        csv_file = f"data/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåæ ‡_advanced_{timestamp}.csv"
        try:
            df = pd.DataFrame(self.all_results)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        
        # ä¿å­˜JSON
        json_file = f"data/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåæ ‡_advanced_{timestamp}.json"
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜JSONå¤±è´¥: {e}")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_report(timestamp)
    
    def generate_report(self, timestamp):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report_file = f"reports/shanghai/ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåˆ†å¸ƒæŠ¥å‘Š_advanced_{timestamp}.txt"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒåˆ†å¸ƒåˆ†ææŠ¥å‘Šï¼ˆé«˜çº§ç‰ˆï¼‰\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"çˆ¬å–ç­–ç•¥: å¤šURL + åˆ†é¡µ + API + å¹¶å‘çˆ¬å–\n")
                f.write(f"æ€»æ•°æ®ä¸­å¿ƒæ•°é‡: {len(self.all_results)}\n")
                f.write(f"é¢„æœŸç›®æ ‡: 80+ ä¸ªæ•°æ®ä¸­å¿ƒ\n")
                
                if len(self.all_results) >= 80:
                    f.write(f"âœ… ç›®æ ‡è¾¾æˆåº¦: {len(self.all_results)/80*100:.1f}%\n\n")
                else:
                    f.write(f"âš ï¸  ç›®æ ‡è¾¾æˆåº¦: {len(self.all_results)/80*100:.1f}%ï¼ˆæœªè¾¾åˆ°é¢„æœŸï¼‰\n\n")
                
                # æ•°æ®æºç»Ÿè®¡
                source_stats = {}
                for result in self.all_results:
                    source = result['source']
                    source_stats[source] = source_stats.get(source, 0) + 1
                
                f.write(f"æ•°æ®æºåˆ†å¸ƒ:\n")
                f.write("-" * 30 + "\n")
                for source, count in source_stats.items():
                    f.write(f"{source}: {count} ä¸ª\n")
                
                # åŒºåŸŸåˆ†å¸ƒç»Ÿè®¡
                district_data = {}
                for result in self.all_results:
                    district = result['district']
                    if district not in district_data:
                        district_data[district] = []
                    district_data[district].append(result)
                
                f.write(f"\nåŒºåŸŸåˆ†å¸ƒç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")
                for district, data in district_data.items():
                    percentage = len(data) / len(self.all_results) * 100
                    f.write(f"{district}: {len(data)} ä¸ª ({percentage:.1f}%)\n")
                
                f.write(f"\nè¯¦ç»†åˆ—è¡¨:\n")
                f.write("-" * 30 + "\n")
                
                for district, data in district_data.items():
                    f.write(f"\n{district} ({len(data)} ä¸ª):\n")
                    for i, dc in enumerate(data, 1):
                        f.write(f"  {i:2d}. {dc['name']}\n")
                        f.write(f"      åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                        f.write(f"      æ¥æº: {dc['source']}\n")
                        f.write(f"      æ—¶é—´: {dc['crawl_time']}\n\n")
                
                # åœ°ç†åˆ†å¸ƒåˆ†æ
                if self.all_results:
                    lats = [r['latitude'] for r in self.all_results]
                    lngs = [r['longitude'] for r in self.all_results]
                    
                    f.write(f"åœ°ç†åˆ†å¸ƒåˆ†æ:\n")
                    f.write("-" * 30 + "\n")
                    f.write(f"çº¬åº¦èŒƒå›´: {min(lats):.6f} ~ {max(lats):.6f}\n")
                    f.write(f"ç»åº¦èŒƒå›´: {min(lngs):.6f} ~ {max(lngs):.6f}\n")
                    f.write(f"ä¸­å¿ƒç‚¹: ({sum(lats)/len(lats):.6f}, {sum(lngs)/len(lngs):.6f})\n")
                
                # æŠ€æœ¯ç‰¹ç‚¹è¯´æ˜
                f.write(f"\né«˜çº§çˆ¬å–æŠ€æœ¯:\n")
                f.write("-" * 30 + "\n")
                f.write(f"1. å¤šURLç­–ç•¥: ä¸»ç«™ + å„åŒº + APIç«¯ç‚¹\n")
                f.write(f"2. æ™ºèƒ½åˆ†é¡µ: è‡ªåŠ¨æ£€æµ‹å¹¶çˆ¬å–æ‰€æœ‰åˆ†é¡µ\n")
                f.write(f"3. å¹¶å‘çˆ¬å–: å¤šçº¿ç¨‹æé«˜æ•ˆç‡\n")
                f.write(f"4. é«˜çº§æ­£åˆ™: å¤šç§åæ ‡å’Œåç§°æå–æ¨¡å¼\n")
                f.write(f"5. ç²¾ç¡®è¿‡æ»¤: ä¸¥æ ¼çš„ä¸Šæµ·å¸‚è¡Œæ”¿åŒºåŸŸéªŒè¯\n")
                f.write(f"6. æ™ºèƒ½å»é‡: åŸºäºåæ ‡ç²¾åº¦å»é‡\n")
                f.write(f"7. å®¹é”™å¤„ç†: ç½‘ç»œé‡è¯•å’Œå¼‚å¸¸æ¢å¤\n")
                f.write(f"8. åŠ¨æ€UA: æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º\n")
            
            logger.info(f"âœ… é«˜çº§æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def display_results(self):
        """æ˜¾ç¤ºç»“æœæ‘˜è¦"""
        if not self.all_results:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬å–ç»“æœï¼ˆé«˜çº§ç‰ˆï¼‰")
        print(f"{'='*80}")
        
        # æŒ‰åŒºåŸŸåˆ†ç»„æ˜¾ç¤ºå‰å‡ ä¸ª
        district_data = {}
        for result in self.all_results:
            district = result['district']
            if district not in district_data:
                district_data[district] = []
            district_data[district].append(result)
        
        for district, data in district_data.items():
            print(f"\nğŸ“ {district} ({len(data)} ä¸ªæ•°æ®ä¸­å¿ƒ):")
            print("-" * 50)
            # åªæ˜¾ç¤ºå‰3ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡é•¿
            for i, dc in enumerate(data[:3], 1):
                print(f"{i:2d}. {dc['name']}")
                print(f"    ğŸ—ºï¸  åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})")
            if len(data) > 3:
                print(f"    ... è¿˜æœ‰ {len(data) - 3} ä¸ªæ•°æ®ä¸­å¿ƒ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§ä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒçˆ¬è™«å¯åŠ¨")
    print("ğŸ¯ ç›®æ ‡ï¼šé€šè¿‡å¤šç§ç­–ç•¥è·å–80+ä¸ªæ•°æ®ä¸­å¿ƒ")
    print("ğŸ”§ æŠ€æœ¯ï¼šå¤šURL + åˆ†é¡µ + API + å¹¶å‘çˆ¬å–")
    
    crawler = AdvancedShanghaiDataCenterCrawler()
    
    try:
        # å¼€å§‹çˆ¬å–
        results = crawler.crawl_all_sources()
        
        if results:
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            crawler.display_results()
            
            # ä¿å­˜æ•°æ®
            crawler.save_results()
            
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
            print(f"âœ… æˆåŠŸè·å– {len(results)} ä¸ªä¸Šæµ·å¸‚æ•°æ®ä¸­å¿ƒ")
            
            if len(results) >= 80:
                print(f"ğŸ¯ è¾¾åˆ°é¢„æœŸç›®æ ‡ï¼ˆ80+ä¸ªï¼‰ï¼")
            elif len(results) >= 50:
                print(f"âœ… è·å¾—äº†ç›¸å½“æ•°é‡çš„æ•°æ®ä¸­å¿ƒ")
            else:
                print(f"âš ï¸ æ•°æ®é‡å¯èƒ½ä¸è¶³ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥")
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° data/shanghai/ ç›®å½•")
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆåˆ° reports/shanghai/ ç›®å½•")
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            print("ğŸ’¡ å¯èƒ½åŸå› ï¼šç½‘ç»œé—®é¢˜ã€ç½‘ç«™ç»“æ„å˜åŒ–æˆ–åçˆ¬æœºåˆ¶")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä»»åŠ¡")
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
