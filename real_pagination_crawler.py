#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SeleniumçœŸå®ç¿»é¡µçˆ¬è™«
æ¨¡æ‹Ÿç‚¹å‡»é¡µé¢æŒ‰é’®è¿›è¡Œç¿»é¡µï¼Œè·å–æ¯é¡µçœŸå®æ•°æ®
"""

import json
import time
import os
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup

class RealPaginationCrawler:
    def __init__(self):
        self.base_url = "https://www.datacenters.com/locations/china/shanghai"
        
        # è®¾ç½®Chromeé€‰é¡¹
        self.chrome_options = Options()
        # å…ˆä¸ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼Œæ–¹ä¾¿è°ƒè¯•
        # self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--window-size=1920,1080')
        self.chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = None
        self.wait = None
        self.all_datacenters = []
        self.page_data = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("data/shanghai", exist_ok=True)
        os.makedirs("html_sources/shanghai", exist_ok=True)
    
    def setup_driver(self):
        """åˆå§‹åŒ–WebDriver"""
        try:
            # å°è¯•åˆå§‹åŒ–Chrome WebDriver
            print("ğŸ”§ åˆå§‹åŒ–Chrome WebDriver...")
            self.driver = webdriver.Chrome(options=self.chrome_options)
            self.driver.implicitly_wait(10)
            self.wait = WebDriverWait(self.driver, 15)
            print("âœ… WebDriveråˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ WebDriveråˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriver")
            return False
    
    def load_page_and_wait(self):
        """åŠ è½½é¡µé¢å¹¶ç­‰å¾…å†…å®¹åŠ è½½å®Œæˆ"""
        print("ğŸ“„ åŠ è½½ä¸»é¡µé¢...")
        
        try:
            self.driver.get(self.base_url)
            print("  é¡µé¢å·²åŠ è½½ï¼Œç­‰å¾…å†…å®¹æ¸²æŸ“...")
            
            # ç­‰å¾…é¡µé¢ä¸»è¦å†…å®¹åŠ è½½
            time.sleep(5)
            
            # ç­‰å¾…åœ°å›¾æˆ–æ•°æ®åŠ è½½å®Œæˆ
            try:
                # ç­‰å¾…æŸä¸ªå…³é”®å…ƒç´ å‡ºç°
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                print("  âœ… é¡µé¢å†…å®¹åŠ è½½å®Œæˆ")
            except TimeoutException:
                print("  âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            return True
            
        except Exception as e:
            print(f"  âŒ é¡µé¢åŠ è½½å¤±è´¥: {e}")
            return False
    
    def find_pagination_elements(self):
        """æ‰¾åˆ°é¡µé¢ä¸Šçš„ç¿»é¡µå…ƒç´ """
        print("ğŸ” æŸ¥æ‰¾ç¿»é¡µå…ƒç´ ...")
        
        pagination_info = {
            'total_pages': 0,
            'page_buttons': [],
            'next_button': None,
            'current_page': 1
        }
        
        # ä¿å­˜å½“å‰é¡µé¢HTMLç”¨äºåˆ†æ
        with open("html_sources/shanghai/pagination_analysis.html", 'w', encoding='utf-8') as f:
            f.write(self.driver.page_source)
        
        # æŸ¥æ‰¾é¡µé¢æŒ‰é’®çš„å¤šç§æ–¹å¼
        page_button_patterns = [
            # ç›´æ¥æŸ¥æ‰¾åŒ…å«æ•°å­—çš„æŒ‰é’®æˆ–é“¾æ¥
            "//a[contains(@class, 'page') and text()='2']",
            "//button[contains(@class, 'page') and text()='2']",
            "//a[contains(@href, 'page') and text()='2']",
            
            # æŸ¥æ‰¾åˆ†é¡µå®¹å™¨ä¸­çš„æ•°å­—æŒ‰é’®
            "//*[contains(@class, 'pagination')]//a[text()='2']",
            "//*[contains(@class, 'pager')]//a[text()='2']",
            "//*[contains(@class, 'page')]//a[text()='2']",
            
            # é€šè¿‡onclickæˆ–å…¶ä»–å±æ€§æŸ¥æ‰¾
            "//a[contains(@onclick, 'page') and text()='2']",
            "//button[contains(@onclick, 'page') and text()='2']",
            
            # é€šè¿‡dataå±æ€§æŸ¥æ‰¾
            "//a[@data-page='2']",
            "//button[@data-page='2']",
            "//*[@data-page='2']",
        ]
        
        # å°è¯•æ‰¾åˆ°ç¬¬2é¡µæŒ‰é’®ï¼ˆä½œä¸ºæµ‹è¯•ï¼‰
        second_page_button = None
        for pattern in page_button_patterns:
            try:
                elements = self.driver.find_elements(By.XPATH, pattern)
                if elements:
                    second_page_button = elements[0]
                    print(f"  âœ… æ‰¾åˆ°ç¬¬2é¡µæŒ‰é’®: {pattern}")
                    break
            except:
                continue
        
        if second_page_button:
            # å¦‚æœæ‰¾åˆ°äº†ç¬¬2é¡µæŒ‰é’®ï¼Œå°è¯•æ‰¾æ‰€æœ‰é¡µé¢æŒ‰é’®
            try:
                # åŸºäºæ‰¾åˆ°çš„æ¨¡å¼ï¼ŒæŸ¥æ‰¾æ‰€æœ‰é¡µé¢æŒ‰é’®
                all_page_patterns = [
                    "//a[contains(@class, 'page') and string-length(text())=1 and number(text())=text()]",
                    "//*[contains(@class, 'pagination')]//a[string-length(text())=1 and number(text())=text()]",
                    "//a[@data-page]",
                    "//*[@data-page and string-length(@data-page)=1]"
                ]
                
                for pattern in all_page_patterns:
                    try:
                        elements = self.driver.find_elements(By.XPATH, pattern)
                        if elements:
                            page_numbers = []
                            for elem in elements:
                                try:
                                    # å°è¯•ä»æ–‡æœ¬è·å–é¡µç 
                                    text = elem.text.strip()
                                    if text.isdigit():
                                        page_numbers.append(int(text))
                                    
                                    # å°è¯•ä»data-pageå±æ€§è·å–é¡µç 
                                    data_page = elem.get_attribute('data-page')
                                    if data_page and data_page.isdigit():
                                        page_numbers.append(int(data_page))
                                except:
                                    continue
                              if page_numbers:
                                pagination_info['total_pages'] = max(page_numbers)
                                pagination_info['page_buttons'] = elements
                                print(f"  âœ… æ€»é¡µæ•°: {pagination_info['total_pages']}")
                                print(f"  âœ… æ‰¾åˆ°é¡µé¢æŒ‰é’®: {len(elements)} ä¸ª")
                                break
                    except Exception:
                        continue
        
        # æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"æŒ‰é’®
        next_button_patterns = [
            "//a[contains(text(), 'Next')]",
            "//button[contains(text(), 'Next')]",
            "//a[contains(@class, 'next')]",
            "//button[contains(@class, 'next')]",
            "//*[contains(@onclick, 'next')]",
            "//a[contains(text(), 'ä¸‹ä¸€é¡µ')]",
            "//a[contains(text(), '>')]"
        ]
        
        for pattern in next_button_patterns:
            try:
                element = self.driver.find_element(By.XPATH, pattern)
                if element:
                    pagination_info['next_button'] = element
                    print(f"  âœ… æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: {pattern}")
                    break
            except:
                continue
        
        # å¦‚æœæ²¡æ‰¾åˆ°å…·ä½“é¡µæ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
        if pagination_info['total_pages'] == 0:
            pagination_info['total_pages'] = 2  # æ ¹æ®ä¹‹å‰åˆ†æè®¾ç½®é»˜è®¤å€¼
            print(f"  âš ï¸ æœªæ‰¾åˆ°å…·ä½“é¡µæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {pagination_info['total_pages']}")
        
        return pagination_info
    
    def extract_current_page_data(self, page_num):
        """æå–å½“å‰é¡µé¢çš„æ•°æ®"""
        print(f"  ğŸ“Š æå–ç¬¬ {page_num} é¡µæ•°æ®...")
        
        datacenters = []
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            time.sleep(3)
            
            # ä¿å­˜å½“å‰é¡µé¢HTML
            with open(f"html_sources/shanghai/page_{page_num}.html", 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            
            # æ–¹æ³•1: ä»JavaScriptå˜é‡ä¸­æå–æ•°æ®
            js_data = self.extract_from_javascript_vars(page_num)
            if js_data:
                datacenters.extend(js_data)
                print(f"    JavaScriptå˜é‡æå–: {len(js_data)} ä¸ª")
            
            # æ–¹æ³•2: ä»é¡µé¢æºç çš„æ­£åˆ™åŒ¹é…æå–
            regex_data = self.extract_from_page_source(page_num)
            if regex_data:
                datacenters.extend(regex_data)
                print(f"    æ­£åˆ™åŒ¹é…æå–: {len(regex_data)} ä¸ª")
            
            # æ–¹æ³•3: ä»DOMå…ƒç´ æå–
            dom_data = self.extract_from_dom_elements(page_num)
            if dom_data:
                datacenters.extend(dom_data)
                print(f"    DOMå…ƒç´ æå–: {len(dom_data)} ä¸ª")
            
            # å»é‡å½“å‰é¡µé¢æ•°æ®
            unique_datacenters = self.deduplicate_page_data(datacenters)
            
            print(f"    âœ… ç¬¬ {page_num} é¡µæ€»è®¡: {len(unique_datacenters)} ä¸ªæ•°æ®ä¸­å¿ƒ")
            return unique_datacenters
            
        except Exception as e:
            print(f"    âŒ ç¬¬ {page_num} é¡µæ•°æ®æå–å¤±è´¥: {e}")
            return []
    
    def extract_from_javascript_vars(self, page_num):
        """ä»JavaScriptå˜é‡ä¸­æå–æ•°æ®"""
        datacenters = []
        
        try:
            # å°è¯•æ‰§è¡Œä¸åŒçš„JavaScriptå‘½ä»¤è·å–æ•°æ®
            js_commands = [
                "return window.datacenters || [];",
                "return window.locations || [];",
                "return window.facilities || [];",
                "return window.markers || [];",
                "return window.mapData || [];",
                "return window.map_data || [];",
                "return window.data || [];",
            ]
            
            for cmd in js_commands:
                try:
                    result = self.driver.execute_script(cmd)
                    if result and isinstance(result, list) and len(result) > 0:
                        for item in result:
                            if isinstance(item, dict):
                                lat = item.get('latitude') or item.get('lat') or item.get('y')
                                lng = item.get('longitude') or item.get('lng') or item.get('x')
                                
                                if lat and lng:
                                    try:
                                        lat, lng = float(lat), float(lng)
                                        if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                                            name = item.get('name') or item.get('title') or f"æ•°æ®ä¸­å¿ƒ_{len(datacenters)+1}"
                                            
                                            datacenters.append({
                                                'name': name,
                                                'latitude': lat,
                                                'longitude': lng,
                                                'location': 'ä¸Šæµ·å¸‚',
                                                'source_page': page_num,
                                                'extraction_method': 'JavaScript',
                                                'raw_data': item
                                            })
                                    except ValueError:
                                        continue
                        
                        if datacenters:
                            break  # æ‰¾åˆ°æ•°æ®å°±åœæ­¢å°è¯•å…¶ä»–å‘½ä»¤
                except:
                    continue
                    
        except Exception as e:
            pass
        
        return datacenters
    
    def extract_from_page_source(self, page_num):
        """ä»é¡µé¢æºç ä¸­é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–æ•°æ®"""
        datacenters = []
        
        try:
            page_source = self.driver.page_source
            
            # å¤šç§æ­£åˆ™æ¨¡å¼æå–åæ ‡å’Œåç§°
            patterns = [
                # JSONæ ¼å¼æ•°æ®
                r'"latitude":\s*([\d\.-]+).*?"longitude":\s*([\d\.-]+).*?"name":\s*"([^"]*)"',
                r'"lat":\s*([\d\.-]+).*?"lng":\s*([\d\.-]+).*?"title":\s*"([^"]*)"',
                r'"y":\s*([\d\.-]+).*?"x":\s*([\d\.-]+).*?"name":\s*"([^"]*)"',
                
                # JavaScriptå¯¹è±¡æ ¼å¼
                r'latitude:\s*([\d\.-]+).*?longitude:\s*([\d\.-]+).*?name:\s*["\']([^"\']*)["\']',
                r'lat:\s*([\d\.-]+).*?lng:\s*([\d\.-]+).*?title:\s*["\']([^"\']*)["\']',
                
                # æ•°ç»„æ ¼å¼
                r'\[([\d\.-]+),\s*([\d\.-]+)\].*?"([^"]*(?:Data Center|IDC|æ•°æ®ä¸­å¿ƒ)[^"]*)"',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, page_source, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    try:
                        lat, lng, name = float(match[0]), float(match[1]), match[2].strip()
                        
                        # éªŒè¯åæ ‡èŒƒå›´
                        if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                            datacenters.append({
                                'name': name or f"æ•°æ®ä¸­å¿ƒ_{len(datacenters)+1}",
                                'latitude': lat,
                                'longitude': lng,
                                'location': 'ä¸Šæµ·å¸‚',
                                'source_page': page_num,
                                'extraction_method': 'Regex',
                                'raw_data': {
                                    'pattern': pattern,
                                    'match': match
                                }
                            })
                    except (ValueError, IndexError):
                        continue
                
                # å¦‚æœæŸä¸ªæ¨¡å¼æ‰¾åˆ°äº†æ•°æ®ï¼Œå¯ä»¥ç»§ç»­å°è¯•å…¶ä»–æ¨¡å¼ä»¥è·å–æ›´å¤šæ•°æ®
        
        except Exception as e:
            pass
        
        return datacenters
    
    def extract_from_dom_elements(self, page_num):
        """ä»DOMå…ƒç´ ä¸­æå–æ•°æ®"""
        datacenters = []
        
        try:
            # æŸ¥æ‰¾åŒ…å«åæ ‡ä¿¡æ¯çš„DOMå…ƒç´ 
            selectors = [
                '[data-lat][data-lng]',
                '[data-latitude][data-longitude]',
                '.marker[data-coordinates]',
                '.location[data-lat]',
                '.datacenter[data-coordinates]'
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        try:
                            lat = elem.get_attribute('data-lat') or elem.get_attribute('data-latitude')
                            lng = elem.get_attribute('data-lng') or elem.get_attribute('data-longitude')
                            
                            if lat and lng:
                                lat, lng = float(lat), float(lng)
                                
                                if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                                    name = (elem.get_attribute('title') or 
                                           elem.get_attribute('data-name') or
                                           elem.text.strip() or
                                           f"æ•°æ®ä¸­å¿ƒ_{len(datacenters)+1}")
                                    
                                    datacenters.append({
                                        'name': name,
                                        'latitude': lat,
                                        'longitude': lng,
                                        'location': 'ä¸Šæµ·å¸‚',
                                        'source_page': page_num,
                                        'extraction_method': 'DOM',
                                        'raw_data': {
                                            'tag': elem.tag_name,
                                            'attributes': elem.get_attribute('outerHTML')[:200]
                                        }
                                    })
                        except (ValueError, TypeError):
                            continue
                except:
                    continue
        
        except Exception as e:
            pass
        
        return datacenters
    
    def deduplicate_page_data(self, datacenters):
        """å»é‡å•é¡µæ•°æ®"""
        unique_datacenters = []
        seen_coords = set()
        
        for dc in datacenters:
            coord_key = (round(dc['latitude'], 5), round(dc['longitude'], 5))
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                unique_datacenters.append(dc)
        
        return unique_datacenters
    
    def click_next_page(self, target_page, pagination_info):
        """ç‚¹å‡»åˆ°æŒ‡å®šé¡µé¢"""
        print(f"  ğŸ–±ï¸ ç‚¹å‡»åˆ°ç¬¬ {target_page} é¡µ...")
        
        try:
            # æ–¹æ³•1: ç›´æ¥ç‚¹å‡»é¡µé¢æŒ‰é’®
            if pagination_info['page_buttons']:
                for button in pagination_info['page_buttons']:
                    try:
                        button_text = button.text.strip()
                        data_page = button.get_attribute('data-page')
                        
                        if (button_text == str(target_page) or 
                            data_page == str(target_page)):
                            
                            # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                            self.driver.execute_script("arguments[0].scrollIntoView(true);", button)
                            time.sleep(1)
                            
                            # ç‚¹å‡»æŒ‰é’®
                            self.driver.execute_script("arguments[0].click();", button)
                            print(f"    âœ… å·²ç‚¹å‡»ç¬¬ {target_page} é¡µæŒ‰é’®")
                            
                            # ç­‰å¾…é¡µé¢åŠ è½½
                            time.sleep(5)
                            return True
                    except Exception as e:
                        continue
            
            # æ–¹æ³•2: ä½¿ç”¨ä¸‹ä¸€é¡µæŒ‰é’®
            if pagination_info['next_button'] and target_page == 2:
                try:
                    # æ»šåŠ¨åˆ°ä¸‹ä¸€é¡µæŒ‰é’®
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", pagination_info['next_button'])
                    time.sleep(1)
                    
                    # ç‚¹å‡»ä¸‹ä¸€é¡µ
                    self.driver.execute_script("arguments[0].click();", pagination_info['next_button'])
                    print(f"    âœ… å·²ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®")
                    
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    time.sleep(5)
                    return True
                except Exception as e:
                    print(f"    âŒ ç‚¹å‡»ä¸‹ä¸€é¡µå¤±è´¥: {e}")
            
            # æ–¹æ³•3: å°è¯•ç›´æ¥æ‰§è¡ŒJavaScriptç¿»é¡µ
            js_functions = [
                f"goToPage({target_page})",
                f"loadPage({target_page})", 
                f"showPage({target_page})",
                f"setPage({target_page})",
                f"page = {target_page}; loadData()",
            ]
            
            for func in js_functions:
                try:
                    self.driver.execute_script(func)
                    time.sleep(3)
                    print(f"    âœ… æ‰§è¡ŒJavaScript: {func}")
                    return True
                except:
                    continue
            
            print(f"    âŒ æ— æ³•å¯¼èˆªåˆ°ç¬¬ {target_page} é¡µ")
            return False
            
        except Exception as e:
            print(f"    âŒ ç¿»é¡µå¤±è´¥: {e}")
            return False
    
    def run_real_pagination_crawl(self):
        """è¿è¡ŒçœŸå®ç¿»é¡µçˆ¬è™«"""
        print("ğŸš€ ä¸Šæµ·æ•°æ®ä¸­å¿ƒçœŸå®ç¿»é¡µçˆ¬è™«å¯åŠ¨")
        print("ğŸ¯ ç›®æ ‡ï¼šé€šè¿‡ç‚¹å‡»é¡µé¢æŒ‰é’®è·å–æ‰€æœ‰é¡µé¢æ•°æ®")
        print("="*70)
        
        if not self.setup_driver():
            return []
        
        try:
            # 1. åŠ è½½ä¸»é¡µé¢
            if not self.load_page_and_wait():
                return []
            
            # 2. æŸ¥æ‰¾ç¿»é¡µå…ƒç´ 
            pagination_info = self.find_pagination_elements()
            total_pages = pagination_info['total_pages']
            
            print(f"ğŸ“Š æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}")
            
            if total_pages == 0:
                print("âŒ æœªæ£€æµ‹åˆ°ç¿»é¡µå…ƒç´ ")
                return []
            
            # 3. é€é¡µçˆ¬å–æ•°æ®
            all_datacenters = []
            
            for page_num in range(1, total_pages + 1):
                print(f"\nğŸ“„ å¤„ç†ç¬¬ {page_num} é¡µ ({page_num}/{total_pages})")
                
                # å¦‚æœä¸æ˜¯ç¬¬ä¸€é¡µï¼Œéœ€è¦ç¿»é¡µ
                if page_num > 1:
                    success = self.click_next_page(page_num, pagination_info)
                    if not success:
                        print(f"    âŒ æ— æ³•ç¿»åˆ°ç¬¬ {page_num} é¡µ")
                        continue
                
                # æå–å½“å‰é¡µæ•°æ®
                page_data = self.extract_current_page_data(page_num)
                
                if page_data:
                    all_datacenters.extend(page_data)
                    self.page_data[f'page_{page_num}'] = page_data
                    print(f"    âœ… ç¬¬ {page_num} é¡µæˆåŠŸè·å– {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                else:
                    print(f"    âš ï¸ ç¬¬ {page_num} é¡µæœªè·å–åˆ°æ•°æ®")
                    self.page_data[f'page_{page_num}'] = []
                
                # é¡µé¢é—´å»¶è¿Ÿ
                time.sleep(2)
            
            # 4. æœ€ç»ˆå»é‡
            unique_datacenters = self.final_deduplicate(all_datacenters)
            
            # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            print(f"\n{'='*70}")
            print(f"ğŸ“Š çœŸå®ç¿»é¡µçˆ¬å–å®Œæˆ:")
            print(f"  æ€»é¡µæ•°: {total_pages}")
            print(f"  åŸå§‹æ•°æ®: {len(all_datacenters)} ä¸ª")
            print(f"  å»é‡åæ•°æ®: {len(unique_datacenters)} ä¸ª")
            
            # æŒ‰é¡µç»Ÿè®¡
            for page_num in range(1, total_pages + 1):
                page_count = len(self.page_data.get(f'page_{page_num}', []))
                print(f"  ç¬¬ {page_num} é¡µ: {page_count} ä¸ªæ•°æ®ä¸­å¿ƒ")
            
            return unique_datacenters
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
                print("ğŸ”š WebDriverå·²å…³é—­")
    
    def final_deduplicate(self, datacenters):
        """æœ€ç»ˆå»é‡å¤„ç†"""
        print(f"ğŸ”„ æœ€ç»ˆå»é‡å¤„ç†...")
        
        unique_datacenters = []
        seen_coords = set()
        
        for dc in datacenters:
            coord_key = (round(dc['latitude'], 5), round(dc['longitude'], 5))
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                unique_datacenters.append(dc)
        
        print(f"  å»é‡å‰: {len(datacenters)} ä¸ª")
        print(f"  å»é‡å: {len(unique_datacenters)} ä¸ª")
        
        return unique_datacenters
    
    def save_results(self, datacenters):
        """ä¿å­˜ç»“æœ"""
        if not datacenters:
            print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ•°æ®
        json_file = f"data/shanghai/ä¸Šæµ·æ•°æ®ä¸­å¿ƒçœŸå®ç¿»é¡µ_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(datacenters, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        
        # ä¿å­˜åˆ†é¡µè¯¦æƒ…
        page_detail_file = f"data/shanghai/çœŸå®ç¿»é¡µè¯¦æƒ…_{timestamp}.json"
        with open(page_detail_file, 'w', encoding='utf-8') as f:
            json.dump(self.page_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ†é¡µè¯¦æƒ…å·²ä¿å­˜: {page_detail_file}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(datacenters, timestamp)
    
    def generate_report(self, datacenters, timestamp):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report_file = f"data/shanghai/çœŸå®ç¿»é¡µçˆ¬å–æŠ¥å‘Š_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ä¸Šæµ·æ•°æ®ä¸­å¿ƒçœŸå®ç¿»é¡µçˆ¬å–æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"çˆ¬å–æ–¹æ³•: SeleniumçœŸå®ç‚¹å‡»ç¿»é¡µ\n")
            f.write(f"æ€»æ•°æ®ä¸­å¿ƒ: {len(datacenters)} ä¸ª\n")
            f.write(f"ç›®æ ‡å®Œæˆåº¦: {len(datacenters)}/54 ({len(datacenters)/54*100:.1f}%)\n\n")
            
            # æŒ‰é¡µç»Ÿè®¡
            f.write("åˆ†é¡µç»Ÿè®¡:\n")
            f.write("-"*30 + "\n")
            for page_key, page_data in self.page_data.items():
                f.write(f"{page_key}: {len(page_data)} ä¸ªæ•°æ®ä¸­å¿ƒ\n")
            f.write("\n")
            
            # æŒ‰æå–æ–¹æ³•ç»Ÿè®¡
            method_stats = {}
            for dc in datacenters:
                method = dc.get('extraction_method', 'æœªçŸ¥')
                method_stats[method] = method_stats.get(method, 0) + 1
            
            f.write("æå–æ–¹æ³•ç»Ÿè®¡:\n")
            f.write("-"*30 + "\n")
            for method, count in method_stats.items():
                f.write(f"{method}: {count} ä¸ª\n")
            f.write("\n")
            
            # è¯¦ç»†æ•°æ®åˆ—è¡¨
            f.write("æ•°æ®ä¸­å¿ƒè¯¦ç»†åˆ—è¡¨:\n")
            f.write("-"*30 + "\n")
            for i, dc in enumerate(datacenters, 1):
                f.write(f"{i:2d}. {dc['name']}\n")
                f.write(f"    åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                f.write(f"    æ¥æºé¡µ: ç¬¬{dc['source_page']}é¡µ\n")
                f.write(f"    æå–æ–¹æ³•: {dc['extraction_method']}\n\n")
        
        print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¸Šæµ·æ•°æ®ä¸­å¿ƒçœŸå®ç¿»é¡µçˆ¬è™«")
    print("ğŸ–±ï¸ é€šè¿‡Seleniumæ¨¡æ‹Ÿç‚¹å‡»é¡µé¢æŒ‰é’®å®ç°ç¿»é¡µ")
    print("ğŸ“„ ç›®æ ‡ï¼šç¬¬1é¡µ40ä¸ªï¼Œç¬¬2é¡µ14ä¸ªï¼Œå…±54ä¸ªæ•°æ®ä¸­å¿ƒ")
    
    crawler = RealPaginationCrawler()
    
    try:
        # è¿è¡ŒçœŸå®ç¿»é¡µçˆ¬è™«
        results = crawler.run_real_pagination_crawl()
        
        if results:
            print(f"\nğŸ‰ çœŸå®ç¿»é¡µçˆ¬å–å®Œæˆï¼")
            print(f"âœ… è·å–åˆ° {len(results)} ä¸ªä¸Šæµ·æ•°æ®ä¸­å¿ƒ")
            
            # æ˜¾ç¤ºå‰10ä¸ªç»“æœé¢„è§ˆ
            print(f"\nğŸ“‹ å‰10ä¸ªæ•°æ®ä¸­å¿ƒé¢„è§ˆ:")
            for i, dc in enumerate(results[:10], 1):
                print(f"  {i:2d}. {dc['name']} (ç¬¬{dc['source_page']}é¡µ, {dc['extraction_method']})")
            
            if len(results) > 10:
                print(f"     ... è¿˜æœ‰ {len(results) - 10} ä¸ª")
            
            # ä¿å­˜ç»“æœ
            crawler.save_results(results)
            
            # æ£€æŸ¥ç›®æ ‡è¾¾æˆæƒ…å†µ
            if len(results) >= 54:
                print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆï¼è·å–åˆ° {len(results)} ä¸ªæ•°æ®ä¸­å¿ƒ (ç›®æ ‡54ä¸ª)")
            else:
                print(f"\nâš ï¸ è·ç¦»ç›®æ ‡è¿˜å·® {54 - len(results)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                print(f"ğŸ“‹ å½“å‰è·å–: {len(results)}/54")
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
