#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šæµ·æ•°æ®ä¸­å¿ƒå®Œæ•´ç¿»é¡µåˆ†æå·¥å…·
åˆ†æç½‘ç«™çš„ç¿»é¡µæœºåˆ¶å¹¶ç¡®ä¿è·å–æ‰€æœ‰54ä¸ªæ•°æ®ä¸­å¿ƒ
"""

import requests
import re
import json
import time
import os
from datetime import datetime
from bs4 import BeautifulSoup

class ShanghaiPaginationAnalyzer:
    def __init__(self):
        self.base_url = "https://www.datacenters.com/locations/china/shanghai"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.datacenters.com/',
            'Cache-Control': 'no-cache',
            'DNT': '1',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.all_datacenters = []
        self.page_data = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("html_sources/shanghai", exist_ok=True)
        os.makedirs("data/shanghai", exist_ok=True)
    
    def analyze_main_page(self):
        """åˆ†æä¸»é¡µé¢ï¼Œäº†è§£ç¿»é¡µç»“æ„"""
        print("ğŸ” åˆ†æä¸»é¡µé¢ç»“æ„...")
        
        try:
            response = self.session.get(self.base_url, timeout=30)
            if response.status_code == 200:
                # ä¿å­˜é¡µé¢
                with open("html_sources/shanghai/main_analysis.html", 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯
                pagination_info = self.extract_pagination_info(soup, response.text)
                
                # æŸ¥æ‰¾æ•°æ®å®¹å™¨
                data_containers = self.find_data_containers(soup)
                
                # æŸ¥æ‰¾AJAXç«¯ç‚¹
                ajax_endpoints = self.find_ajax_endpoints(response.text)
                
                print(f"âœ… ä¸»é¡µé¢åˆ†æå®Œæˆ")
                print(f"  ç¿»é¡µä¿¡æ¯: {pagination_info}")
                print(f"  æ•°æ®å®¹å™¨: {len(data_containers)} ä¸ª")
                print(f"  AJAXç«¯ç‚¹: {len(ajax_endpoints)} ä¸ª")
                
                return {
                    'pagination': pagination_info,
                    'containers': data_containers,
                    'ajax': ajax_endpoints,
                    'content': response.text
                }
            else:
                print(f"âŒ ä¸»é¡µé¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ ä¸»é¡µé¢åˆ†æé”™è¯¯: {e}")
            return None
    
    def extract_pagination_info(self, soup, content):
        """æå–åˆ†é¡µä¿¡æ¯"""
        pagination_info = {
            'total_pages': 0,
            'current_page': 1,
            'items_per_page': 0,
            'total_items': 0,
            'pagination_type': 'unknown'
        }
        
        # æŸ¥æ‰¾åˆ†é¡µç›¸å…³çš„æ–‡æœ¬å’Œå…ƒç´ 
        pagination_patterns = [
            r'(?:Page|é¡µé¢)\s+(\d+)\s+of\s+(\d+)',
            r'(\d+)\s*-\s*(\d+)\s+of\s+(\d+)',
            r'Showing\s+(\d+)\s*-\s*(\d+)\s+of\s+(\d+)',
            r'"totalPages":\s*(\d+)',
            r'"currentPage":\s*(\d+)',
            r'"totalResults":\s*(\d+)',
            r'"itemsPerPage":\s*(\d+)',
        ]
        
        for pattern in pagination_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                print(f"  æ‰¾åˆ°åˆ†é¡µæ¨¡å¼: {pattern} -> {matches}")
        
        # æŸ¥æ‰¾åˆ†é¡µæŒ‰é’®
        pagination_elements = soup.find_all(['a', 'button'], text=re.compile(r'(Next|Previous|ä¸‹ä¸€é¡µ|ä¸Šä¸€é¡µ|\d+)', re.IGNORECASE))
        if pagination_elements:
            print(f"  æ‰¾åˆ°åˆ†é¡µæŒ‰é’®: {len(pagination_elements)} ä¸ª")
        
        # æŸ¥æ‰¾æ€»æ•°ä¿¡æ¯
        total_patterns = [
            r'(\d+)\s*(?:data centers?|æ•°æ®ä¸­å¿ƒ)',
            r'Total:\s*(\d+)',
            r'Found\s+(\d+)',
        ]
        
        for pattern in total_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                total = int(matches[0])
                pagination_info['total_items'] = total
                print(f"  æ‰¾åˆ°æ€»æ•°: {total}")
                break
        
        return pagination_info
    
    def find_data_containers(self, soup):
        """æŸ¥æ‰¾æ•°æ®å®¹å™¨"""
        containers = []
        
        # å¸¸è§çš„æ•°æ®å®¹å™¨é€‰æ‹©å™¨
        container_selectors = [
            '.datacenter-item',
            '.location-item',
            '.facility-item',
            '[data-lat]',
            '[data-longitude]',
            '.result-item',
            '.listing-item',
        ]
        
        for selector in container_selectors:
            elements = soup.select(selector)
            if elements:
                containers.extend(elements)
                print(f"  æ‰¾åˆ°å®¹å™¨ {selector}: {len(elements)} ä¸ª")
        
        return containers
    
    def find_ajax_endpoints(self, content):
        """æŸ¥æ‰¾AJAXç«¯ç‚¹"""
        endpoints = []
        
        # AJAXç«¯ç‚¹æ¨¡å¼
        ajax_patterns = [
            r'["\']([^"\']*api[^"\']*)["\']',
            r'["\']([^"\']*ajax[^"\']*)["\']',
            r'url:\s*["\']([^"\']+)["\']',
            r'fetch\(["\']([^"\']+)["\']',
        ]
        
        for pattern in ajax_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if 'datacenter' in match.lower() or 'location' in match.lower():
                    endpoints.append(match)
        
        return list(set(endpoints))
    
    def try_different_pagination_methods(self):
        """å°è¯•ä¸åŒçš„ç¿»é¡µæ–¹æ³•"""
        print("ğŸ”„ å°è¯•ä¸åŒçš„ç¿»é¡µæ–¹æ³•...")
        
        all_results = []
        methods_tried = []
        
        # æ–¹æ³•1: URLå‚æ•°ç¿»é¡µ
        print("  æ–¹æ³•1: URLå‚æ•°ç¿»é¡µ")
        results1 = self.try_url_pagination()
        if results1:
            all_results.extend(results1)
            methods_tried.append("URLå‚æ•°ç¿»é¡µ")
        
        # æ–¹æ³•2: AJAXç¿»é¡µ
        print("  æ–¹æ³•2: AJAXç¿»é¡µ")
        results2 = self.try_ajax_pagination()
        if results2:
            all_results.extend(results2)
            methods_tried.append("AJAXç¿»é¡µ")
        
        # æ–¹æ³•3: è¡¨å•æäº¤ç¿»é¡µ
        print("  æ–¹æ³•3: è¡¨å•æäº¤ç¿»é¡µ")
        results3 = self.try_form_pagination()
        if results3:
            all_results.extend(results3)
            methods_tried.append("è¡¨å•æäº¤ç¿»é¡µ")
        
        # æ–¹æ³•4: ç›´æ¥URLæ„é€ 
        print("  æ–¹æ³•4: ç›´æ¥URLæ„é€ ")
        results4 = self.try_direct_urls()
        if results4:
            all_results.extend(results4)
            methods_tried.append("ç›´æ¥URLæ„é€ ")
        
        print(f"âœ… å°è¯•çš„æ–¹æ³•: {methods_tried}")
        print(f"âœ… æ€»è·å–æ•°æ®: {len(all_results)} ä¸ª")
        
        return all_results, methods_tried
    
    def try_url_pagination(self):
        """å°è¯•URLå‚æ•°ç¿»é¡µ"""
        results = []
        
        url_patterns = [
            f"{self.base_url}?page={{}}",
            f"{self.base_url}?p={{}}",
            f"{self.base_url}?offset={{}}",
            f"{self.base_url}/page/{{}}",
            f"{self.base_url}?start={{}}",
            f"{self.base_url}?from={{}}",
        ]
        
        for pattern in url_patterns:
            print(f"    å°è¯•æ¨¡å¼: {pattern.replace('{}', 'N')}")
            
            for page in range(1, 10):  # å°è¯•å‰9é¡µ
                url = pattern.format(page)
                
                try:
                    response = self.session.get(url, timeout=20)
                    if response.status_code == 200:
                        # ä¿å­˜é¡µé¢
                        page_file = f"html_sources/shanghai/page_{page}_pattern_{pattern.split('?')[-1].split('=')[0]}.html"
                        with open(page_file, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        
                        # æå–æ•°æ®
                        page_results = self.extract_datacenters(response.text, f"page_{page}")
                        
                        if page_results:
                            results.extend(page_results)
                            print(f"      é¡µé¢ {page}: æ‰¾åˆ° {len(page_results)} ä¸ªæ•°æ®ä¸­å¿ƒ")
                        else:
                            if page > 1:  # ç¬¬ä¸€é¡µæ²¡æ•°æ®å¯èƒ½æ˜¯æ¨¡å¼é”™è¯¯ï¼Œå…¶ä»–é¡µæ²¡æ•°æ®å¯èƒ½æ˜¯ç»“æŸäº†
                                break
                    
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    print(f"      é¡µé¢ {page} é”™è¯¯: {e}")
                    if page > 1:
                        break
            
            if results:
                print(f"    æ¨¡å¼ {pattern.replace('{}', 'N')} æˆåŠŸï¼Œå…±è·å– {len(results)} ä¸ª")
                break  # æ‰¾åˆ°å·¥ä½œçš„æ¨¡å¼å°±åœæ­¢
        
        return results
    
    def try_ajax_pagination(self):
        """å°è¯•AJAXç¿»é¡µ"""
        results = []
        
        # å¸¸è§çš„AJAXç«¯ç‚¹
        ajax_endpoints = [
            "/api/datacenters",
            "/ajax/locations",
            "/search/datacenters",
            "/locations/shanghai/data",
        ]
        
        for endpoint in ajax_endpoints:
            url = f"https://www.datacenters.com{endpoint}"
            
            try:
                # å°è¯•ä¸åŒçš„å‚æ•°
                params_list = [
                    {'location': 'shanghai', 'page': 1},
                    {'city': 'shanghai', 'page': 1},
                    {'query': 'shanghai', 'offset': 0},
                    {'lat': 31.2304, 'lng': 121.4737, 'radius': 50},
                ]
                
                for params in params_list:
                    response = self.session.get(url, params=params, timeout=20)
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if isinstance(data, list) and data:
                                results.extend(self.process_json_data(data, "ajax"))
                                print(f"    AJAXç«¯ç‚¹ {endpoint} æˆåŠŸ")
                                return results
                        except:
                            # å¯èƒ½è¿”å›çš„æ˜¯HTML
                            page_results = self.extract_datacenters(response.text, "ajax")
                            if page_results:
                                results.extend(page_results)
                                return results
                
            except Exception as e:
                print(f"    AJAXç«¯ç‚¹ {endpoint} é”™è¯¯: {e}")
        
        return results
    
    def try_form_pagination(self):
        """å°è¯•è¡¨å•æäº¤ç¿»é¡µ"""
        results = []
        
        try:
            # è·å–ä¸»é¡µé¢æ‰¾åˆ°è¡¨å•
            response = self.session.get(self.base_url, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æœç´¢æˆ–åˆ†é¡µè¡¨å•
                forms = soup.find_all('form')
                for form in forms:
                    action = form.get('action', '')
                    method = form.get('method', 'GET').upper()
                    
                    if 'search' in action.lower() or 'location' in action.lower():
                        # æå–è¡¨å•æ•°æ®
                        form_data = {}
                        for input_elem in form.find_all(['input', 'select']):
                            name = input_elem.get('name')
                            value = input_elem.get('value', '')
                            if name:
                                form_data[name] = value
                        
                        # æ·»åŠ ä¸Šæµ·ç›¸å…³å‚æ•°
                        form_data.update({
                            'location': 'shanghai',
                            'city': 'shanghai',
                            'country': 'china',
                            'page': '1'
                        })
                        
                        # æäº¤è¡¨å•
                        if method == 'POST':
                            form_response = self.session.post(f"https://www.datacenters.com{action}", data=form_data, timeout=20)
                        else:
                            form_response = self.session.get(f"https://www.datacenters.com{action}", params=form_data, timeout=20)
                        
                        if form_response.status_code == 200:
                            page_results = self.extract_datacenters(form_response.text, "form")
                            if page_results:
                                results.extend(page_results)
                                print(f"    è¡¨å•æäº¤æˆåŠŸï¼Œè·å– {len(page_results)} ä¸ª")
        
        except Exception as e:
            print(f"    è¡¨å•æäº¤é”™è¯¯: {e}")
        
        return results
    
    def try_direct_urls(self):
        """å°è¯•ç›´æ¥æ„é€ URL"""
        results = []
        
        # å°è¯•ä¸åŒçš„URLæ„é€ æ–¹å¼
        direct_urls = [
            "https://www.datacenters.com/locations/china-shanghai",
            "https://www.datacenters.com/shanghai",
            "https://www.datacenters.com/search?location=shanghai",
            "https://www.datacenters.com/search?q=shanghai",
            "https://www.datacenters.com/china/shanghai/all",
            "https://www.datacenters.com/locations/shanghai/list",
        ]
        
        for url in direct_urls:
            try:
                response = self.session.get(url, timeout=20)
                if response.status_code == 200:
                    page_results = self.extract_datacenters(response.text, f"direct_{url.split('/')[-1]}")
                    if page_results:
                        results.extend(page_results)
                        print(f"    ç›´æ¥URL {url} æˆåŠŸï¼Œè·å– {len(page_results)} ä¸ª")
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    ç›´æ¥URL {url} é”™è¯¯: {e}")
        
        return results
    
    def extract_datacenters(self, content, source):
        """ä»å†…å®¹ä¸­æå–æ•°æ®ä¸­å¿ƒä¿¡æ¯"""
        results = []
        
        # JavaScriptæ•°æ®æå–
        js_patterns = [
            r'"latitude":\s*([\d\.-]+).*?"longitude":\s*([\d\.-]+).*?"name":\s*"([^"]*)"',
            r'"lat":\s*([\d\.-]+).*?"lng":\s*([\d\.-]+).*?"title":\s*"([^"]*)"',
            r'position:\s*\[([\d\.-]+),\s*([\d\.-]+)\].*?name:\s*"([^"]*)"',
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    lat, lng, name = float(match[0]), float(match[1]), match[2].strip()
                    
                    # éªŒè¯åæ ‡æ˜¯å¦åœ¨ä¸Šæµ·èŒƒå›´å†…
                    if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                        results.append({
                            'name': name,
                            'latitude': lat,
                            'longitude': lng,
                            'source': source,
                            'province': 'ä¸Šæµ·å¸‚',
                            'city': 'ä¸Šæµ·å¸‚',
                            'coordinates': f"{lat},{lng}",
                            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                except ValueError:
                    continue
        
        # HTMLç»“æ„åŒ–æ•°æ®æå–
        soup = BeautifulSoup(content, 'html.parser')
        
        # æŸ¥æ‰¾å¸¦åæ ‡å±æ€§çš„å…ƒç´ 
        coord_elements = soup.find_all(attrs={'data-lat': True}) + soup.find_all(attrs={'data-latitude': True})
        for elem in coord_elements:
            try:
                lat = float(elem.get('data-lat') or elem.get('data-latitude'))
                lng = float(elem.get('data-lng') or elem.get('data-longitude'))
                
                name = (elem.get('title') or 
                       elem.get('data-name') or 
                       elem.get_text(strip=True) or 
                       f"ä¸Šæµ·æ•°æ®ä¸­å¿ƒ_{len(results)+1}")
                
                if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                    results.append({
                        'name': name,
                        'latitude': lat,
                        'longitude': lng,
                        'source': source,
                        'province': 'ä¸Šæµ·å¸‚',
                        'city': 'ä¸Šæµ·å¸‚',
                        'coordinates': f"{lat},{lng}",
                        'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
            except (ValueError, TypeError):
                continue
        
        return results
    
    def process_json_data(self, data, source):
        """å¤„ç†JSONæ•°æ®"""
        results = []
        
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    lat = item.get('latitude') or item.get('lat')
                    lng = item.get('longitude') or item.get('lng')
                    name = item.get('name') or item.get('title') or f"æ•°æ®ä¸­å¿ƒ_{len(results)+1}"
                    
                    if lat and lng:
                        try:
                            lat, lng = float(lat), float(lng)
                            if 30.6 <= lat <= 31.9 and 120.8 <= lng <= 122.2:
                                results.append({
                                    'name': name,
                                    'latitude': lat,
                                    'longitude': lng,
                                    'source': source,
                                    'province': 'ä¸Šæµ·å¸‚',
                                    'city': 'ä¸Šæµ·å¸‚',
                                    'coordinates': f"{lat},{lng}",
                                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                })
                        except ValueError:
                            continue
        
        return results
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ ä¸Šæµ·æ•°æ®ä¸­å¿ƒå®Œæ•´ç¿»é¡µåˆ†æå¯åŠ¨")
        print("ğŸ¯ ç›®æ ‡ï¼šåˆ†æç¿»é¡µæœºåˆ¶å¹¶è·å–æ‰€æœ‰54ä¸ªæ•°æ®ä¸­å¿ƒ")
        print("="*70)
        
        # 1. åˆ†æä¸»é¡µé¢
        main_analysis = self.analyze_main_page()
        
        # 2. å°è¯•ä¸åŒç¿»é¡µæ–¹æ³•
        all_results, methods = self.try_different_pagination_methods()
        
        # 3. å»é‡
        unique_results = self.deduplicate_results(all_results)
        
        # 4. ä¿å­˜ç»“æœ
        if unique_results:
            self.save_analysis_results(unique_results, methods, main_analysis)
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š å®Œæ•´åˆ†æç»“æœ:")
        print(f"  æ€»æ•°æ®ä¸­å¿ƒ: {len(unique_results)} ä¸ª")
        print(f"  æˆåŠŸæ–¹æ³•: {methods}")
        print(f"  ç›®æ ‡è¾¾æˆ: {'âœ…' if len(unique_results) >= 54 else 'âŒ'} ({len(unique_results)}/54)")
        
        return unique_results
    
    def deduplicate_results(self, results):
        """å»é‡ç»“æœ"""
        print(f"ğŸ”„ æ•°æ®å»é‡: {len(results)} -> ", end="")
        
        unique_results = []
        seen_coords = set()
        
        for result in results:
            # åŸºäºåæ ‡å»é‡ï¼ˆç²¾ç¡®åˆ°å°æ•°ç‚¹å5ä½ï¼‰
            coord_key = (round(result['latitude'], 5), round(result['longitude'], 5))
            
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                result['index'] = len(unique_results) + 1
                unique_results.append(result)
        
        print(f"{len(unique_results)}")
        return unique_results
    
    def save_analysis_results(self, results, methods, main_analysis):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ•°æ®
        json_file = f"data/shanghai/ä¸Šæµ·æ•°æ®ä¸­å¿ƒå®Œæ•´ç¿»é¡µ_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®å·²ä¿å­˜: {json_file}")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report_file = f"data/shanghai/ç¿»é¡µåˆ†ææŠ¥å‘Š_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ä¸Šæµ·æ•°æ®ä¸­å¿ƒå®Œæ•´ç¿»é¡µåˆ†ææŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç›®æ ‡æ•°é‡: 54ä¸ªæ•°æ®ä¸­å¿ƒ\n")
            f.write(f"å®é™…è·å–: {len(results)}ä¸ªæ•°æ®ä¸­å¿ƒ\n")
            f.write(f"è¾¾æˆç‡: {len(results)/54*100:.1f}%\n\n")
            
            f.write("æˆåŠŸçš„ç¿»é¡µæ–¹æ³•:\n")
            f.write("-"*30 + "\n")
            for method in methods:
                f.write(f"âœ… {method}\n")
            f.write("\n")
            
            if main_analysis:
                f.write("ä¸»é¡µé¢åˆ†æç»“æœ:\n")
                f.write("-"*30 + "\n")
                f.write(f"ç¿»é¡µä¿¡æ¯: {main_analysis.get('pagination', {})}\n")
                f.write(f"æ•°æ®å®¹å™¨: {len(main_analysis.get('containers', []))}ä¸ª\n")
                f.write(f"AJAXç«¯ç‚¹: {len(main_analysis.get('ajax', []))}ä¸ª\n\n")
            
            f.write("è·å–çš„æ•°æ®ä¸­å¿ƒåˆ—è¡¨:\n")
            f.write("-"*30 + "\n")
            for i, dc in enumerate(results, 1):
                f.write(f"{i:2d}. {dc['name']}\n")
                f.write(f"    åæ ‡: ({dc['latitude']:.6f}, {dc['longitude']:.6f})\n")
                f.write(f"    æ¥æº: {dc['source']}\n\n")
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” ä¸Šæµ·æ•°æ®ä¸­å¿ƒå®Œæ•´ç¿»é¡µåˆ†æå·¥å…·")
    print("ğŸ¯ åˆ†æç¿»é¡µæœºåˆ¶ï¼Œç¡®ä¿è·å–æ‰€æœ‰54ä¸ªæ•°æ®ä¸­å¿ƒ")
    
    analyzer = ShanghaiPaginationAnalyzer()
    
    try:
        results = analyzer.run_complete_analysis()
        
        if len(results) >= 54:
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸè·å– {len(results)} ä¸ªæ•°æ®ä¸­å¿ƒ")
        else:
            print(f"\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šç›®å‰è·å– {len(results)} ä¸ªï¼Œç›®æ ‡ 54 ä¸ª")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
